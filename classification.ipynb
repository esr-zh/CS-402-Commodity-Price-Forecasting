{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "# args.output_attention = True\n",
    "\n",
    "#exp = Exp(args)\n",
    "\n",
    "#model = exp.model\n",
    "\n",
    "#setting = 'ETTh1_96_24_Autoformer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0'\n",
    "#path = os.path.join(args.checkpoints,setting,'checkpoint.pth')\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #disable CUDA by setting CUDA_VISIBLE_DEVICES to an empty string \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In finance and stock market analysis, it's not uncommon to consider a small percentage change as \"neutral\" or \"no change\" because daily stock prices can fluctuate due to a variety of factors, including market volatility. By setting a threshold of 0.25%, you're essentially saying that any change in price less than this percentage is insignificant and should be considered neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "commodities = ['gold', 'silver', 'natural_gas', 'coffee', 'corn', 'crude_oil']\n",
    "OT_dict = {0: 'Decrease', 1: 'Increase'}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(commodities), figsize=(20, 5))\n",
    "\n",
    "\n",
    "for i, commodity in enumerate(commodities):\n",
    "    df = pd.read_csv(f'./dataset/commodity/{commodity}_data.csv')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Make sure the 'close' column is correctly named\n",
    "    df.columns = df.columns.str.replace('OT', 'close')\n",
    "\n",
    "    # Calculate the percentage change between the closing price of the next day and the current day\n",
    "    df['close_pct_change'] = df['close'].pct_change(periods=-1) * 100\n",
    "    \n",
    "    # Create the target column based on the 'close_pct_change' column\n",
    "    df['OT'] = 0.0 # neutral\n",
    "    df.loc[df['close_pct_change'] > 0, 'OT'] = 1.0 # increase\n",
    "    # df.loc[df['close_pct_change'] < -0.25, 'OT'] = 2 # decrease\n",
    "\n",
    "    df = df.drop(columns=['close_pct_change'])\n",
    "    df = df.drop(columns=['close', 'Adj Close'])\n",
    "\n",
    "    # Drop the last row as it doesn't have a valid 'OT' value\n",
    "    df = df[:-1]\n",
    "\n",
    "    # Save the dataframe as a new CSV file\n",
    "    df.to_csv(f'./dataset/commodity/{commodity}_data_c.csv')\n",
    "    \n",
    "    sns.countplot(x='OT', data=df, ax=axes[i])\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([OT_dict[i] for i in [0, 1]])\n",
    "    axes[i].set_title(f'Distribution of {commodity} classes')\n",
    "\n",
    "# Displaying the plots\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_bar.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f'./dataset/commodity/gold_data_c.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################  gold_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.4515233\n",
      "\tspeed: 0.2572s/iter; left time: 2559.4394s\n",
      "\titers: 200, epoch: 1 | loss: 0.5054529\n",
      "\tspeed: 0.0026s/iter; left time: 25.7438s\n",
      "Epoch: 1 cost time: 26.88035297393799\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.4764939 Vali Loss: 1.4964520 Test Loss: 0.9893274\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4311605\n",
      "\tspeed: 0.3708s/iter; left time: 3615.2854s\n",
      "\titers: 200, epoch: 2 | loss: 0.5266655\n",
      "\tspeed: 0.0022s/iter; left time: 21.6547s\n",
      "Epoch: 2 cost time: 24.580622673034668\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.4432168 Vali Loss: 1.3520074 Test Loss: 0.8852887\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4864195\n",
      "\tspeed: 0.3779s/iter; left time: 3608.8218s\n",
      "\titers: 200, epoch: 3 | loss: 0.4028131\n",
      "\tspeed: 0.0024s/iter; left time: 22.3972s\n",
      "Epoch: 3 cost time: 24.560643672943115\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.4352664 Vali Loss: 1.4681276 Test Loss: 1.0421034\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4980484\n",
      "\tspeed: 0.3758s/iter; left time: 3513.2024s\n",
      "\titers: 200, epoch: 4 | loss: 0.4548787\n",
      "\tspeed: 0.0025s/iter; left time: 22.7927s\n",
      "Epoch: 4 cost time: 24.802781581878662\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.4317233 Vali Loss: 1.3828634 Test Loss: 0.9136162\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4083761\n",
      "\tspeed: 0.3761s/iter; left time: 3440.3401s\n",
      "\titers: 200, epoch: 5 | loss: 0.4057913\n",
      "\tspeed: 0.0023s/iter; left time: 20.9331s\n",
      "Epoch: 5 cost time: 24.103899478912354\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.4306567 Vali Loss: 1.3448943 Test Loss: 0.8669825\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4604294\n",
      "\tspeed: 0.3721s/iter; left time: 3329.1925s\n",
      "\titers: 200, epoch: 6 | loss: 0.3909421\n",
      "\tspeed: 0.0022s/iter; left time: 19.5877s\n",
      "Epoch: 6 cost time: 24.076597690582275\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.4299075 Vali Loss: 1.4023142 Test Loss: 0.9188581\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4006211\n",
      "\tspeed: 0.3716s/iter; left time: 3249.9856s\n",
      "\titers: 200, epoch: 7 | loss: 0.5073584\n",
      "\tspeed: 0.0026s/iter; left time: 22.3279s\n",
      "Epoch: 7 cost time: 24.56306290626526\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.4295690 Vali Loss: 1.3639027 Test Loss: 0.8951111\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.4590106\n",
      "\tspeed: 0.3750s/iter; left time: 3203.6197s\n",
      "\titers: 200, epoch: 8 | loss: 0.3677874\n",
      "\tspeed: 0.0025s/iter; left time: 21.0536s\n",
      "Epoch: 8 cost time: 24.21582293510437\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.4293980 Vali Loss: 1.3745050 Test Loss: 0.8940009\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4941946\n",
      "\tspeed: 0.3701s/iter; left time: 3088.1286s\n",
      "\titers: 200, epoch: 9 | loss: 0.4135148\n",
      "\tspeed: 0.0023s/iter; left time: 19.2371s\n",
      "Epoch: 9 cost time: 24.363267421722412\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.4293007 Vali Loss: 1.3654432 Test Loss: 0.8930917\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.4431312\n",
      "\tspeed: 0.3958s/iter; left time: 3222.3929s\n",
      "\titers: 200, epoch: 10 | loss: 0.4413811\n",
      "\tspeed: 0.0025s/iter; left time: 20.4523s\n",
      "Epoch: 10 cost time: 26.17076539993286\n",
      "Epoch: 10, Steps: 201 | Train Loss: 0.4292563 Vali Loss: 1.3724684 Test Loss: 0.8907480\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.753 & 0.698 \n",
      "440, 440\n",
      "######################  silver_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1607\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.3429714\n",
      "\tspeed: 0.2497s/iter; left time: 2471.8171s\n",
      "\titers: 200, epoch: 1 | loss: 0.4067443\n",
      "\tspeed: 0.0030s/iter; left time: 29.2272s\n",
      "Epoch: 1 cost time: 26.25163960456848\n",
      "Epoch: 1, Steps: 200 | Train Loss: 0.4136783 Vali Loss: 0.8662719 Test Loss: 0.5110341\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4954151\n",
      "\tspeed: 0.4048s/iter; left time: 3927.1110s\n",
      "\titers: 200, epoch: 2 | loss: 0.3864937\n",
      "\tspeed: 0.0029s/iter; left time: 28.0296s\n",
      "Epoch: 2 cost time: 25.739697456359863\n",
      "Epoch: 2, Steps: 200 | Train Loss: 0.3920603 Vali Loss: 0.8330175 Test Loss: 0.4579434\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4535121\n",
      "\tspeed: 0.3966s/iter; left time: 3767.7044s\n",
      "\titers: 200, epoch: 3 | loss: 0.3583914\n",
      "\tspeed: 0.0023s/iter; left time: 21.7522s\n",
      "Epoch: 3 cost time: 24.795146465301514\n",
      "Epoch: 3, Steps: 200 | Train Loss: 0.3857642 Vali Loss: 0.8420712 Test Loss: 0.4740285\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.3850468\n",
      "\tspeed: 0.4032s/iter; left time: 3749.7010s\n",
      "\titers: 200, epoch: 4 | loss: 0.3329832\n",
      "\tspeed: 0.0024s/iter; left time: 21.8018s\n",
      "Epoch: 4 cost time: 26.821147203445435\n",
      "Epoch: 4, Steps: 200 | Train Loss: 0.3834346 Vali Loss: 0.8271567 Test Loss: 0.4873953\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3739680\n",
      "\tspeed: 0.3803s/iter; left time: 3461.5448s\n",
      "\titers: 200, epoch: 5 | loss: 0.3701926\n",
      "\tspeed: 0.0029s/iter; left time: 26.1232s\n",
      "Epoch: 5 cost time: 24.673859357833862\n",
      "Epoch: 5, Steps: 200 | Train Loss: 0.3823878 Vali Loss: 0.8511489 Test Loss: 0.5046185\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3616370\n",
      "\tspeed: 0.3809s/iter; left time: 3390.0085s\n",
      "\titers: 200, epoch: 6 | loss: 0.3285088\n",
      "\tspeed: 0.0023s/iter; left time: 20.1006s\n",
      "Epoch: 6 cost time: 24.654746055603027\n",
      "Epoch: 6, Steps: 200 | Train Loss: 0.3814039 Vali Loss: 0.8260028 Test Loss: 0.4901604\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3190453\n",
      "\tspeed: 0.4129s/iter; left time: 3592.9321s\n",
      "\titers: 200, epoch: 7 | loss: 0.3619530\n",
      "\tspeed: 0.0024s/iter; left time: 20.2443s\n",
      "Epoch: 7 cost time: 25.037441730499268\n",
      "Epoch: 7, Steps: 200 | Train Loss: 0.3809689 Vali Loss: 0.8452312 Test Loss: 0.4934424\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3453070\n",
      "\tspeed: 0.3801s/iter; left time: 3231.6217s\n",
      "\titers: 200, epoch: 8 | loss: 0.3484500\n",
      "\tspeed: 0.0025s/iter; left time: 20.8628s\n",
      "Epoch: 8 cost time: 24.692857027053833\n",
      "Epoch: 8, Steps: 200 | Train Loss: 0.3813692 Vali Loss: 0.8401949 Test Loss: 0.4965870\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3829463\n",
      "\tspeed: 0.3888s/iter; left time: 3227.8381s\n",
      "\titers: 200, epoch: 9 | loss: 0.3352991\n",
      "\tspeed: 0.0023s/iter; left time: 18.6484s\n",
      "Epoch: 9 cost time: 25.160370111465454\n",
      "Epoch: 9, Steps: 200 | Train Loss: 0.3813581 Vali Loss: 0.8493994 Test Loss: 0.4968916\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3914669\n",
      "\tspeed: 0.3799s/iter; left time: 3077.7816s\n",
      "\titers: 200, epoch: 10 | loss: 0.3940373\n",
      "\tspeed: 0.0023s/iter; left time: 18.0401s\n",
      "Epoch: 10 cost time: 24.58053994178772\n",
      "Epoch: 10, Steps: 200 | Train Loss: 0.3811495 Vali Loss: 0.8329509 Test Loss: 0.4977292\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.3649439\n",
      "\tspeed: 0.3988s/iter; left time: 3150.8984s\n",
      "\titers: 200, epoch: 11 | loss: 0.3440503\n",
      "\tspeed: 0.0023s/iter; left time: 17.9204s\n",
      "Epoch: 11 cost time: 26.396111249923706\n",
      "Epoch: 11, Steps: 200 | Train Loss: 0.3810385 Vali Loss: 0.8526033 Test Loss: 0.4979418\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, f1: 0.744 & 0.709 \n",
      "440, 440\n",
      "######################  crude_oil_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 0.3636216\n",
      "\tspeed: 0.2346s/iter; left time: 2334.2418s\n",
      "\titers: 200, epoch: 1 | loss: 0.4252815\n",
      "\tspeed: 0.0023s/iter; left time: 22.2039s\n",
      "Epoch: 1 cost time: 24.502159595489502\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.3394222 Vali Loss: 0.7322459 Test Loss: 0.3920832\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4000925\n",
      "\tspeed: 0.3790s/iter; left time: 3694.8723s\n",
      "\titers: 200, epoch: 2 | loss: 0.3211789\n",
      "\tspeed: 0.0022s/iter; left time: 21.1756s\n",
      "Epoch: 2 cost time: 24.31145691871643\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.3158242 Vali Loss: 0.7225709 Test Loss: 0.3844586\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2705851\n",
      "\tspeed: 0.3913s/iter; left time: 3736.8928s\n",
      "\titers: 200, epoch: 3 | loss: 0.2867422\n",
      "\tspeed: 0.0028s/iter; left time: 26.2016s\n",
      "Epoch: 3 cost time: 26.279030323028564\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.3095236 Vali Loss: 0.7242330 Test Loss: 0.3878854\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.3194849\n",
      "\tspeed: 0.4037s/iter; left time: 3774.1154s\n",
      "\titers: 200, epoch: 4 | loss: 0.3183167\n",
      "\tspeed: 0.0034s/iter; left time: 31.2250s\n",
      "Epoch: 4 cost time: 25.91870403289795\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.3075537 Vali Loss: 0.7274697 Test Loss: 0.3934291\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3162522\n",
      "\tspeed: 0.3731s/iter; left time: 3412.4250s\n",
      "\titers: 200, epoch: 5 | loss: 0.3101261\n",
      "\tspeed: 0.0025s/iter; left time: 23.0083s\n",
      "Epoch: 5 cost time: 23.71123456954956\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.3064397 Vali Loss: 0.7093901 Test Loss: 0.3883434\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2904770\n",
      "\tspeed: 0.3995s/iter; left time: 3573.5540s\n",
      "\titers: 200, epoch: 6 | loss: 0.3065311\n",
      "\tspeed: 0.0025s/iter; left time: 21.7032s\n",
      "Epoch: 6 cost time: 25.069129705429077\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.3060197 Vali Loss: 0.7127331 Test Loss: 0.3899024\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2732230\n",
      "\tspeed: 0.3718s/iter; left time: 3251.0913s\n",
      "\titers: 200, epoch: 7 | loss: 0.4121045\n",
      "\tspeed: 0.0024s/iter; left time: 20.6066s\n",
      "Epoch: 7 cost time: 24.336217403411865\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.3058037 Vali Loss: 0.7104926 Test Loss: 0.3905853\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2781565\n",
      "\tspeed: 0.3722s/iter; left time: 3180.4705s\n",
      "\titers: 200, epoch: 8 | loss: 0.3164793\n",
      "\tspeed: 0.0021s/iter; left time: 17.6009s\n",
      "Epoch: 8 cost time: 24.163323402404785\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.3056285 Vali Loss: 0.7200780 Test Loss: 0.3905851\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4028400\n",
      "\tspeed: 0.3725s/iter; left time: 3107.8765s\n",
      "\titers: 200, epoch: 9 | loss: 0.3880374\n",
      "\tspeed: 0.0022s/iter; left time: 18.0947s\n",
      "Epoch: 9 cost time: 23.833280324935913\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.3055335 Vali Loss: 0.7088038 Test Loss: 0.3904576\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2799068\n",
      "\tspeed: 0.3685s/iter; left time: 3000.5074s\n",
      "\titers: 200, epoch: 10 | loss: 0.2921512\n",
      "\tspeed: 0.0023s/iter; left time: 18.1693s\n",
      "Epoch: 10 cost time: 24.008238792419434\n",
      "Epoch: 10, Steps: 201 | Train Loss: 0.3055407 Vali Loss: 0.7189915 Test Loss: 0.3907468\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2885185\n",
      "\tspeed: 0.3756s/iter; left time: 2982.2460s\n",
      "\titers: 200, epoch: 11 | loss: 0.3288202\n",
      "\tspeed: 0.0024s/iter; left time: 18.8759s\n",
      "Epoch: 11 cost time: 24.331761360168457\n",
      "Epoch: 11, Steps: 201 | Train Loss: 0.3055923 Vali Loss: 0.7222898 Test Loss: 0.3907362\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2959236\n",
      "\tspeed: 0.3748s/iter; left time: 2901.2119s\n",
      "\titers: 200, epoch: 12 | loss: 0.2853683\n",
      "\tspeed: 0.0023s/iter; left time: 17.3719s\n",
      "Epoch: 12 cost time: 24.860512018203735\n",
      "Epoch: 12, Steps: 201 | Train Loss: 0.3056088 Vali Loss: 0.7146372 Test Loss: 0.3907623\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2779629\n",
      "\tspeed: 0.3855s/iter; left time: 2905.9458s\n",
      "\titers: 200, epoch: 13 | loss: 0.3249744\n",
      "\tspeed: 0.0030s/iter; left time: 22.4059s\n",
      "Epoch: 13 cost time: 25.18999457359314\n",
      "Epoch: 13, Steps: 201 | Train Loss: 0.3051968 Vali Loss: 0.7119608 Test Loss: 0.3907587\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.3122974\n",
      "\tspeed: 0.4067s/iter; left time: 2984.1716s\n",
      "\titers: 200, epoch: 14 | loss: 0.3721328\n",
      "\tspeed: 0.0024s/iter; left time: 17.0782s\n",
      "Epoch: 14 cost time: 25.031968116760254\n",
      "Epoch: 14, Steps: 201 | Train Loss: 0.3055612 Vali Loss: 0.7131653 Test Loss: 0.3907589\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "accuracy, f1: 0.729 & 0.692 \n",
      "448, 448\n",
      "######################  natural_gas_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 0.5415632\n",
      "\tspeed: 0.2364s/iter; left time: 2352.9005s\n",
      "\titers: 200, epoch: 1 | loss: 0.6909869\n",
      "\tspeed: 0.0024s/iter; left time: 23.2846s\n",
      "Epoch: 1 cost time: 24.768684148788452\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.5191352 Vali Loss: 0.6425692 Test Loss: 2.3725162\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4129009\n",
      "\tspeed: 0.3965s/iter; left time: 3865.7948s\n",
      "\titers: 200, epoch: 2 | loss: 0.5654786\n",
      "\tspeed: 0.0026s/iter; left time: 25.1194s\n",
      "Epoch: 2 cost time: 25.704981565475464\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.4846902 Vali Loss: 0.6098507 Test Loss: 2.2203066\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.5146548\n",
      "\tspeed: 0.3735s/iter; left time: 3566.3920s\n",
      "\titers: 200, epoch: 3 | loss: 0.3947835\n",
      "\tspeed: 0.0023s/iter; left time: 21.4863s\n",
      "Epoch: 3 cost time: 24.13913869857788\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.4781510 Vali Loss: 0.5353286 Test Loss: 1.9773861\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4721712\n",
      "\tspeed: 0.3839s/iter; left time: 3588.6770s\n",
      "\titers: 200, epoch: 4 | loss: 0.4578505\n",
      "\tspeed: 0.0021s/iter; left time: 19.5540s\n",
      "Epoch: 4 cost time: 23.99319100379944\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.4740844 Vali Loss: 0.5177185 Test Loss: 1.9084686\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3587133\n",
      "\tspeed: 0.4172s/iter; left time: 3816.3317s\n",
      "\titers: 200, epoch: 5 | loss: 0.4536008\n",
      "\tspeed: 0.0025s/iter; left time: 22.6474s\n",
      "Epoch: 5 cost time: 25.948873043060303\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.4735655 Vali Loss: 0.5320565 Test Loss: 1.9651006\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.6433718\n",
      "\tspeed: 0.3949s/iter; left time: 3533.0896s\n",
      "\titers: 200, epoch: 6 | loss: 0.7235935\n",
      "\tspeed: 0.0022s/iter; left time: 19.6741s\n",
      "Epoch: 6 cost time: 25.94055938720703\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.4731923 Vali Loss: 0.5352255 Test Loss: 1.9791659\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4331243\n",
      "\tspeed: 0.3822s/iter; left time: 3342.2567s\n",
      "\titers: 200, epoch: 7 | loss: 0.5923846\n",
      "\tspeed: 0.0024s/iter; left time: 20.9514s\n",
      "Epoch: 7 cost time: 24.759974718093872\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.4727681 Vali Loss: 0.5390250 Test Loss: 1.9968593\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.5426145\n",
      "\tspeed: 0.4231s/iter; left time: 3614.6846s\n",
      "\titers: 200, epoch: 8 | loss: 0.5950985\n",
      "\tspeed: 0.0024s/iter; left time: 19.8719s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 cost time: 26.886470317840576\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.4725164 Vali Loss: 0.5384447 Test Loss: 1.9978476\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4365718\n",
      "\tspeed: 0.3869s/iter; left time: 3227.5665s\n",
      "\titers: 200, epoch: 9 | loss: 0.5031409\n",
      "\tspeed: 0.0022s/iter; left time: 17.7573s\n",
      "Epoch: 9 cost time: 24.930137872695923\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.4723713 Vali Loss: 0.5414367 Test Loss: 1.9970576\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "accuracy, f1: 0.720 & 0.685 \n",
      "448, 448\n",
      "######################  corn_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1606\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.4020340\n",
      "\tspeed: 0.2319s/iter; left time: 2295.5695s\n",
      "\titers: 200, epoch: 1 | loss: 0.3829076\n",
      "\tspeed: 0.0026s/iter; left time: 25.6102s\n",
      "Epoch: 1 cost time: 24.205502033233643\n",
      "Epoch: 1, Steps: 200 | Train Loss: 0.4191712 Vali Loss: 0.3954868 Test Loss: 0.9822990\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3649648\n",
      "\tspeed: 0.3906s/iter; left time: 3789.4397s\n",
      "\titers: 200, epoch: 2 | loss: 0.3302635\n",
      "\tspeed: 0.0023s/iter; left time: 22.5584s\n",
      "Epoch: 2 cost time: 25.026959657669067\n",
      "Epoch: 2, Steps: 200 | Train Loss: 0.3985668 Vali Loss: 0.3601108 Test Loss: 0.8654332\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4140693\n",
      "\tspeed: 0.3859s/iter; left time: 3666.6786s\n",
      "\titers: 200, epoch: 3 | loss: 0.4004828\n",
      "\tspeed: 0.0030s/iter; left time: 28.1279s\n",
      "Epoch: 3 cost time: 24.92748761177063\n",
      "Epoch: 3, Steps: 200 | Train Loss: 0.3925067 Vali Loss: 0.4143865 Test Loss: 1.0317301\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4265279\n",
      "\tspeed: 0.3882s/iter; left time: 3610.7091s\n",
      "\titers: 200, epoch: 4 | loss: 0.3578810\n",
      "\tspeed: 0.0024s/iter; left time: 21.6566s\n",
      "Epoch: 4 cost time: 24.646936416625977\n",
      "Epoch: 4, Steps: 200 | Train Loss: 0.3911944 Vali Loss: 0.3980988 Test Loss: 0.9780452\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3646369\n",
      "\tspeed: 0.3897s/iter; left time: 3546.9672s\n",
      "\titers: 200, epoch: 5 | loss: 0.4002678\n",
      "\tspeed: 0.0023s/iter; left time: 20.3778s\n",
      "Epoch: 5 cost time: 24.614730834960938\n",
      "Epoch: 5, Steps: 200 | Train Loss: 0.3899976 Vali Loss: 0.3858177 Test Loss: 0.9429185\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3660639\n",
      "\tspeed: 0.3743s/iter; left time: 3331.7506s\n",
      "\titers: 200, epoch: 6 | loss: 0.4510862\n",
      "\tspeed: 0.0027s/iter; left time: 23.4360s\n",
      "Epoch: 6 cost time: 24.242719173431396\n",
      "Epoch: 6, Steps: 200 | Train Loss: 0.3895401 Vali Loss: 0.3864846 Test Loss: 0.9430829\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3507617\n",
      "\tspeed: 0.3899s/iter; left time: 3392.2157s\n",
      "\titers: 200, epoch: 7 | loss: 0.3966511\n",
      "\tspeed: 0.0023s/iter; left time: 19.9869s\n",
      "Epoch: 7 cost time: 25.463223934173584\n",
      "Epoch: 7, Steps: 200 | Train Loss: 0.3896851 Vali Loss: 0.3877868 Test Loss: 0.9470392\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.744 & 0.703 \n",
      "440, 440\n",
      "######################  coffee_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.4254616\n",
      "\tspeed: 0.2359s/iter; left time: 2347.7539s\n",
      "\titers: 200, epoch: 1 | loss: 0.8710947\n",
      "\tspeed: 0.0022s/iter; left time: 21.6144s\n",
      "Epoch: 1 cost time: 24.64991855621338\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.5264239 Vali Loss: 0.5599752 Test Loss: 0.8539894\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7880015\n",
      "\tspeed: 0.3813s/iter; left time: 3717.6644s\n",
      "\titers: 200, epoch: 2 | loss: 0.4078561\n",
      "\tspeed: 0.0023s/iter; left time: 22.1359s\n",
      "Epoch: 2 cost time: 24.231972455978394\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.4935851 Vali Loss: 0.5334582 Test Loss: 0.6626964\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4933087\n",
      "\tspeed: 0.3739s/iter; left time: 3570.6241s\n",
      "\titers: 200, epoch: 3 | loss: 0.4035379\n",
      "\tspeed: 0.0028s/iter; left time: 26.5326s\n",
      "Epoch: 3 cost time: 24.704347133636475\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.4856216 Vali Loss: 0.5425858 Test Loss: 0.7536012\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4404876\n",
      "\tspeed: 0.4040s/iter; left time: 3776.8427s\n",
      "\titers: 200, epoch: 4 | loss: 0.5035900\n",
      "\tspeed: 0.0029s/iter; left time: 26.8484s\n",
      "Epoch: 4 cost time: 25.84488534927368\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.4828672 Vali Loss: 0.5355501 Test Loss: 0.7201304\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3846228\n",
      "\tspeed: 0.3933s/iter; left time: 3597.3590s\n",
      "\titers: 200, epoch: 5 | loss: 0.4194827\n",
      "\tspeed: 0.0025s/iter; left time: 22.5426s\n",
      "Epoch: 5 cost time: 24.742058038711548\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.4817556 Vali Loss: 0.5346548 Test Loss: 0.7159836\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3650307\n",
      "\tspeed: 0.3913s/iter; left time: 3500.8185s\n",
      "\titers: 200, epoch: 6 | loss: 0.6047705\n",
      "\tspeed: 0.0028s/iter; left time: 24.4140s\n",
      "Epoch: 6 cost time: 25.028266191482544\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.4812254 Vali Loss: 0.5342893 Test Loss: 0.7008145\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4479428\n",
      "\tspeed: 0.3732s/iter; left time: 3263.9089s\n",
      "\titers: 200, epoch: 7 | loss: 0.3847212\n",
      "\tspeed: 0.0022s/iter; left time: 19.2269s\n",
      "Epoch: 7 cost time: 24.31790256500244\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.4808901 Vali Loss: 0.5356296 Test Loss: 0.7121283\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.708 & 0.731 \n",
      "440, 440\n"
     ]
    }
   ],
   "source": [
    "# D Linear\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "models = ['Autoformer', 'Informer', 'Transformer', 'DLinear', 'NLinear']\n",
    "data_paths = ['gold_data_c.csv', 'silver_data_c.csv', 'crude_oil_data_c.csv', 'natural_gas_data_c.csv', 'corn_data_c.csv', 'coffee_data_c.csv']\n",
    "label_lens = [14, 28, 42, 56]\n",
    "pred_lens = [56]\n",
    "args = dotdict()\n",
    "\n",
    "for data_path in data_paths:\n",
    "    for pred_len in pred_lens:\n",
    "\n",
    "        # basic config\n",
    "        args.is_training = 1\n",
    "        args.model_id = ''\n",
    "        args.model = 'DLinear' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "        # data loader\n",
    "        args.data = 'custom'\n",
    "        args.root_path = './dataset/commodity/'\n",
    "        args.data_path = data_path\n",
    "        args.features = 'M'\n",
    "        args.target = 'OT'\n",
    "        args.freq = 'd'\n",
    "        args.checkpoints = './checkpoints/'\n",
    "\n",
    "        # forecasting task\n",
    "        args.seq_len = 96\n",
    "        args.label_len = pred_len\n",
    "        args.pred_len = pred_len\n",
    "\n",
    "        # DLinear\n",
    "        args.individual = False\n",
    "\n",
    "        # Formers \n",
    "        args.embed_type = 0\n",
    "        args.enc_in = 6\n",
    "        args.dec_in = 6\n",
    "        args.c_out = 6\n",
    "        args.d_model = 512\n",
    "        args.n_heads = 8\n",
    "        args.e_layers = 2\n",
    "        args.d_layers = 1\n",
    "        args.d_ff = 2048\n",
    "        args.moving_avg = 25\n",
    "        args.factor = 3\n",
    "        args.distil = True\n",
    "        args.dropout = 0.05\n",
    "        args.embed = 'timeF'\n",
    "        args.do_predict = True\n",
    "\n",
    "        # optimization\n",
    "        args.num_workers = 10\n",
    "        args.itr = 1\n",
    "        args.train_epochs = 50\n",
    "        args.patience = 5\n",
    "        args.learning_rate = 0.0005\n",
    "        args.batch_size = 8\n",
    "        args.lradj = 'type1'\n",
    "        args.des = 'Exp'\n",
    "\n",
    "        # GPU\n",
    "        args.gpu = 0\n",
    "        args.devices = '0'\n",
    "        args.use_gpu = True\n",
    "        args.use_multi_gpu = False\n",
    "\n",
    "        # print('Args in experiment:')\n",
    "        # print(args)\n",
    "\n",
    "        Exp = Exp_Main\n",
    "        print(f'######################  {data_path}_{args.model}_{pred_len}_{pred_len}  ######################')\n",
    "        if args.is_training:\n",
    "            for ii in range(args.itr):\n",
    "                # setting record of experiments\n",
    "                setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                    args.data_path.split(\"_\")[0],\n",
    "                    args.model_id,\n",
    "                    args.model,\n",
    "                    args.data,\n",
    "                    args.features,\n",
    "                    args.seq_len,\n",
    "                    args.label_len,\n",
    "                    args.pred_len,\n",
    "                    args.d_model,\n",
    "                    args.n_heads,\n",
    "                    args.e_layers,\n",
    "                    args.d_layers,\n",
    "                    args.d_ff,\n",
    "                    args.factor,\n",
    "                    args.embed,\n",
    "                    args.distil,\n",
    "                    args.des, ii)\n",
    "\n",
    "                exp = Exp(args)  # set experiments\n",
    "                print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                exp.train(setting)\n",
    "\n",
    "                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.test(setting)\n",
    "\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "exp.args.root_path = './dataset/commodity/'\n",
    "exp.args.data_path = 'gold_data.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
    "df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# # df.to_csv(os.path.join(args.root_path, args.data_path), index=False) \n",
    "# last_date = df['date'].max()\n",
    "# new_rows = pd.DataFrame(columns=df.columns, index=range(1, 31))\n",
    "# new_rows['date'] = pd.date_range(start=last_date + pd.DateOffset(1), periods=30)\n",
    "# df = pd.concat([df, new_rows], sort=False)\n",
    "# df.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_predict:\n",
    "    print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    prediction=exp.predict(setting, True)\n",
    "    prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "    prediction.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_Pred\n",
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "Data = Dataset_Custom\n",
    "\n",
    "freq = 'b'\n",
    "\n",
    "Data = Dataset_Custom\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    target=args.target, # HULL here\n",
    "    freq=args.freq # 'h': hourly, 't':minutely\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)\n",
    "\n",
    "print(len(data_set), len(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.predict(setting, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "# The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "preds = np.load('./results/'+setting+'/pred.npy')\n",
    "trues = np.load('./results/'+setting+'/true.npy')\n",
    "\n",
    "# [samples, pred_len, dimensions]\n",
    "print(preds.shape, trues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "data_pred = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "data_pred = torch.from_numpy(data_pred).permute(0,2,1)\n",
    "\n",
    "plt.figure()\n",
    "print(data_pred.shape)\n",
    "#预测OT\n",
    "plt.plot(data_pred[-1,-1,:])\n",
    "print(data_pred[-1,-1,:].shape)\n",
    "plt.show()\n",
    "plt.plot(data_pred[0,-1,:])\n",
    "print(data_pred[0,-1,:].shape)\n",
    "#plt.show()\n",
    "# draw HUFL prediction\n",
    "plt.plot(data_pred[0,0,:])\n",
    "print(data_pred[-1,-1,:].shape)\n",
    "plt.show()\n",
    "'''\n",
    "Ground Truth\n",
    "'''\n",
    "data_gt = np.load('./results/'+setting+'/true.npy')\n",
    "data_gt = torch.from_numpy(data_gt).permute(0,2,1)\n",
    "\n",
    "# OT\n",
    "plt.plot(data_gt[-1,-1,:])\n",
    "print(data_gt[-1,-1,:].shape)\n",
    "plt.show()\n",
    "plt.plot(data_gt[0,-1,:])\n",
    "print(data_gt[0,-1,:].shape)\n",
    "#plt.show()\n",
    "# draw HUFL prediction\n",
    "plt.plot(data_gt[0,0,:])\n",
    "print(data_gt[-1,-1,:].shape)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'coffee_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Linear_Seasonal.weight'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      8\u001B[0m weights \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(model_path,map_location\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[0;32m      9\u001B[0m weights_list \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m---> 10\u001B[0m weights_list[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mseasonal\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mweights\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLinear_Seasonal.weight\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     11\u001B[0m weights_list[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrend\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m weights[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLinear_Trend.weight\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m#print(root)\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m#print(root.split('/'))\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'Linear_Seasonal.weight'"
     ]
    }
   ],
   "source": [
    "if (args.model == 'DLinear'):\n",
    "    model_name = setting\n",
    "    for root, dirs, files in os.walk(\"checkpoints\"):\n",
    "        for name in files:\n",
    "            model_path = os.path.join(root, name)\n",
    "            if model_name not in model_path:\n",
    "                continue\n",
    "            weights = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "            weights_list = {}\n",
    "            weights_list['seasonal'] = weights['Linear_Seasonal.weight'].numpy()\n",
    "            weights_list['trend'] = weights['Linear_Trend.weight'].numpy()\n",
    "\n",
    "            #print(root)\n",
    "            #print(root.split('/'))\n",
    "\n",
    "            save_root = 'weights_plot/%s'%root.split('\\\\')[1] #changed from save_root = 'weights_plot/%s'%root.split('/')[1]\n",
    "            if not os.path.exists('weights_plot'):\n",
    "                os.mkdir('weights_plot')\n",
    "            if not os.path.exists(save_root):\n",
    "                os.mkdir(save_root)\n",
    "        \n",
    "            for w_name,weight in weights_list.items():\n",
    "                fig,ax=plt.subplots()\n",
    "                im=ax.imshow(weight,cmap='plasma_r')\n",
    "                fig.colorbar(im,pad=0.03)\n",
    "                plt.savefig(os.path.join(save_root,w_name + '.pdf'),dpi=500)\n",
    "                plt.show()\n",
    "                plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57785bf53e86c458d31dd8512073d1ac6cae98f342ec9a1a9a8506681d63dcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
