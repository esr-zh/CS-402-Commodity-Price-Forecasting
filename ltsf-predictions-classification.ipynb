{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "\n",
    "# args.output_attention = True\n",
    "\n",
    "#exp = Exp(args)\n",
    "\n",
    "#model = exp.model\n",
    "\n",
    "#setting = 'ETTh1_96_24_Autoformer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0'\n",
    "#path = os.path.join(args.checkpoints,setting,'checkpoint.pth')\n",
    "#model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' #disable CUDA by setting CUDA_VISIBLE_DEVICES to an empty string \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In finance and stock market analysis, it's not uncommon to consider a small percentage change as \"neutral\" or \"no change\" because daily stock prices can fluctuate due to a variety of factors, including market volatility. By setting a threshold of 0.25%, you're essentially saying that any change in price less than this percentage is insignificant and should be considered neutral. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "commodities = ['gold', 'silver', 'natural_gas', 'coffee', 'corn', 'crude_oil']\n",
    "OT_dict = {0: 'Decrease', 1: 'Increase'}\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(commodities), figsize=(20, 5))\n",
    "\n",
    "\n",
    "for i, commodity in enumerate(commodities):\n",
    "    df = pd.read_csv(f'./dataset/commodity/{commodity}_data.csv')\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Make sure the 'close' column is correctly named\n",
    "    df.columns = df.columns.str.replace('OT', 'close')\n",
    "\n",
    "    # Calculate the percentage change between the closing price of the next day and the current day\n",
    "    df['close_pct_change'] = df['close'].pct_change(periods=-1) * 100\n",
    "    \n",
    "    # Create the target column based on the 'close_pct_change' column\n",
    "    df['OT'] = 0.0 # neutral\n",
    "    df.loc[df['close_pct_change'] > 0, 'OT'] = 1.0 # increase\n",
    "    # df.loc[df['close_pct_change'] < -0.25, 'OT'] = 2 # decrease\n",
    "\n",
    "    df = df.drop(columns=['close_pct_change'])\n",
    "    df = df.drop(columns=['close', 'Adj Close'])\n",
    "\n",
    "    # Drop the last row as it doesn't have a valid 'OT' value\n",
    "    df = df[:-1]\n",
    "\n",
    "    # Save the dataframe as a new CSV file\n",
    "    df.to_csv(f'./dataset/commodity/{commodity}_data_c.csv')\n",
    "    \n",
    "    sns.countplot(x='OT', data=df, ax=axes[i])\n",
    "    axes[i].set_xticks([0, 1])\n",
    "    axes[i].set_xticklabels([OT_dict[i] for i in [0, 1]])\n",
    "    axes[i].set_title(f'Distribution of {commodity} classes')\n",
    "\n",
    "# Displaying the plots\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_bar.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f'./dataset/commodity/gold_data_c.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################  gold_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.4515233\n",
      "\tspeed: 0.2572s/iter; left time: 2559.4394s\n",
      "\titers: 200, epoch: 1 | loss: 0.5054529\n",
      "\tspeed: 0.0026s/iter; left time: 25.7438s\n",
      "Epoch: 1 cost time: 26.88035297393799\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.4764939 Vali Loss: 1.4964520 Test Loss: 0.9893274\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4311605\n",
      "\tspeed: 0.3708s/iter; left time: 3615.2854s\n",
      "\titers: 200, epoch: 2 | loss: 0.5266655\n",
      "\tspeed: 0.0022s/iter; left time: 21.6547s\n",
      "Epoch: 2 cost time: 24.580622673034668\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.4432168 Vali Loss: 1.3520074 Test Loss: 0.8852887\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4864195\n",
      "\tspeed: 0.3779s/iter; left time: 3608.8218s\n",
      "\titers: 200, epoch: 3 | loss: 0.4028131\n",
      "\tspeed: 0.0024s/iter; left time: 22.3972s\n",
      "Epoch: 3 cost time: 24.560643672943115\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.4352664 Vali Loss: 1.4681276 Test Loss: 1.0421034\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4980484\n",
      "\tspeed: 0.3758s/iter; left time: 3513.2024s\n",
      "\titers: 200, epoch: 4 | loss: 0.4548787\n",
      "\tspeed: 0.0025s/iter; left time: 22.7927s\n",
      "Epoch: 4 cost time: 24.802781581878662\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.4317233 Vali Loss: 1.3828634 Test Loss: 0.9136162\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4083761\n",
      "\tspeed: 0.3761s/iter; left time: 3440.3401s\n",
      "\titers: 200, epoch: 5 | loss: 0.4057913\n",
      "\tspeed: 0.0023s/iter; left time: 20.9331s\n",
      "Epoch: 5 cost time: 24.103899478912354\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.4306567 Vali Loss: 1.3448943 Test Loss: 0.8669825\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4604294\n",
      "\tspeed: 0.3721s/iter; left time: 3329.1925s\n",
      "\titers: 200, epoch: 6 | loss: 0.3909421\n",
      "\tspeed: 0.0022s/iter; left time: 19.5877s\n",
      "Epoch: 6 cost time: 24.076597690582275\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.4299075 Vali Loss: 1.4023142 Test Loss: 0.9188581\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4006211\n",
      "\tspeed: 0.3716s/iter; left time: 3249.9856s\n",
      "\titers: 200, epoch: 7 | loss: 0.5073584\n",
      "\tspeed: 0.0026s/iter; left time: 22.3279s\n",
      "Epoch: 7 cost time: 24.56306290626526\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.4295690 Vali Loss: 1.3639027 Test Loss: 0.8951111\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.4590106\n",
      "\tspeed: 0.3750s/iter; left time: 3203.6197s\n",
      "\titers: 200, epoch: 8 | loss: 0.3677874\n",
      "\tspeed: 0.0025s/iter; left time: 21.0536s\n",
      "Epoch: 8 cost time: 24.21582293510437\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.4293980 Vali Loss: 1.3745050 Test Loss: 0.8940009\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4941946\n",
      "\tspeed: 0.3701s/iter; left time: 3088.1286s\n",
      "\titers: 200, epoch: 9 | loss: 0.4135148\n",
      "\tspeed: 0.0023s/iter; left time: 19.2371s\n",
      "Epoch: 9 cost time: 24.363267421722412\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.4293007 Vali Loss: 1.3654432 Test Loss: 0.8930917\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.4431312\n",
      "\tspeed: 0.3958s/iter; left time: 3222.3929s\n",
      "\titers: 200, epoch: 10 | loss: 0.4413811\n",
      "\tspeed: 0.0025s/iter; left time: 20.4523s\n",
      "Epoch: 10 cost time: 26.17076539993286\n",
      "Epoch: 10, Steps: 201 | Train Loss: 0.4292563 Vali Loss: 1.3724684 Test Loss: 0.8907480\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.753 & 0.698 \n",
      "440, 440\n",
      "######################  silver_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1607\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.3429714\n",
      "\tspeed: 0.2497s/iter; left time: 2471.8171s\n",
      "\titers: 200, epoch: 1 | loss: 0.4067443\n",
      "\tspeed: 0.0030s/iter; left time: 29.2272s\n",
      "Epoch: 1 cost time: 26.25163960456848\n",
      "Epoch: 1, Steps: 200 | Train Loss: 0.4136783 Vali Loss: 0.8662719 Test Loss: 0.5110341\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4954151\n",
      "\tspeed: 0.4048s/iter; left time: 3927.1110s\n",
      "\titers: 200, epoch: 2 | loss: 0.3864937\n",
      "\tspeed: 0.0029s/iter; left time: 28.0296s\n",
      "Epoch: 2 cost time: 25.739697456359863\n",
      "Epoch: 2, Steps: 200 | Train Loss: 0.3920603 Vali Loss: 0.8330175 Test Loss: 0.4579434\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4535121\n",
      "\tspeed: 0.3966s/iter; left time: 3767.7044s\n",
      "\titers: 200, epoch: 3 | loss: 0.3583914\n",
      "\tspeed: 0.0023s/iter; left time: 21.7522s\n",
      "Epoch: 3 cost time: 24.795146465301514\n",
      "Epoch: 3, Steps: 200 | Train Loss: 0.3857642 Vali Loss: 0.8420712 Test Loss: 0.4740285\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.3850468\n",
      "\tspeed: 0.4032s/iter; left time: 3749.7010s\n",
      "\titers: 200, epoch: 4 | loss: 0.3329832\n",
      "\tspeed: 0.0024s/iter; left time: 21.8018s\n",
      "Epoch: 4 cost time: 26.821147203445435\n",
      "Epoch: 4, Steps: 200 | Train Loss: 0.3834346 Vali Loss: 0.8271567 Test Loss: 0.4873953\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3739680\n",
      "\tspeed: 0.3803s/iter; left time: 3461.5448s\n",
      "\titers: 200, epoch: 5 | loss: 0.3701926\n",
      "\tspeed: 0.0029s/iter; left time: 26.1232s\n",
      "Epoch: 5 cost time: 24.673859357833862\n",
      "Epoch: 5, Steps: 200 | Train Loss: 0.3823878 Vali Loss: 0.8511489 Test Loss: 0.5046185\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3616370\n",
      "\tspeed: 0.3809s/iter; left time: 3390.0085s\n",
      "\titers: 200, epoch: 6 | loss: 0.3285088\n",
      "\tspeed: 0.0023s/iter; left time: 20.1006s\n",
      "Epoch: 6 cost time: 24.654746055603027\n",
      "Epoch: 6, Steps: 200 | Train Loss: 0.3814039 Vali Loss: 0.8260028 Test Loss: 0.4901604\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3190453\n",
      "\tspeed: 0.4129s/iter; left time: 3592.9321s\n",
      "\titers: 200, epoch: 7 | loss: 0.3619530\n",
      "\tspeed: 0.0024s/iter; left time: 20.2443s\n",
      "Epoch: 7 cost time: 25.037441730499268\n",
      "Epoch: 7, Steps: 200 | Train Loss: 0.3809689 Vali Loss: 0.8452312 Test Loss: 0.4934424\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.3453070\n",
      "\tspeed: 0.3801s/iter; left time: 3231.6217s\n",
      "\titers: 200, epoch: 8 | loss: 0.3484500\n",
      "\tspeed: 0.0025s/iter; left time: 20.8628s\n",
      "Epoch: 8 cost time: 24.692857027053833\n",
      "Epoch: 8, Steps: 200 | Train Loss: 0.3813692 Vali Loss: 0.8401949 Test Loss: 0.4965870\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.3829463\n",
      "\tspeed: 0.3888s/iter; left time: 3227.8381s\n",
      "\titers: 200, epoch: 9 | loss: 0.3352991\n",
      "\tspeed: 0.0023s/iter; left time: 18.6484s\n",
      "Epoch: 9 cost time: 25.160370111465454\n",
      "Epoch: 9, Steps: 200 | Train Loss: 0.3813581 Vali Loss: 0.8493994 Test Loss: 0.4968916\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.3914669\n",
      "\tspeed: 0.3799s/iter; left time: 3077.7816s\n",
      "\titers: 200, epoch: 10 | loss: 0.3940373\n",
      "\tspeed: 0.0023s/iter; left time: 18.0401s\n",
      "Epoch: 10 cost time: 24.58053994178772\n",
      "Epoch: 10, Steps: 200 | Train Loss: 0.3811495 Vali Loss: 0.8329509 Test Loss: 0.4977292\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.3649439\n",
      "\tspeed: 0.3988s/iter; left time: 3150.8984s\n",
      "\titers: 200, epoch: 11 | loss: 0.3440503\n",
      "\tspeed: 0.0023s/iter; left time: 17.9204s\n",
      "Epoch: 11 cost time: 26.396111249923706\n",
      "Epoch: 11, Steps: 200 | Train Loss: 0.3810385 Vali Loss: 0.8526033 Test Loss: 0.4979418\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, f1: 0.744 & 0.709 \n",
      "440, 440\n",
      "######################  crude_oil_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 0.3636216\n",
      "\tspeed: 0.2346s/iter; left time: 2334.2418s\n",
      "\titers: 200, epoch: 1 | loss: 0.4252815\n",
      "\tspeed: 0.0023s/iter; left time: 22.2039s\n",
      "Epoch: 1 cost time: 24.502159595489502\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.3394222 Vali Loss: 0.7322459 Test Loss: 0.3920832\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4000925\n",
      "\tspeed: 0.3790s/iter; left time: 3694.8723s\n",
      "\titers: 200, epoch: 2 | loss: 0.3211789\n",
      "\tspeed: 0.0022s/iter; left time: 21.1756s\n",
      "Epoch: 2 cost time: 24.31145691871643\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.3158242 Vali Loss: 0.7225709 Test Loss: 0.3844586\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.2705851\n",
      "\tspeed: 0.3913s/iter; left time: 3736.8928s\n",
      "\titers: 200, epoch: 3 | loss: 0.2867422\n",
      "\tspeed: 0.0028s/iter; left time: 26.2016s\n",
      "Epoch: 3 cost time: 26.279030323028564\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.3095236 Vali Loss: 0.7242330 Test Loss: 0.3878854\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.3194849\n",
      "\tspeed: 0.4037s/iter; left time: 3774.1154s\n",
      "\titers: 200, epoch: 4 | loss: 0.3183167\n",
      "\tspeed: 0.0034s/iter; left time: 31.2250s\n",
      "Epoch: 4 cost time: 25.91870403289795\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.3075537 Vali Loss: 0.7274697 Test Loss: 0.3934291\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3162522\n",
      "\tspeed: 0.3731s/iter; left time: 3412.4250s\n",
      "\titers: 200, epoch: 5 | loss: 0.3101261\n",
      "\tspeed: 0.0025s/iter; left time: 23.0083s\n",
      "Epoch: 5 cost time: 23.71123456954956\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.3064397 Vali Loss: 0.7093901 Test Loss: 0.3883434\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.2904770\n",
      "\tspeed: 0.3995s/iter; left time: 3573.5540s\n",
      "\titers: 200, epoch: 6 | loss: 0.3065311\n",
      "\tspeed: 0.0025s/iter; left time: 21.7032s\n",
      "Epoch: 6 cost time: 25.069129705429077\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.3060197 Vali Loss: 0.7127331 Test Loss: 0.3899024\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.2732230\n",
      "\tspeed: 0.3718s/iter; left time: 3251.0913s\n",
      "\titers: 200, epoch: 7 | loss: 0.4121045\n",
      "\tspeed: 0.0024s/iter; left time: 20.6066s\n",
      "Epoch: 7 cost time: 24.336217403411865\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.3058037 Vali Loss: 0.7104926 Test Loss: 0.3905853\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.2781565\n",
      "\tspeed: 0.3722s/iter; left time: 3180.4705s\n",
      "\titers: 200, epoch: 8 | loss: 0.3164793\n",
      "\tspeed: 0.0021s/iter; left time: 17.6009s\n",
      "Epoch: 8 cost time: 24.163323402404785\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.3056285 Vali Loss: 0.7200780 Test Loss: 0.3905851\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4028400\n",
      "\tspeed: 0.3725s/iter; left time: 3107.8765s\n",
      "\titers: 200, epoch: 9 | loss: 0.3880374\n",
      "\tspeed: 0.0022s/iter; left time: 18.0947s\n",
      "Epoch: 9 cost time: 23.833280324935913\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.3055335 Vali Loss: 0.7088038 Test Loss: 0.3904576\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.2799068\n",
      "\tspeed: 0.3685s/iter; left time: 3000.5074s\n",
      "\titers: 200, epoch: 10 | loss: 0.2921512\n",
      "\tspeed: 0.0023s/iter; left time: 18.1693s\n",
      "Epoch: 10 cost time: 24.008238792419434\n",
      "Epoch: 10, Steps: 201 | Train Loss: 0.3055407 Vali Loss: 0.7189915 Test Loss: 0.3907468\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.2885185\n",
      "\tspeed: 0.3756s/iter; left time: 2982.2460s\n",
      "\titers: 200, epoch: 11 | loss: 0.3288202\n",
      "\tspeed: 0.0024s/iter; left time: 18.8759s\n",
      "Epoch: 11 cost time: 24.331761360168457\n",
      "Epoch: 11, Steps: 201 | Train Loss: 0.3055923 Vali Loss: 0.7222898 Test Loss: 0.3907362\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.2959236\n",
      "\tspeed: 0.3748s/iter; left time: 2901.2119s\n",
      "\titers: 200, epoch: 12 | loss: 0.2853683\n",
      "\tspeed: 0.0023s/iter; left time: 17.3719s\n",
      "Epoch: 12 cost time: 24.860512018203735\n",
      "Epoch: 12, Steps: 201 | Train Loss: 0.3056088 Vali Loss: 0.7146372 Test Loss: 0.3907623\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.2779629\n",
      "\tspeed: 0.3855s/iter; left time: 2905.9458s\n",
      "\titers: 200, epoch: 13 | loss: 0.3249744\n",
      "\tspeed: 0.0030s/iter; left time: 22.4059s\n",
      "Epoch: 13 cost time: 25.18999457359314\n",
      "Epoch: 13, Steps: 201 | Train Loss: 0.3051968 Vali Loss: 0.7119608 Test Loss: 0.3907587\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.3122974\n",
      "\tspeed: 0.4067s/iter; left time: 2984.1716s\n",
      "\titers: 200, epoch: 14 | loss: 0.3721328\n",
      "\tspeed: 0.0024s/iter; left time: 17.0782s\n",
      "Epoch: 14 cost time: 25.031968116760254\n",
      "Epoch: 14, Steps: 201 | Train Loss: 0.3055612 Vali Loss: 0.7131653 Test Loss: 0.3907589\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "accuracy, f1: 0.729 & 0.692 \n",
      "448, 448\n",
      "######################  natural_gas_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 0.5415632\n",
      "\tspeed: 0.2364s/iter; left time: 2352.9005s\n",
      "\titers: 200, epoch: 1 | loss: 0.6909869\n",
      "\tspeed: 0.0024s/iter; left time: 23.2846s\n",
      "Epoch: 1 cost time: 24.768684148788452\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.5191352 Vali Loss: 0.6425692 Test Loss: 2.3725162\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.4129009\n",
      "\tspeed: 0.3965s/iter; left time: 3865.7948s\n",
      "\titers: 200, epoch: 2 | loss: 0.5654786\n",
      "\tspeed: 0.0026s/iter; left time: 25.1194s\n",
      "Epoch: 2 cost time: 25.704981565475464\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.4846902 Vali Loss: 0.6098507 Test Loss: 2.2203066\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.5146548\n",
      "\tspeed: 0.3735s/iter; left time: 3566.3920s\n",
      "\titers: 200, epoch: 3 | loss: 0.3947835\n",
      "\tspeed: 0.0023s/iter; left time: 21.4863s\n",
      "Epoch: 3 cost time: 24.13913869857788\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.4781510 Vali Loss: 0.5353286 Test Loss: 1.9773861\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4721712\n",
      "\tspeed: 0.3839s/iter; left time: 3588.6770s\n",
      "\titers: 200, epoch: 4 | loss: 0.4578505\n",
      "\tspeed: 0.0021s/iter; left time: 19.5540s\n",
      "Epoch: 4 cost time: 23.99319100379944\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.4740844 Vali Loss: 0.5177185 Test Loss: 1.9084686\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3587133\n",
      "\tspeed: 0.4172s/iter; left time: 3816.3317s\n",
      "\titers: 200, epoch: 5 | loss: 0.4536008\n",
      "\tspeed: 0.0025s/iter; left time: 22.6474s\n",
      "Epoch: 5 cost time: 25.948873043060303\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.4735655 Vali Loss: 0.5320565 Test Loss: 1.9651006\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.6433718\n",
      "\tspeed: 0.3949s/iter; left time: 3533.0896s\n",
      "\titers: 200, epoch: 6 | loss: 0.7235935\n",
      "\tspeed: 0.0022s/iter; left time: 19.6741s\n",
      "Epoch: 6 cost time: 25.94055938720703\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.4731923 Vali Loss: 0.5352255 Test Loss: 1.9791659\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4331243\n",
      "\tspeed: 0.3822s/iter; left time: 3342.2567s\n",
      "\titers: 200, epoch: 7 | loss: 0.5923846\n",
      "\tspeed: 0.0024s/iter; left time: 20.9514s\n",
      "Epoch: 7 cost time: 24.759974718093872\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.4727681 Vali Loss: 0.5390250 Test Loss: 1.9968593\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.5426145\n",
      "\tspeed: 0.4231s/iter; left time: 3614.6846s\n",
      "\titers: 200, epoch: 8 | loss: 0.5950985\n",
      "\tspeed: 0.0024s/iter; left time: 19.8719s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 cost time: 26.886470317840576\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.4725164 Vali Loss: 0.5384447 Test Loss: 1.9978476\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4365718\n",
      "\tspeed: 0.3869s/iter; left time: 3227.5665s\n",
      "\titers: 200, epoch: 9 | loss: 0.5031409\n",
      "\tspeed: 0.0022s/iter; left time: 17.7573s\n",
      "Epoch: 9 cost time: 24.930137872695923\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.4723713 Vali Loss: 0.5414367 Test Loss: 1.9970576\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "accuracy, f1: 0.720 & 0.685 \n",
      "448, 448\n",
      "######################  corn_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1606\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.4020340\n",
      "\tspeed: 0.2319s/iter; left time: 2295.5695s\n",
      "\titers: 200, epoch: 1 | loss: 0.3829076\n",
      "\tspeed: 0.0026s/iter; left time: 25.6102s\n",
      "Epoch: 1 cost time: 24.205502033233643\n",
      "Epoch: 1, Steps: 200 | Train Loss: 0.4191712 Vali Loss: 0.3954868 Test Loss: 0.9822990\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.3649648\n",
      "\tspeed: 0.3906s/iter; left time: 3789.4397s\n",
      "\titers: 200, epoch: 2 | loss: 0.3302635\n",
      "\tspeed: 0.0023s/iter; left time: 22.5584s\n",
      "Epoch: 2 cost time: 25.026959657669067\n",
      "Epoch: 2, Steps: 200 | Train Loss: 0.3985668 Vali Loss: 0.3601108 Test Loss: 0.8654332\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4140693\n",
      "\tspeed: 0.3859s/iter; left time: 3666.6786s\n",
      "\titers: 200, epoch: 3 | loss: 0.4004828\n",
      "\tspeed: 0.0030s/iter; left time: 28.1279s\n",
      "Epoch: 3 cost time: 24.92748761177063\n",
      "Epoch: 3, Steps: 200 | Train Loss: 0.3925067 Vali Loss: 0.4143865 Test Loss: 1.0317301\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4265279\n",
      "\tspeed: 0.3882s/iter; left time: 3610.7091s\n",
      "\titers: 200, epoch: 4 | loss: 0.3578810\n",
      "\tspeed: 0.0024s/iter; left time: 21.6566s\n",
      "Epoch: 4 cost time: 24.646936416625977\n",
      "Epoch: 4, Steps: 200 | Train Loss: 0.3911944 Vali Loss: 0.3980988 Test Loss: 0.9780452\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3646369\n",
      "\tspeed: 0.3897s/iter; left time: 3546.9672s\n",
      "\titers: 200, epoch: 5 | loss: 0.4002678\n",
      "\tspeed: 0.0023s/iter; left time: 20.3778s\n",
      "Epoch: 5 cost time: 24.614730834960938\n",
      "Epoch: 5, Steps: 200 | Train Loss: 0.3899976 Vali Loss: 0.3858177 Test Loss: 0.9429185\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3660639\n",
      "\tspeed: 0.3743s/iter; left time: 3331.7506s\n",
      "\titers: 200, epoch: 6 | loss: 0.4510862\n",
      "\tspeed: 0.0027s/iter; left time: 23.4360s\n",
      "Epoch: 6 cost time: 24.242719173431396\n",
      "Epoch: 6, Steps: 200 | Train Loss: 0.3895401 Vali Loss: 0.3864846 Test Loss: 0.9430829\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.3507617\n",
      "\tspeed: 0.3899s/iter; left time: 3392.2157s\n",
      "\titers: 200, epoch: 7 | loss: 0.3966511\n",
      "\tspeed: 0.0023s/iter; left time: 19.9869s\n",
      "Epoch: 7 cost time: 25.463223934173584\n",
      "Epoch: 7, Steps: 200 | Train Loss: 0.3896851 Vali Loss: 0.3877868 Test Loss: 0.9470392\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.744 & 0.703 \n",
      "440, 440\n",
      "######################  coffee_data_c.csv_DLinear_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.4254616\n",
      "\tspeed: 0.2359s/iter; left time: 2347.7539s\n",
      "\titers: 200, epoch: 1 | loss: 0.8710947\n",
      "\tspeed: 0.0022s/iter; left time: 21.6144s\n",
      "Epoch: 1 cost time: 24.64991855621338\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.5264239 Vali Loss: 0.5599752 Test Loss: 0.8539894\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7880015\n",
      "\tspeed: 0.3813s/iter; left time: 3717.6644s\n",
      "\titers: 200, epoch: 2 | loss: 0.4078561\n",
      "\tspeed: 0.0023s/iter; left time: 22.1359s\n",
      "Epoch: 2 cost time: 24.231972455978394\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.4935851 Vali Loss: 0.5334582 Test Loss: 0.6626964\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4933087\n",
      "\tspeed: 0.3739s/iter; left time: 3570.6241s\n",
      "\titers: 200, epoch: 3 | loss: 0.4035379\n",
      "\tspeed: 0.0028s/iter; left time: 26.5326s\n",
      "Epoch: 3 cost time: 24.704347133636475\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.4856216 Vali Loss: 0.5425858 Test Loss: 0.7536012\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4404876\n",
      "\tspeed: 0.4040s/iter; left time: 3776.8427s\n",
      "\titers: 200, epoch: 4 | loss: 0.5035900\n",
      "\tspeed: 0.0029s/iter; left time: 26.8484s\n",
      "Epoch: 4 cost time: 25.84488534927368\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.4828672 Vali Loss: 0.5355501 Test Loss: 0.7201304\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.3846228\n",
      "\tspeed: 0.3933s/iter; left time: 3597.3590s\n",
      "\titers: 200, epoch: 5 | loss: 0.4194827\n",
      "\tspeed: 0.0025s/iter; left time: 22.5426s\n",
      "Epoch: 5 cost time: 24.742058038711548\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.4817556 Vali Loss: 0.5346548 Test Loss: 0.7159836\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.3650307\n",
      "\tspeed: 0.3913s/iter; left time: 3500.8185s\n",
      "\titers: 200, epoch: 6 | loss: 0.6047705\n",
      "\tspeed: 0.0028s/iter; left time: 24.4140s\n",
      "Epoch: 6 cost time: 25.028266191482544\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.4812254 Vali Loss: 0.5342893 Test Loss: 0.7008145\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4479428\n",
      "\tspeed: 0.3732s/iter; left time: 3263.9089s\n",
      "\titers: 200, epoch: 7 | loss: 0.3847212\n",
      "\tspeed: 0.0022s/iter; left time: 19.2269s\n",
      "Epoch: 7 cost time: 24.31790256500244\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.4808901 Vali Loss: 0.5356296 Test Loss: 0.7121283\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee__DLinear_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.708 & 0.731 \n",
      "440, 440\n"
     ]
    }
   ],
   "source": [
    "# D Linear\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "models = ['Autoformer', 'Informer', 'Transformer', 'DLinear', 'NLinear']\n",
    "data_paths = ['gold_data_c.csv', 'silver_data_c.csv', 'crude_oil_data_c.csv', 'natural_gas_data_c.csv', 'corn_data_c.csv', 'coffee_data_c.csv']\n",
    "label_lens = [14, 28, 42, 56]\n",
    "pred_lens = [56]\n",
    "args = dotdict()\n",
    "\n",
    "for data_path in data_paths:\n",
    "    for pred_len in pred_lens:\n",
    "\n",
    "        # basic config\n",
    "        args.is_training = 1\n",
    "        args.model_id = ''\n",
    "        args.model = 'DLinear' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "        # data loader\n",
    "        args.data = 'custom'\n",
    "        args.root_path = './dataset/commodity/'\n",
    "        args.data_path = data_path\n",
    "        args.features = 'M'\n",
    "        args.target = 'OT'\n",
    "        args.freq = 'd'\n",
    "        args.checkpoints = './checkpoints/'\n",
    "\n",
    "        # forecasting task\n",
    "        args.seq_len = 96\n",
    "        args.label_len = pred_len\n",
    "        args.pred_len = pred_len\n",
    "\n",
    "        # DLinear\n",
    "        args.individual = False\n",
    "\n",
    "        # Formers \n",
    "        args.embed_type = 0\n",
    "        args.enc_in = 6\n",
    "        args.dec_in = 6\n",
    "        args.c_out = 6\n",
    "        args.d_model = 512\n",
    "        args.n_heads = 8\n",
    "        args.e_layers = 2\n",
    "        args.d_layers = 1\n",
    "        args.d_ff = 2048\n",
    "        args.moving_avg = 25\n",
    "        args.factor = 3\n",
    "        args.distil = True\n",
    "        args.dropout = 0.05\n",
    "        args.embed = 'timeF'\n",
    "        args.do_predict = True\n",
    "\n",
    "        # optimization\n",
    "        args.num_workers = 10\n",
    "        args.itr = 1\n",
    "        args.train_epochs = 50\n",
    "        args.patience = 5\n",
    "        args.learning_rate = 0.0005\n",
    "        args.batch_size = 8\n",
    "        args.lradj = 'type1'\n",
    "        args.des = 'Exp'\n",
    "\n",
    "        # GPU\n",
    "        args.gpu = 0\n",
    "        args.devices = '0'\n",
    "        args.use_gpu = True\n",
    "        args.use_multi_gpu = False\n",
    "\n",
    "        # print('Args in experiment:')\n",
    "        # print(args)\n",
    "\n",
    "        Exp = Exp_Main\n",
    "        print(f'######################  {data_path}_{args.model}_{pred_len}_{pred_len}  ######################')\n",
    "        if args.is_training:\n",
    "            for ii in range(args.itr):\n",
    "                # setting record of experiments\n",
    "                setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                    args.data_path.split(\"_\")[0],\n",
    "                    args.model_id,\n",
    "                    args.model,\n",
    "                    args.data,\n",
    "                    args.features,\n",
    "                    args.seq_len,\n",
    "                    args.label_len,\n",
    "                    args.pred_len,\n",
    "                    args.d_model,\n",
    "                    args.n_heads,\n",
    "                    args.e_layers,\n",
    "                    args.d_layers,\n",
    "                    args.d_ff,\n",
    "                    args.factor,\n",
    "                    args.embed,\n",
    "                    args.distil,\n",
    "                    args.des, ii)\n",
    "\n",
    "                exp = Exp(args)  # set experiments\n",
    "                print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                exp.train(setting)\n",
    "\n",
    "                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.test(setting)\n",
    "\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################  gold_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.7150675\n",
      "\tspeed: 0.2876s/iter; left time: 2933.9672s\n",
      "\titers: 200, epoch: 1 | loss: 1.0009717\n",
      "\tspeed: 0.0523s/iter; left time: 527.7991s\n",
      "Epoch: 1 cost time: 35.08281636238098\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.7102846 Vali Loss: 5.8019609 Test Loss: 8.5066147\n",
      "Validation loss decreased (inf --> 5.801961).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6824962\n",
      "\tspeed: 0.4688s/iter; left time: 4685.9951s\n",
      "\titers: 200, epoch: 2 | loss: 0.5746744\n",
      "\tspeed: 0.0523s/iter; left time: 517.6164s\n",
      "Epoch: 2 cost time: 35.66663384437561\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.7020987 Vali Loss: 5.6443119 Test Loss: 8.4638128\n",
      "Validation loss decreased (5.801961 --> 5.644312).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7148576\n",
      "\tspeed: 0.4744s/iter; left time: 4643.9161s\n",
      "\titers: 200, epoch: 3 | loss: 0.6398650\n",
      "\tspeed: 0.0529s/iter; left time: 512.7099s\n",
      "Epoch: 3 cost time: 36.14379286766052\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.6855648 Vali Loss: 5.7093654 Test Loss: 8.4993229\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4827015\n",
      "\tspeed: 0.5037s/iter; left time: 4826.6125s\n",
      "\titers: 200, epoch: 4 | loss: 0.5259620\n",
      "\tspeed: 0.0539s/iter; left time: 511.2567s\n",
      "Epoch: 4 cost time: 38.8472695350647\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.6806992 Vali Loss: 5.7225533 Test Loss: 8.4656172\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5274728\n",
      "\tspeed: 0.5085s/iter; left time: 4768.2888s\n",
      "\titers: 200, epoch: 5 | loss: 0.9052879\n",
      "\tspeed: 0.0551s/iter; left time: 511.0581s\n",
      "Epoch: 5 cost time: 38.35389494895935\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.6768525 Vali Loss: 5.7834997 Test Loss: 8.4701185\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5503940\n",
      "\tspeed: 0.5178s/iter; left time: 4749.0074s\n",
      "\titers: 200, epoch: 6 | loss: 0.5321650\n",
      "\tspeed: 0.0584s/iter; left time: 529.7976s\n",
      "Epoch: 6 cost time: 40.114503383636475\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.6739516 Vali Loss: 5.6728654 Test Loss: 8.4779854\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6875799\n",
      "\tspeed: 0.5072s/iter; left time: 4547.0597s\n",
      "\titers: 200, epoch: 7 | loss: 0.4752099\n",
      "\tspeed: 0.0578s/iter; left time: 512.4128s\n",
      "Epoch: 7 cost time: 38.05297636985779\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.6720031 Vali Loss: 5.6672688 Test Loss: 8.4728003\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "( accuracy, f1: 0.714 & 0.714 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.291 & 0.183 \n",
      "######################  gold_data_c.csv_Autoformer_14_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1636\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.7995639\n",
      "\tspeed: 0.3070s/iter; left time: 3101.3172s\n",
      "\titers: 200, epoch: 1 | loss: 0.6097545\n",
      "\tspeed: 0.0574s/iter; left time: 574.4796s\n",
      "Epoch: 1 cost time: 37.491429567337036\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.6922429 Vali Loss: 5.7467380 Test Loss: 8.5101166\n",
      "Validation loss decreased (inf --> 5.746738).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6137936\n",
      "\tspeed: 0.5119s/iter; left time: 5065.8698s\n",
      "\titers: 200, epoch: 2 | loss: 0.6800414\n",
      "\tspeed: 0.0597s/iter; left time: 584.7358s\n",
      "Epoch: 2 cost time: 37.84688663482666\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.6784469 Vali Loss: 5.7042680 Test Loss: 8.5154037\n",
      "Validation loss decreased (5.746738 --> 5.704268).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4946167\n",
      "\tspeed: 0.5064s/iter; left time: 4908.0524s\n",
      "\titers: 200, epoch: 3 | loss: 0.5005394\n",
      "\tspeed: 0.0625s/iter; left time: 599.8216s\n",
      "Epoch: 3 cost time: 38.48580718040466\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.6672762 Vali Loss: 5.7495799 Test Loss: 8.5794630\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5862250\n",
      "\tspeed: 0.5029s/iter; left time: 4771.8340s\n",
      "\titers: 200, epoch: 4 | loss: 0.6132087\n",
      "\tspeed: 0.0578s/iter; left time: 542.3349s\n",
      "Epoch: 4 cost time: 38.03922367095947\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.6614301 Vali Loss: 5.7065711 Test Loss: 8.5174322\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6767598\n",
      "\tspeed: 0.4920s/iter; left time: 4568.4311s\n",
      "\titers: 200, epoch: 5 | loss: 0.5329231\n",
      "\tspeed: 0.0563s/iter; left time: 517.1494s\n",
      "Epoch: 5 cost time: 37.370739221572876\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.6554543 Vali Loss: 5.6896615 Test Loss: 8.5130672\n",
      "Validation loss decreased (5.704268 --> 5.689662).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7790090\n",
      "\tspeed: 0.5221s/iter; left time: 4741.2192s\n",
      "\titers: 200, epoch: 6 | loss: 0.6748101\n",
      "\tspeed: 0.0616s/iter; left time: 553.2591s\n",
      "Epoch: 6 cost time: 39.51206040382385\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.6519001 Vali Loss: 5.6851096 Test Loss: 8.5376759\n",
      "Validation loss decreased (5.689662 --> 5.685110).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5944937\n",
      "\tspeed: 0.5288s/iter; left time: 4694.0877s\n",
      "\titers: 200, epoch: 7 | loss: 0.6763533\n",
      "\tspeed: 0.0588s/iter; left time: 516.0567s\n",
      "Epoch: 7 cost time: 39.476048707962036\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.6504488 Vali Loss: 5.7213778 Test Loss: 8.5230341\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.7303656\n",
      "\tspeed: 0.5213s/iter; left time: 4521.5659s\n",
      "\titers: 200, epoch: 8 | loss: 0.7944818\n",
      "\tspeed: 0.0580s/iter; left time: 497.0754s\n",
      "Epoch: 8 cost time: 39.69068002700806\n",
      "Epoch: 8, Steps: 204 | Train Loss: 0.6477830 Vali Loss: 5.7054801 Test Loss: 8.5126448\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.5573787\n",
      "\tspeed: 0.4981s/iter; left time: 4218.1459s\n",
      "\titers: 200, epoch: 9 | loss: 0.7136486\n",
      "\tspeed: 0.0580s/iter; left time: 485.0201s\n",
      "Epoch: 9 cost time: 37.85344862937927\n",
      "Epoch: 9, Steps: 204 | Train Loss: 0.6473111 Vali Loss: 5.7050385 Test Loss: 8.5124607\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.7239680\n",
      "\tspeed: 0.4996s/iter; left time: 4129.4396s\n",
      "\titers: 200, epoch: 10 | loss: 0.6474157\n",
      "\tspeed: 0.0583s/iter; left time: 475.9857s\n",
      "Epoch: 10 cost time: 37.54504060745239\n",
      "Epoch: 10, Steps: 204 | Train Loss: 0.6467489 Vali Loss: 5.6649437 Test Loss: 8.5093012\n",
      "Validation loss decreased (5.685110 --> 5.664944).  Saving model ...\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.6280564\n",
      "\tspeed: 0.5166s/iter; left time: 4164.3278s\n",
      "\titers: 200, epoch: 11 | loss: 0.7354674\n",
      "\tspeed: 0.0600s/iter; left time: 477.4916s\n",
      "Epoch: 11 cost time: 39.25749897956848\n",
      "Epoch: 11, Steps: 204 | Train Loss: 0.6475700 Vali Loss: 5.7115326 Test Loss: 8.5100412\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.5416046\n",
      "\tspeed: 0.5374s/iter; left time: 4222.1670s\n",
      "\titers: 200, epoch: 12 | loss: 0.6141921\n",
      "\tspeed: 0.0611s/iter; left time: 474.2737s\n",
      "Epoch: 12 cost time: 41.589168548583984\n",
      "Epoch: 12, Steps: 204 | Train Loss: 0.6465495 Vali Loss: 5.6725001 Test Loss: 8.5099897\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.7773773\n",
      "\tspeed: 0.5701s/iter; left time: 4362.9332s\n",
      "\titers: 200, epoch: 13 | loss: 0.9000143\n",
      "\tspeed: 0.0662s/iter; left time: 500.3500s\n",
      "Epoch: 13 cost time: 42.28160381317139\n",
      "Epoch: 13, Steps: 204 | Train Loss: 0.6467532 Vali Loss: 5.6732059 Test Loss: 8.5102510\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 14 | loss: 0.5757647\n",
      "\tspeed: 0.5506s/iter; left time: 4101.3575s\n",
      "\titers: 200, epoch: 14 | loss: 0.5699505\n",
      "\tspeed: 0.0601s/iter; left time: 441.7375s\n",
      "Epoch: 14 cost time: 40.53955602645874\n",
      "Epoch: 14, Steps: 204 | Train Loss: 0.6461241 Vali Loss: 5.6951518 Test Loss: 8.5104895\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.6900135\n",
      "\tspeed: 0.5100s/iter; left time: 3695.2613s\n",
      "\titers: 200, epoch: 15 | loss: 0.7408783\n",
      "\tspeed: 0.0620s/iter; left time: 443.3261s\n",
      "Epoch: 15 cost time: 38.941726207733154\n",
      "Epoch: 15, Steps: 204 | Train Loss: 0.6469281 Vali Loss: 5.6701417 Test Loss: 8.5104685\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "( accuracy, f1: 0.690 & 0.711 \n",
      "472, 472\n",
      "79296, 79296\n",
      " accuracy, f1: 0.273 & 0.150 \n",
      "######################  gold_data_c.csv_Autoformer_14_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1622\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 0.5062157\n",
      "\tspeed: 0.3481s/iter; left time: 3481.5765s\n",
      "\titers: 200, epoch: 1 | loss: 0.6877006\n",
      "\tspeed: 0.0629s/iter; left time: 623.0249s\n",
      "Epoch: 1 cost time: 42.22383403778076\n",
      "Epoch: 1, Steps: 202 | Train Loss: 0.6680740 Vali Loss: 5.7487817 Test Loss: 8.5841732\n",
      "Validation loss decreased (inf --> 5.748782).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6828619\n",
      "\tspeed: 0.4939s/iter; left time: 4839.4570s\n",
      "\titers: 200, epoch: 2 | loss: 0.5513708\n",
      "\tspeed: 0.0613s/iter; left time: 594.6133s\n",
      "Epoch: 2 cost time: 37.63430595397949\n",
      "Epoch: 2, Steps: 202 | Train Loss: 0.6652223 Vali Loss: 5.7789760 Test Loss: 8.5964785\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4646416\n",
      "\tspeed: 0.5052s/iter; left time: 4848.2836s\n",
      "\titers: 200, epoch: 3 | loss: 1.1347950\n",
      "\tspeed: 0.0658s/iter; left time: 624.7865s\n",
      "Epoch: 3 cost time: 38.98834156990051\n",
      "Epoch: 3, Steps: 202 | Train Loss: 0.6447674 Vali Loss: 5.7800651 Test Loss: 8.8075733\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.7751573\n",
      "\tspeed: 0.5255s/iter; left time: 4937.0204s\n",
      "\titers: 200, epoch: 4 | loss: 0.5640636\n",
      "\tspeed: 0.0611s/iter; left time: 567.9533s\n",
      "Epoch: 4 cost time: 39.60297083854675\n",
      "Epoch: 4, Steps: 202 | Train Loss: 0.6428111 Vali Loss: 5.7451105 Test Loss: 8.6219215\n",
      "Validation loss decreased (5.748782 --> 5.745111).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6568892\n",
      "\tspeed: 0.5440s/iter; left time: 5000.7119s\n",
      "\titers: 200, epoch: 5 | loss: 0.6605738\n",
      "\tspeed: 0.0678s/iter; left time: 616.3807s\n",
      "Epoch: 5 cost time: 42.217613220214844\n",
      "Epoch: 5, Steps: 202 | Train Loss: 0.6485183 Vali Loss: 5.7403612 Test Loss: 8.6102781\n",
      "Validation loss decreased (5.745111 --> 5.740361).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7177969\n",
      "\tspeed: 0.5325s/iter; left time: 4787.8941s\n",
      "\titers: 200, epoch: 6 | loss: 0.4900132\n",
      "\tspeed: 0.0645s/iter; left time: 573.2602s\n",
      "Epoch: 6 cost time: 40.054558515548706\n",
      "Epoch: 6, Steps: 202 | Train Loss: 0.6437844 Vali Loss: 5.6955719 Test Loss: 8.6085911\n",
      "Validation loss decreased (5.740361 --> 5.695572).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5193800\n",
      "\tspeed: 0.5283s/iter; left time: 4643.6147s\n",
      "\titers: 200, epoch: 7 | loss: 0.5606427\n",
      "\tspeed: 0.0637s/iter; left time: 553.5631s\n",
      "Epoch: 7 cost time: 40.2153902053833\n",
      "Epoch: 7, Steps: 202 | Train Loss: 0.6406462 Vali Loss: 5.7131200 Test Loss: 8.6214409\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.7383446\n",
      "\tspeed: 0.5274s/iter; left time: 4528.9609s\n",
      "\titers: 200, epoch: 8 | loss: 0.7047855\n",
      "\tspeed: 0.0605s/iter; left time: 513.7345s\n",
      "Epoch: 8 cost time: 39.53958559036255\n",
      "Epoch: 8, Steps: 202 | Train Loss: 0.6386641 Vali Loss: 5.7742405 Test Loss: 8.6307478\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.5069928\n",
      "\tspeed: 0.5087s/iter; left time: 4265.0826s\n",
      "\titers: 200, epoch: 9 | loss: 0.6454304\n",
      "\tspeed: 0.0646s/iter; left time: 535.6168s\n",
      "Epoch: 9 cost time: 39.86376667022705\n",
      "Epoch: 9, Steps: 202 | Train Loss: 0.6371387 Vali Loss: 5.7016973 Test Loss: 8.6248302\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.7549148\n",
      "\tspeed: 0.5132s/iter; left time: 4199.1679s\n",
      "\titers: 200, epoch: 10 | loss: 0.7972682\n",
      "\tspeed: 0.0590s/iter; left time: 477.1259s\n",
      "Epoch: 10 cost time: 38.17651391029358\n",
      "Epoch: 10, Steps: 202 | Train Loss: 0.6358955 Vali Loss: 5.7260995 Test Loss: 8.6197653\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.5524803\n",
      "\tspeed: 0.5120s/iter; left time: 4086.4527s\n",
      "\titers: 200, epoch: 11 | loss: 0.7388567\n",
      "\tspeed: 0.0592s/iter; left time: 466.7391s\n",
      "Epoch: 11 cost time: 39.512548208236694\n",
      "Epoch: 11, Steps: 202 | Train Loss: 0.6369067 Vali Loss: 5.7003126 Test Loss: 8.6197472\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "( accuracy, f1: 0.710 & 0.746 \n",
      "456, 456\n",
      "114912, 114912\n",
      " accuracy, f1: 0.299 & 0.199 \n",
      "######################  gold_data_c.csv_Autoformer_14_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.6104003\n",
      "\tspeed: 0.3112s/iter; left time: 3096.8531s\n",
      "\titers: 200, epoch: 1 | loss: 0.6377038\n",
      "\tspeed: 0.0637s/iter; left time: 627.1007s\n",
      "Epoch: 1 cost time: 38.37746477127075\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.6595956 Vali Loss: 5.6788268 Test Loss: 8.5819178\n",
      "Validation loss decreased (inf --> 5.678827).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6487120\n",
      "\tspeed: 0.4993s/iter; left time: 4868.1885s\n",
      "\titers: 200, epoch: 2 | loss: 0.6775471\n",
      "\tspeed: 0.0639s/iter; left time: 616.4961s\n",
      "Epoch: 2 cost time: 38.62257623672485\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.6313999 Vali Loss: 5.8768115 Test Loss: 8.7270412\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6484728\n",
      "\tspeed: 0.5006s/iter; left time: 4780.1610s\n",
      "\titers: 200, epoch: 3 | loss: 0.4314429\n",
      "\tspeed: 0.0640s/iter; left time: 605.1796s\n",
      "Epoch: 3 cost time: 38.78727912902832\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.6003618 Vali Loss: 5.6605339 Test Loss: 8.6062593\n",
      "Validation loss decreased (5.678827 --> 5.660534).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6117454\n",
      "\tspeed: 0.5098s/iter; left time: 4765.5739s\n",
      "\titers: 200, epoch: 4 | loss: 0.6435603\n",
      "\tspeed: 0.0657s/iter; left time: 607.5438s\n",
      "Epoch: 4 cost time: 39.32303285598755\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.5814021 Vali Loss: 5.8453903 Test Loss: 8.7027216\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5944253\n",
      "\tspeed: 0.5358s/iter; left time: 4900.9620s\n",
      "\titers: 200, epoch: 5 | loss: 0.4604311\n",
      "\tspeed: 0.0665s/iter; left time: 601.9062s\n",
      "Epoch: 5 cost time: 41.12275838851929\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.5717362 Vali Loss: 5.8996606 Test Loss: 8.6923275\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4581249\n",
      "\tspeed: 0.5399s/iter; left time: 4830.0090s\n",
      "\titers: 200, epoch: 6 | loss: 0.4821383\n",
      "\tspeed: 0.0671s/iter; left time: 593.9263s\n",
      "Epoch: 6 cost time: 41.40515851974487\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.5672471 Vali Loss: 5.8246007 Test Loss: 8.6752987\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4879823\n",
      "\tspeed: 0.5432s/iter; left time: 4749.8482s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 7 | loss: 0.6741790\n",
      "\tspeed: 0.0671s/iter; left time: 579.9081s\n",
      "Epoch: 7 cost time: 41.78963088989258\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.5651924 Vali Loss: 5.8021598 Test Loss: 8.7012186\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.6579133\n",
      "\tspeed: 0.5444s/iter; left time: 4651.3574s\n",
      "\titers: 200, epoch: 8 | loss: 0.5703865\n",
      "\tspeed: 0.0679s/iter; left time: 573.4148s\n",
      "Epoch: 8 cost time: 41.866419076919556\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.5639870 Vali Loss: 5.8640161 Test Loss: 8.6802149\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "( accuracy, f1: 0.708 & 0.742 \n",
      "440, 440\n",
      "147840, 147840\n",
      " accuracy, f1: 0.275 & 0.159 \n",
      "######################  silver_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1649\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.6963052\n",
      "\tspeed: 0.3368s/iter; left time: 3435.9939s\n",
      "\titers: 200, epoch: 1 | loss: 0.5963148\n",
      "\tspeed: 0.0579s/iter; left time: 584.9458s\n",
      "Epoch: 1 cost time: 40.714534282684326\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.6438144 Vali Loss: 1.1385685 Test Loss: 2.0080717\n",
      "Validation loss decreased (inf --> 1.138569).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7484682\n",
      "\tspeed: 0.5393s/iter; left time: 5390.7417s\n",
      "\titers: 200, epoch: 2 | loss: 0.8327875\n",
      "\tspeed: 0.0571s/iter; left time: 564.5112s\n",
      "Epoch: 2 cost time: 39.980525732040405\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.6358083 Vali Loss: 1.1016949 Test Loss: 1.9495727\n",
      "Validation loss decreased (1.138569 --> 1.101695).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6351675\n",
      "\tspeed: 0.5388s/iter; left time: 5274.7970s\n",
      "\titers: 200, epoch: 3 | loss: 0.4977731\n",
      "\tspeed: 0.0573s/iter; left time: 554.8576s\n",
      "Epoch: 3 cost time: 40.21639013290405\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.6269839 Vali Loss: 1.0845183 Test Loss: 1.9573058\n",
      "Validation loss decreased (1.101695 --> 1.084518).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5307012\n",
      "\tspeed: 0.5371s/iter; left time: 5147.2013s\n",
      "\titers: 200, epoch: 4 | loss: 0.6409451\n",
      "\tspeed: 0.0579s/iter; left time: 548.7175s\n",
      "Epoch: 4 cost time: 40.103293895721436\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.6181943 Vali Loss: 1.0803009 Test Loss: 1.9721799\n",
      "Validation loss decreased (1.084518 --> 1.080301).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4369963\n",
      "\tspeed: 0.5385s/iter; left time: 5049.4202s\n",
      "\titers: 200, epoch: 5 | loss: 0.7249206\n",
      "\tspeed: 0.0585s/iter; left time: 542.3250s\n",
      "Epoch: 5 cost time: 40.070927143096924\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.6149467 Vali Loss: 1.1093593 Test Loss: 1.9691010\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.6032799\n",
      "\tspeed: 0.5369s/iter; left time: 4923.7047s\n",
      "\titers: 200, epoch: 6 | loss: 0.6025329\n",
      "\tspeed: 0.0570s/iter; left time: 517.3320s\n",
      "Epoch: 6 cost time: 40.04897165298462\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.6121274 Vali Loss: 1.1054856 Test Loss: 1.9697568\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.7430208\n",
      "\tspeed: 0.5350s/iter; left time: 4796.6542s\n",
      "\titers: 200, epoch: 7 | loss: 0.6506736\n",
      "\tspeed: 0.0571s/iter; left time: 506.0803s\n",
      "Epoch: 7 cost time: 39.892706632614136\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.6110579 Vali Loss: 1.0963438 Test Loss: 1.9713278\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.6213113\n",
      "\tspeed: 0.5382s/iter; left time: 4714.2034s\n",
      "\titers: 200, epoch: 8 | loss: 0.5441282\n",
      "\tspeed: 0.0576s/iter; left time: 498.9821s\n",
      "Epoch: 8 cost time: 40.301586627960205\n",
      "Epoch: 8, Steps: 206 | Train Loss: 0.6089957 Vali Loss: 1.0804033 Test Loss: 1.9715687\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.5736638\n",
      "\tspeed: 0.5398s/iter; left time: 4617.1940s\n",
      "\titers: 200, epoch: 9 | loss: 0.5487615\n",
      "\tspeed: 0.0580s/iter; left time: 490.6513s\n",
      "Epoch: 9 cost time: 40.34981060028076\n",
      "Epoch: 9, Steps: 206 | Train Loss: 0.6094511 Vali Loss: 1.0975096 Test Loss: 1.9712942\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "( accuracy, f1: 0.696 & 0.721 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.395 & 0.359 \n",
      "######################  silver_data_c.csv_Autoformer_14_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1635\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.4084159\n",
      "\tspeed: 0.3334s/iter; left time: 3367.9814s\n",
      "\titers: 200, epoch: 1 | loss: 0.4705709\n",
      "\tspeed: 0.0607s/iter; left time: 607.1348s\n",
      "Epoch: 1 cost time: 40.57355546951294\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.6306878 Vali Loss: 1.1276259 Test Loss: 2.0156813\n",
      "Validation loss decreased (inf --> 1.127626).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.5916599\n",
      "\tspeed: 0.5469s/iter; left time: 5412.5420s\n",
      "\titers: 200, epoch: 2 | loss: 0.6060092\n",
      "\tspeed: 0.0627s/iter; left time: 614.3491s\n",
      "Epoch: 2 cost time: 41.259207248687744\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.6228817 Vali Loss: 1.0905060 Test Loss: 1.9817575\n",
      "Validation loss decreased (1.127626 --> 1.090506).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6302710\n",
      "\tspeed: 0.5457s/iter; left time: 5289.4705s\n",
      "\titers: 200, epoch: 3 | loss: 0.6093452\n",
      "\tspeed: 0.0620s/iter; left time: 594.8051s\n",
      "Epoch: 3 cost time: 40.95103454589844\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.6106755 Vali Loss: 1.1045433 Test Loss: 1.9939135\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5849006\n",
      "\tspeed: 0.5456s/iter; left time: 5177.6581s\n",
      "\titers: 200, epoch: 4 | loss: 0.7253177\n",
      "\tspeed: 0.0622s/iter; left time: 584.2230s\n",
      "Epoch: 4 cost time: 41.08782649040222\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.6022460 Vali Loss: 1.1555437 Test Loss: 2.0455849\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7169650\n",
      "\tspeed: 0.5446s/iter; left time: 5056.9276s\n",
      "\titers: 200, epoch: 5 | loss: 0.5741588\n",
      "\tspeed: 0.0624s/iter; left time: 573.3615s\n",
      "Epoch: 5 cost time: 41.07814121246338\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.5972184 Vali Loss: 1.1348722 Test Loss: 2.0440447\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5850267\n",
      "\tspeed: 0.5485s/iter; left time: 4980.6052s\n",
      "\titers: 200, epoch: 6 | loss: 0.6239333\n",
      "\tspeed: 0.0617s/iter; left time: 554.1555s\n",
      "Epoch: 6 cost time: 41.239349365234375\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.5931569 Vali Loss: 1.1355766 Test Loss: 2.0380394\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4555296\n",
      "\tspeed: 0.5442s/iter; left time: 4831.1044s\n",
      "\titers: 200, epoch: 7 | loss: 0.4944690\n",
      "\tspeed: 0.0616s/iter; left time: 540.5773s\n",
      "Epoch: 7 cost time: 41.038777112960815\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.5913229 Vali Loss: 1.1245769 Test Loss: 2.0404565\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "( accuracy, f1: 0.702 & 0.725 \n",
      "472, 472\n",
      "79296, 79296\n",
      " accuracy, f1: 0.407 & 0.376 \n",
      "######################  silver_data_c.csv_Autoformer_14_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1621\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 0.6242871\n",
      "\tspeed: 0.3402s/iter; left time: 3402.2626s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 1 | loss: 0.5427881\n",
      "\tspeed: 0.0643s/iter; left time: 636.5045s\n",
      "Epoch: 1 cost time: 41.497105836868286\n",
      "Epoch: 1, Steps: 202 | Train Loss: 0.6104436 Vali Loss: 1.0630281 Test Loss: 2.0787911\n",
      "Validation loss decreased (inf --> 1.063028).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7407855\n",
      "\tspeed: 0.5492s/iter; left time: 5381.2919s\n",
      "\titers: 200, epoch: 2 | loss: 0.6384847\n",
      "\tspeed: 0.0648s/iter; left time: 628.3207s\n",
      "Epoch: 2 cost time: 41.544090270996094\n",
      "Epoch: 2, Steps: 202 | Train Loss: 0.5916304 Vali Loss: 1.0803471 Test Loss: 2.0822527\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.5769884\n",
      "\tspeed: 0.5467s/iter; left time: 5246.2958s\n",
      "\titers: 200, epoch: 3 | loss: 0.5404185\n",
      "\tspeed: 0.0646s/iter; left time: 613.5083s\n",
      "Epoch: 3 cost time: 41.4304575920105\n",
      "Epoch: 3, Steps: 202 | Train Loss: 0.5744320 Vali Loss: 1.0797876 Test Loss: 2.1002657\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6117943\n",
      "\tspeed: 0.5480s/iter; left time: 5148.0174s\n",
      "\titers: 200, epoch: 4 | loss: 0.5730646\n",
      "\tspeed: 0.0642s/iter; left time: 596.5180s\n",
      "Epoch: 4 cost time: 41.603121757507324\n",
      "Epoch: 4, Steps: 202 | Train Loss: 0.5658066 Vali Loss: 1.0581224 Test Loss: 2.0903203\n",
      "Validation loss decreased (1.063028 --> 1.058122).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4898716\n",
      "\tspeed: 0.5499s/iter; left time: 5054.8083s\n",
      "\titers: 200, epoch: 5 | loss: 0.6563530\n",
      "\tspeed: 0.0642s/iter; left time: 584.1547s\n",
      "Epoch: 5 cost time: 41.45996379852295\n",
      "Epoch: 5, Steps: 202 | Train Loss: 0.5633957 Vali Loss: 1.0819151 Test Loss: 2.1221910\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5522218\n",
      "\tspeed: 0.5485s/iter; left time: 4931.7564s\n",
      "\titers: 200, epoch: 6 | loss: 0.6201929\n",
      "\tspeed: 0.0643s/iter; left time: 571.3072s\n",
      "Epoch: 6 cost time: 41.5259370803833\n",
      "Epoch: 6, Steps: 202 | Train Loss: 0.5595151 Vali Loss: 1.0518451 Test Loss: 2.0887384\n",
      "Validation loss decreased (1.058122 --> 1.051845).  Saving model ...\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6741828\n",
      "\tspeed: 0.5493s/iter; left time: 4828.1449s\n",
      "\titers: 200, epoch: 7 | loss: 0.5306873\n",
      "\tspeed: 0.0646s/iter; left time: 561.6756s\n",
      "Epoch: 7 cost time: 41.71334958076477\n",
      "Epoch: 7, Steps: 202 | Train Loss: 0.5585858 Vali Loss: 1.0422889 Test Loss: 2.1034977\n",
      "Validation loss decreased (1.051845 --> 1.042289).  Saving model ...\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.5542622\n",
      "\tspeed: 0.5494s/iter; left time: 4717.4976s\n",
      "\titers: 200, epoch: 8 | loss: 0.4258674\n",
      "\tspeed: 0.0644s/iter; left time: 546.5073s\n",
      "Epoch: 8 cost time: 41.56245017051697\n",
      "Epoch: 8, Steps: 202 | Train Loss: 0.5576538 Vali Loss: 1.0415595 Test Loss: 2.1009822\n",
      "Validation loss decreased (1.042289 --> 1.041559).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.4879858\n",
      "\tspeed: 0.5497s/iter; left time: 4609.3993s\n",
      "\titers: 200, epoch: 9 | loss: 0.4871616\n",
      "\tspeed: 0.0648s/iter; left time: 536.4847s\n",
      "Epoch: 9 cost time: 41.75076389312744\n",
      "Epoch: 9, Steps: 202 | Train Loss: 0.5568014 Vali Loss: 1.0365934 Test Loss: 2.1128111\n",
      "Validation loss decreased (1.041559 --> 1.036593).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.5840229\n",
      "\tspeed: 0.5496s/iter; left time: 4497.7135s\n",
      "\titers: 200, epoch: 10 | loss: 0.4964562\n",
      "\tspeed: 0.0682s/iter; left time: 551.2864s\n",
      "Epoch: 10 cost time: 42.13100004196167\n",
      "Epoch: 10, Steps: 202 | Train Loss: 0.5580297 Vali Loss: 1.0590640 Test Loss: 2.1159303\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.5543982\n",
      "\tspeed: 0.5588s/iter; left time: 4459.8067s\n",
      "\titers: 200, epoch: 11 | loss: 0.5062497\n",
      "\tspeed: 0.0650s/iter; left time: 511.9982s\n",
      "Epoch: 11 cost time: 42.0298969745636\n",
      "Epoch: 11, Steps: 202 | Train Loss: 0.5564212 Vali Loss: 1.0469130 Test Loss: 2.1138504\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.4690359\n",
      "\tspeed: 0.5550s/iter; left time: 4317.2285s\n",
      "\titers: 200, epoch: 12 | loss: 0.6357045\n",
      "\tspeed: 0.0650s/iter; left time: 499.1138s\n",
      "Epoch: 12 cost time: 41.90164279937744\n",
      "Epoch: 12, Steps: 202 | Train Loss: 0.5572225 Vali Loss: 1.0563679 Test Loss: 2.1120884\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.4841192\n",
      "\tspeed: 0.5560s/iter; left time: 4212.7589s\n",
      "\titers: 200, epoch: 13 | loss: 0.5554536\n",
      "\tspeed: 0.0650s/iter; left time: 486.0505s\n",
      "Epoch: 13 cost time: 42.160253047943115\n",
      "Epoch: 13, Steps: 202 | Train Loss: 0.5572696 Vali Loss: 1.0462167 Test Loss: 2.1127965\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.5528218\n",
      "\tspeed: 0.5583s/iter; left time: 4117.8167s\n",
      "\titers: 200, epoch: 14 | loss: 0.6641735\n",
      "\tspeed: 0.0691s/iter; left time: 502.8868s\n",
      "Epoch: 14 cost time: 42.60391426086426\n",
      "Epoch: 14, Steps: 202 | Train Loss: 0.5575337 Vali Loss: 1.0299774 Test Loss: 2.1127629\n",
      "Validation loss decreased (1.036593 --> 1.029977).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 0.4488851\n",
      "\tspeed: 0.5625s/iter; left time: 4034.6855s\n",
      "\titers: 200, epoch: 15 | loss: 0.8716497\n",
      "\tspeed: 0.0646s/iter; left time: 456.9680s\n",
      "Epoch: 15 cost time: 42.26124691963196\n",
      "Epoch: 15, Steps: 202 | Train Loss: 0.5577896 Vali Loss: 1.0566638 Test Loss: 2.1126494\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 0.5139928\n",
      "\tspeed: 0.5589s/iter; left time: 3896.2629s\n",
      "\titers: 200, epoch: 16 | loss: 0.5979797\n",
      "\tspeed: 0.0653s/iter; left time: 448.8161s\n",
      "Epoch: 16 cost time: 42.28582549095154\n",
      "Epoch: 16, Steps: 202 | Train Loss: 0.5567847 Vali Loss: 1.0487236 Test Loss: 2.1127470\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 0.5278671\n",
      "\tspeed: 0.5534s/iter; left time: 3746.0216s\n",
      "\titers: 200, epoch: 17 | loss: 0.4901922\n",
      "\tspeed: 0.0658s/iter; left time: 439.0458s\n",
      "Epoch: 17 cost time: 42.14126229286194\n",
      "Epoch: 17, Steps: 202 | Train Loss: 0.5572924 Vali Loss: 1.0557636 Test Loss: 2.1127055\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 18 | loss: 0.6818218\n",
      "\tspeed: 0.5571s/iter; left time: 3658.5185s\n",
      "\titers: 200, epoch: 18 | loss: 0.5135947\n",
      "\tspeed: 0.0669s/iter; left time: 432.3283s\n",
      "Epoch: 18 cost time: 42.33049201965332\n",
      "Epoch: 18, Steps: 202 | Train Loss: 0.5553416 Vali Loss: 1.0525610 Test Loss: 2.1127405\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.814697265625e-09\n",
      "\titers: 100, epoch: 19 | loss: 0.5300631\n",
      "\tspeed: 0.5548s/iter; left time: 3531.2554s\n",
      "\titers: 200, epoch: 19 | loss: 0.5293025\n",
      "\tspeed: 0.0655s/iter; left time: 410.5119s\n",
      "Epoch: 19 cost time: 42.01705193519592\n",
      "Epoch: 19, Steps: 202 | Train Loss: 0.5569947 Vali Loss: 1.0447042 Test Loss: 2.1127360\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "( accuracy, f1: 0.722 & 0.748 \n",
      "456, 456\n",
      "114912, 114912\n",
      " accuracy, f1: 0.334 & 0.269 \n",
      "######################  silver_data_c.csv_Autoformer_14_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1607\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.6523234\n",
      "\tspeed: 0.3378s/iter; left time: 3344.8001s\n",
      "\titers: 200, epoch: 1 | loss: 0.5472356\n",
      "\tspeed: 0.0672s/iter; left time: 658.9875s\n",
      "Epoch: 1 cost time: 41.33229875564575\n",
      "Epoch: 1, Steps: 200 | Train Loss: 0.6029993 Vali Loss: 1.0008649 Test Loss: 2.0826299\n",
      "Validation loss decreased (inf --> 1.000865).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.5356464\n",
      "\tspeed: 0.5320s/iter; left time: 5161.1317s\n",
      "\titers: 200, epoch: 2 | loss: 0.7273674\n",
      "\tspeed: 0.0671s/iter; left time: 643.8149s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 41.06181979179382\n",
      "Epoch: 2, Steps: 200 | Train Loss: 0.5851395 Vali Loss: 0.9833139 Test Loss: 2.0300248\n",
      "Validation loss decreased (1.000865 --> 0.983314).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.5657621\n",
      "\tspeed: 0.5269s/iter; left time: 5005.6086s\n",
      "\titers: 200, epoch: 3 | loss: 0.4898215\n",
      "\tspeed: 0.0675s/iter; left time: 634.4131s\n",
      "Epoch: 3 cost time: 40.61103701591492\n",
      "Epoch: 3, Steps: 200 | Train Loss: 0.5616923 Vali Loss: 1.0702858 Test Loss: 2.2037942\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6456875\n",
      "\tspeed: 0.5281s/iter; left time: 4911.5509s\n",
      "\titers: 200, epoch: 4 | loss: 0.5940708\n",
      "\tspeed: 0.0675s/iter; left time: 621.3455s\n",
      "Epoch: 4 cost time: 40.85618448257446\n",
      "Epoch: 4, Steps: 200 | Train Loss: 0.5463177 Vali Loss: 1.1374229 Test Loss: 2.2596152\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5227545\n",
      "\tspeed: 0.5321s/iter; left time: 4843.0646s\n",
      "\titers: 200, epoch: 5 | loss: 0.4510517\n",
      "\tspeed: 0.0670s/iter; left time: 602.6931s\n",
      "Epoch: 5 cost time: 41.233670234680176\n",
      "Epoch: 5, Steps: 200 | Train Loss: 0.5389293 Vali Loss: 1.1392852 Test Loss: 2.2525907\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5186692\n",
      "\tspeed: 0.5331s/iter; left time: 4745.2131s\n",
      "\titers: 200, epoch: 6 | loss: 0.5880623\n",
      "\tspeed: 0.0683s/iter; left time: 601.4569s\n",
      "Epoch: 6 cost time: 40.43740630149841\n",
      "Epoch: 6, Steps: 200 | Train Loss: 0.5353327 Vali Loss: 1.1486678 Test Loss: 2.2739427\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5499898\n",
      "\tspeed: 0.6551s/iter; left time: 5699.6041s\n",
      "\titers: 200, epoch: 7 | loss: 0.4722025\n",
      "\tspeed: 0.0786s/iter; left time: 675.8064s\n",
      "Epoch: 7 cost time: 52.35186982154846\n",
      "Epoch: 7, Steps: 200 | Train Loss: 0.5320487 Vali Loss: 1.1352819 Test Loss: 2.2713354\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "( accuracy, f1: 0.726 & 0.766 \n",
      "440, 440\n",
      "147840, 147840\n",
      " accuracy, f1: 0.392 & 0.357 \n",
      "######################  crude_oil_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1651\n",
      "val 239\n",
      "test 490\n",
      "\titers: 100, epoch: 1 | loss: 0.9355543\n",
      "\tspeed: 0.3854s/iter; left time: 3931.7169s\n",
      "\titers: 200, epoch: 1 | loss: 1.0751430\n",
      "\tspeed: 0.0792s/iter; left time: 799.8685s\n",
      "Epoch: 1 cost time: 48.02131676673889\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.8207918 Vali Loss: 1.5050110 Test Loss: 1.4736171\n",
      "Validation loss decreased (inf --> 1.505011).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6234665\n",
      "\tspeed: 0.6854s/iter; left time: 6850.7405s\n",
      "\titers: 200, epoch: 2 | loss: 0.4563533\n",
      "\tspeed: 0.0765s/iter; left time: 757.4294s\n",
      "Epoch: 2 cost time: 52.154014110565186\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.8014102 Vali Loss: 1.4291126 Test Loss: 1.4030908\n",
      "Validation loss decreased (1.505011 --> 1.429113).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7031909\n",
      "\tspeed: 0.5946s/iter; left time: 5820.9344s\n",
      "\titers: 200, epoch: 3 | loss: 0.8971089\n",
      "\tspeed: 0.0601s/iter; left time: 581.9098s\n",
      "Epoch: 3 cost time: 44.78090190887451\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.7876478 Vali Loss: 1.5402246 Test Loss: 1.5041256\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6792556\n",
      "\tspeed: 0.6099s/iter; left time: 5844.8698s\n",
      "\titers: 200, epoch: 4 | loss: 0.7804250\n",
      "\tspeed: 0.0655s/iter; left time: 621.3505s\n",
      "Epoch: 4 cost time: 46.91198706626892\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.7803595 Vali Loss: 1.4881717 Test Loss: 1.4491938\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7751442\n",
      "\tspeed: 0.6627s/iter; left time: 6214.4632s\n",
      "\titers: 200, epoch: 5 | loss: 0.9360958\n",
      "\tspeed: 0.0698s/iter; left time: 647.7508s\n",
      "Epoch: 5 cost time: 49.73601794242859\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.7746479 Vali Loss: 1.4932040 Test Loss: 1.4651214\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7403077\n",
      "\tspeed: 0.6765s/iter; left time: 6204.4020s\n",
      "\titers: 200, epoch: 6 | loss: 0.6513305\n",
      "\tspeed: 0.0758s/iter; left time: 687.9787s\n",
      "Epoch: 6 cost time: 51.94081711769104\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.7724295 Vali Loss: 1.4567134 Test Loss: 1.4429609\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6208173\n",
      "\tspeed: 0.6824s/iter; left time: 6118.0828s\n",
      "\titers: 200, epoch: 7 | loss: 0.5629314\n",
      "\tspeed: 0.0568s/iter; left time: 503.9141s\n",
      "Epoch: 7 cost time: 46.4086594581604\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.7703260 Vali Loss: 1.4538267 Test Loss: 1.4530997\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 490\n",
      "( accuracy, f1: 0.679 & 0.684 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.524 & 0.468 \n",
      "######################  crude_oil_data_c.csv_Autoformer_14_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1637\n",
      "val 225\n",
      "test 476\n",
      "\titers: 100, epoch: 1 | loss: 1.0699564\n",
      "\tspeed: 0.3194s/iter; left time: 3226.0463s\n",
      "\titers: 200, epoch: 1 | loss: 0.7561638\n",
      "\tspeed: 0.0588s/iter; left time: 587.7751s\n",
      "Epoch: 1 cost time: 38.91115593910217\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.8055172 Vali Loss: 1.4757038 Test Loss: 1.4515668\n",
      "Validation loss decreased (inf --> 1.475704).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7810651\n",
      "\tspeed: 0.5556s/iter; left time: 5499.2158s\n",
      "\titers: 200, epoch: 2 | loss: 0.7964382\n",
      "\tspeed: 0.0644s/iter; left time: 630.5453s\n",
      "Epoch: 2 cost time: 44.16990780830383\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.7862451 Vali Loss: 1.6144069 Test Loss: 1.5113478\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7748085\n",
      "\tspeed: 0.6389s/iter; left time: 6192.8113s\n",
      "\titers: 200, epoch: 3 | loss: 0.6016868\n",
      "\tspeed: 0.0780s/iter; left time: 747.9821s\n",
      "Epoch: 3 cost time: 49.747554540634155\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.7763705 Vali Loss: 1.4828151 Test Loss: 1.4389956\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.8950886\n",
      "\tspeed: 0.6146s/iter; left time: 5832.1336s\n",
      "\titers: 200, epoch: 4 | loss: 0.6485357\n",
      "\tspeed: 0.0661s/iter; left time: 620.3549s\n",
      "Epoch: 4 cost time: 45.050891637802124\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.7676749 Vali Loss: 1.4762706 Test Loss: 1.4349689\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7630203\n",
      "\tspeed: 0.5732s/iter; left time: 5322.2196s\n",
      "\titers: 200, epoch: 5 | loss: 0.6433647\n",
      "\tspeed: 0.0694s/iter; left time: 637.6550s\n",
      "Epoch: 5 cost time: 42.77743220329285\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.7646344 Vali Loss: 1.4711196 Test Loss: 1.4140508\n",
      "Validation loss decreased (1.475704 --> 1.471120).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.6497524\n",
      "\tspeed: 0.6618s/iter; left time: 6010.0423s\n",
      "\titers: 200, epoch: 6 | loss: 0.8720027\n",
      "\tspeed: 0.0593s/iter; left time: 532.3730s\n",
      "Epoch: 6 cost time: 47.96364212036133\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.7635263 Vali Loss: 1.4740971 Test Loss: 1.4244806\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5260628\n",
      "\tspeed: 0.5680s/iter; left time: 5042.2457s\n",
      "\titers: 200, epoch: 7 | loss: 0.6103048\n",
      "\tspeed: 0.0642s/iter; left time: 563.8218s\n",
      "Epoch: 7 cost time: 43.622021436691284\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.7616308 Vali Loss: 1.4804803 Test Loss: 1.4343383\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 8 | loss: 0.8483307\n",
      "\tspeed: 0.6651s/iter; left time: 5768.5080s\n",
      "\titers: 200, epoch: 8 | loss: 0.8134660\n",
      "\tspeed: 0.0780s/iter; left time: 668.7391s\n",
      "Epoch: 8 cost time: 49.40285062789917\n",
      "Epoch: 8, Steps: 204 | Train Loss: 0.7601575 Vali Loss: 1.4844027 Test Loss: 1.4349293\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.6799311\n",
      "\tspeed: 0.6889s/iter; left time: 5834.5519s\n",
      "\titers: 200, epoch: 9 | loss: 0.5864544\n",
      "\tspeed: 0.0785s/iter; left time: 656.7579s\n",
      "Epoch: 9 cost time: 52.01382923126221\n",
      "Epoch: 9, Steps: 204 | Train Loss: 0.7612070 Vali Loss: 1.4896859 Test Loss: 1.4407408\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.7938824\n",
      "\tspeed: 0.7276s/iter; left time: 6013.2269s\n",
      "\titers: 200, epoch: 10 | loss: 0.7354817\n",
      "\tspeed: 0.0787s/iter; left time: 642.8678s\n",
      "Epoch: 10 cost time: 55.03929018974304\n",
      "Epoch: 10, Steps: 204 | Train Loss: 0.7605594 Vali Loss: 1.4872137 Test Loss: 1.4386225\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 476\n",
      "( accuracy, f1: 0.690 & 0.714 \n",
      "472, 472\n",
      "79296, 79296\n",
      " accuracy, f1: 0.530 & 0.479 \n",
      "######################  crude_oil_data_c.csv_Autoformer_14_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1623\n",
      "val 211\n",
      "test 462\n",
      "\titers: 100, epoch: 1 | loss: 0.7043410\n",
      "\tspeed: 0.4114s/iter; left time: 4114.6751s\n",
      "\titers: 200, epoch: 1 | loss: 0.4375700\n",
      "\tspeed: 0.0868s/iter; left time: 858.9855s\n",
      "Epoch: 1 cost time: 51.17033910751343\n",
      "Epoch: 1, Steps: 202 | Train Loss: 0.7993136 Vali Loss: 1.5157977 Test Loss: 1.4850123\n",
      "Validation loss decreased (inf --> 1.515798).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.9553918\n",
      "\tspeed: 0.7351s/iter; left time: 7203.0706s\n",
      "\titers: 200, epoch: 2 | loss: 0.8707283\n",
      "\tspeed: 0.0832s/iter; left time: 806.7938s\n",
      "Epoch: 2 cost time: 55.8107693195343\n",
      "Epoch: 2, Steps: 202 | Train Loss: 0.7708658 Vali Loss: 1.4770012 Test Loss: 1.3867599\n",
      "Validation loss decreased (1.515798 --> 1.477001).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.8871309\n",
      "\tspeed: 0.6679s/iter; left time: 6410.2597s\n",
      "\titers: 200, epoch: 3 | loss: 1.2546933\n",
      "\tspeed: 0.0823s/iter; left time: 782.0175s\n",
      "Epoch: 3 cost time: 52.346030712127686\n",
      "Epoch: 3, Steps: 202 | Train Loss: 0.7593374 Vali Loss: 1.5162671 Test Loss: 1.4282218\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.8573174\n",
      "\tspeed: 0.7377s/iter; left time: 6930.8386s\n",
      "\titers: 200, epoch: 4 | loss: 1.0781221\n",
      "\tspeed: 0.0835s/iter; left time: 775.9185s\n",
      "Epoch: 4 cost time: 57.0237672328949\n",
      "Epoch: 4, Steps: 202 | Train Loss: 0.7539839 Vali Loss: 1.4643695 Test Loss: 1.4263178\n",
      "Validation loss decreased (1.477001 --> 1.464370).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.8104175\n",
      "\tspeed: 0.7994s/iter; left time: 7349.3228s\n",
      "\titers: 200, epoch: 5 | loss: 0.7405005\n",
      "\tspeed: 0.0813s/iter; left time: 739.5979s\n",
      "Epoch: 5 cost time: 54.52781820297241\n",
      "Epoch: 5, Steps: 202 | Train Loss: 0.7519098 Vali Loss: 1.4868910 Test Loss: 1.4205968\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7461739\n",
      "\tspeed: 0.7872s/iter; left time: 7077.8939s\n",
      "\titers: 200, epoch: 6 | loss: 0.5619828\n",
      "\tspeed: 0.0636s/iter; left time: 565.0986s\n",
      "Epoch: 6 cost time: 53.61504244804382\n",
      "Epoch: 6, Steps: 202 | Train Loss: 0.7523633 Vali Loss: 1.4883872 Test Loss: 1.4451967\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.9214648\n",
      "\tspeed: 0.6767s/iter; left time: 5947.4741s\n",
      "\titers: 200, epoch: 7 | loss: 0.4945551\n",
      "\tspeed: 0.0769s/iter; left time: 668.2268s\n",
      "Epoch: 7 cost time: 55.32051110267639\n",
      "Epoch: 7, Steps: 202 | Train Loss: 0.7497378 Vali Loss: 1.5046664 Test Loss: 1.4503618\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.7661189\n",
      "\tspeed: 1.0005s/iter; left time: 8591.0735s\n",
      "\titers: 200, epoch: 8 | loss: 0.6837119\n",
      "\tspeed: 0.1515s/iter; left time: 1285.9264s\n",
      "Epoch: 8 cost time: 94.72146439552307\n",
      "Epoch: 8, Steps: 202 | Train Loss: 0.7500600 Vali Loss: 1.4883932 Test Loss: 1.4411495\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.6308167\n",
      "\tspeed: 1.3053s/iter; left time: 10944.7073s\n",
      "\titers: 200, epoch: 9 | loss: 0.7984927\n",
      "\tspeed: 0.1623s/iter; left time: 1344.9862s\n",
      "Epoch: 9 cost time: 103.91776180267334\n",
      "Epoch: 9, Steps: 202 | Train Loss: 0.7481067 Vali Loss: 1.4992827 Test Loss: 1.4427246\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 462\n",
      "( accuracy, f1: 0.746 & 0.758 \n",
      "456, 456\n",
      "114912, 114912\n",
      " accuracy, f1: 0.520 & 0.468 \n",
      "######################  crude_oil_data_c.csv_Autoformer_14_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 0.9837607\n",
      "\tspeed: 0.7514s/iter; left time: 7476.6901s\n",
      "\titers: 200, epoch: 1 | loss: 0.9512036\n",
      "\tspeed: 0.1106s/iter; left time: 1089.4488s\n",
      "Epoch: 1 cost time: 87.62220454216003\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.8240408 Vali Loss: 1.5855974 Test Loss: 1.4055258\n",
      "Validation loss decreased (inf --> 1.585597).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.9312681\n",
      "\tspeed: 1.1238s/iter; left time: 10956.9218s\n",
      "\titers: 200, epoch: 2 | loss: 1.0576556\n",
      "\tspeed: 0.1321s/iter; left time: 1275.0662s\n",
      "Epoch: 2 cost time: 80.62344360351562\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.7940301 Vali Loss: 1.5684935 Test Loss: 1.4575340\n",
      "Validation loss decreased (1.585597 --> 1.568493).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.8950167\n",
      "\tspeed: 1.0455s/iter; left time: 9983.7919s\n",
      "\titers: 200, epoch: 3 | loss: 0.7420155\n",
      "\tspeed: 0.1492s/iter; left time: 1409.9806s\n",
      "Epoch: 3 cost time: 80.08482193946838\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.7725242 Vali Loss: 1.5776542 Test Loss: 1.4643127\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.9085416\n",
      "\tspeed: 1.0614s/iter; left time: 9922.0115s\n",
      "\titers: 200, epoch: 4 | loss: 0.8032493\n",
      "\tspeed: 0.1142s/iter; left time: 1055.7091s\n",
      "Epoch: 4 cost time: 78.4933454990387\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.7639970 Vali Loss: 1.5265241 Test Loss: 1.4284490\n",
      "Validation loss decreased (1.568493 --> 1.526524).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.8404880\n",
      "\tspeed: 1.0960s/iter; left time: 10025.2321s\n",
      "\titers: 200, epoch: 5 | loss: 0.8328316\n",
      "\tspeed: 0.1469s/iter; left time: 1328.6119s\n",
      "Epoch: 5 cost time: 87.96762800216675\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.7623801 Vali Loss: 1.5292215 Test Loss: 1.4221632\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5500926\n",
      "\tspeed: 1.0943s/iter; left time: 9789.9438s\n",
      "\titers: 200, epoch: 6 | loss: 0.6878158\n",
      "\tspeed: 0.1418s/iter; left time: 1253.9479s\n",
      "Epoch: 6 cost time: 88.12085032463074\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.7612089 Vali Loss: 1.5288893 Test Loss: 1.4306194\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6186399\n",
      "\tspeed: 1.0920s/iter; left time: 9549.7357s\n",
      "\titers: 200, epoch: 7 | loss: 0.7216027\n",
      "\tspeed: 0.1456s/iter; left time: 1258.6593s\n",
      "Epoch: 7 cost time: 86.79304361343384\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.7602915 Vali Loss: 1.5455614 Test Loss: 1.4492019\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.7525916\n",
      "\tspeed: 1.0655s/iter; left time: 9103.8504s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 8 | loss: 0.8025526\n",
      "\tspeed: 0.1455s/iter; left time: 1228.6785s\n",
      "Epoch: 8 cost time: 86.20182943344116\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.7600442 Vali Loss: 1.5473284 Test Loss: 1.4566231\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.5957254\n",
      "\tspeed: 1.0668s/iter; left time: 8900.7088s\n",
      "\titers: 200, epoch: 9 | loss: 0.6889934\n",
      "\tspeed: 0.1440s/iter; left time: 1186.9586s\n",
      "Epoch: 9 cost time: 85.6065788269043\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.7594314 Vali Loss: 1.5372337 Test Loss: 1.4537144\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "( accuracy, f1: 0.735 & 0.755 \n",
      "448, 448\n",
      "150528, 150528\n",
      " accuracy, f1: 0.525 & 0.477 \n",
      "######################  natural_gas_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1651\n",
      "val 239\n",
      "test 490\n",
      "\titers: 100, epoch: 1 | loss: 1.0705878\n",
      "\tspeed: 0.6799s/iter; left time: 6936.1675s\n",
      "\titers: 200, epoch: 1 | loss: 0.6673988\n",
      "\tspeed: 0.1292s/iter; left time: 1305.4288s\n",
      "Epoch: 1 cost time: 83.36032819747925\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.8993524 Vali Loss: 2.0789011 Test Loss: 7.2593179\n",
      "Validation loss decreased (inf --> 2.078901).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.9908223\n",
      "\tspeed: 1.0745s/iter; left time: 10739.9972s\n",
      "\titers: 200, epoch: 2 | loss: 0.6381457\n",
      "\tspeed: 0.1225s/iter; left time: 1212.3989s\n",
      "Epoch: 2 cost time: 84.0249035358429\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.8898829 Vali Loss: 2.0229001 Test Loss: 7.2401285\n",
      "Validation loss decreased (2.078901 --> 2.022900).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.8970442\n",
      "\tspeed: 1.0539s/iter; left time: 10317.0870s\n",
      "\titers: 200, epoch: 3 | loss: 0.7225158\n",
      "\tspeed: 0.1272s/iter; left time: 1232.2664s\n",
      "Epoch: 3 cost time: 82.4504177570343\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.8805198 Vali Loss: 2.0255325 Test Loss: 7.2129412\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6408691\n",
      "\tspeed: 1.0405s/iter; left time: 9971.0008s\n",
      "\titers: 200, epoch: 4 | loss: 1.4497954\n",
      "\tspeed: 0.1283s/iter; left time: 1216.4162s\n",
      "Epoch: 4 cost time: 82.16284918785095\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.8715423 Vali Loss: 2.0272715 Test Loss: 7.2080984\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7812340\n",
      "\tspeed: 1.0475s/iter; left time: 9822.7231s\n",
      "\titers: 200, epoch: 5 | loss: 1.0225493\n",
      "\tspeed: 0.1270s/iter; left time: 1177.8322s\n",
      "Epoch: 5 cost time: 82.1708014011383\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.8647675 Vali Loss: 2.0302715 Test Loss: 7.2101965\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7824565\n",
      "\tspeed: 1.0293s/iter; left time: 9439.8731s\n",
      "\titers: 200, epoch: 6 | loss: 1.1996839\n",
      "\tspeed: 0.1292s/iter; left time: 1171.7044s\n",
      "Epoch: 6 cost time: 82.06911516189575\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.8633306 Vali Loss: 2.0335777 Test Loss: 7.1993604\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.7853487\n",
      "\tspeed: 1.0292s/iter; left time: 9227.0345s\n",
      "\titers: 200, epoch: 7 | loss: 0.8383638\n",
      "\tspeed: 0.1221s/iter; left time: 1082.2581s\n",
      "Epoch: 7 cost time: 80.3862817287445\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.8591207 Vali Loss: 2.0439329 Test Loss: 7.2056961\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 490\n",
      "( accuracy, f1: 0.759 & 0.794 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.471 & 0.404 \n",
      "######################  natural_gas_data_c.csv_Autoformer_14_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1637\n",
      "val 225\n",
      "test 476\n",
      "\titers: 100, epoch: 1 | loss: 1.0003382\n",
      "\tspeed: 0.6740s/iter; left time: 6808.4881s\n",
      "\titers: 200, epoch: 1 | loss: 0.6996025\n",
      "\tspeed: 0.1407s/iter; left time: 1407.4381s\n",
      "Epoch: 1 cost time: 83.67204928398132\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.8932982 Vali Loss: 2.0519242 Test Loss: 7.1391015\n",
      "Validation loss decreased (inf --> 2.051924).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.5809694\n",
      "\tspeed: 1.0612s/iter; left time: 10502.9862s\n",
      "\titers: 200, epoch: 2 | loss: 0.6182752\n",
      "\tspeed: 0.1326s/iter; left time: 1298.6866s\n",
      "Epoch: 2 cost time: 83.94180798530579\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.8866709 Vali Loss: 2.0709462 Test Loss: 7.1356359\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.9856210\n",
      "\tspeed: 1.0598s/iter; left time: 10272.5797s\n",
      "\titers: 200, epoch: 3 | loss: 0.8439475\n",
      "\tspeed: 0.1271s/iter; left time: 1219.6738s\n",
      "Epoch: 3 cost time: 83.02214884757996\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.8725200 Vali Loss: 2.1047108 Test Loss: 7.1228924\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.9995255\n",
      "\tspeed: 1.0452s/iter; left time: 9917.4410s\n",
      "\titers: 200, epoch: 4 | loss: 0.5669205\n",
      "\tspeed: 0.1354s/iter; left time: 1271.4728s\n",
      "Epoch: 4 cost time: 83.45994162559509\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.8555350 Vali Loss: 2.0738733 Test Loss: 7.1308160\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.0326701\n",
      "\tspeed: 0.9916s/iter; left time: 9207.2168s\n",
      "\titers: 200, epoch: 5 | loss: 0.8003764\n",
      "\tspeed: 0.1253s/iter; left time: 1150.7733s\n",
      "Epoch: 5 cost time: 77.1523220539093\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.8417362 Vali Loss: 2.0770056 Test Loss: 7.1382861\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.9229822\n",
      "\tspeed: 1.0302s/iter; left time: 9354.9123s\n",
      "\titers: 200, epoch: 6 | loss: 0.8208900\n",
      "\tspeed: 0.1355s/iter; left time: 1217.3549s\n",
      "Epoch: 6 cost time: 81.87103867530823\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.8369836 Vali Loss: 2.0720325 Test Loss: 7.1421475\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 476\n",
      "( accuracy, f1: 0.732 & 0.776 \n",
      "472, 472\n",
      "79296, 79296\n",
      " accuracy, f1: 0.482 & 0.420 \n",
      "######################  natural_gas_data_c.csv_Autoformer_14_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1623\n",
      "val 211\n",
      "test 462\n",
      "\titers: 100, epoch: 1 | loss: 0.9531618\n",
      "\tspeed: 0.6732s/iter; left time: 6732.4958s\n",
      "\titers: 200, epoch: 1 | loss: 0.8769451\n",
      "\tspeed: 0.1326s/iter; left time: 1312.9107s\n",
      "Epoch: 1 cost time: 82.38229703903198\n",
      "Epoch: 1, Steps: 202 | Train Loss: 0.8891789 Vali Loss: 2.1184921 Test Loss: 6.8686395\n",
      "Validation loss decreased (inf --> 2.118492).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.9135895\n",
      "\tspeed: 1.0510s/iter; left time: 10298.5000s\n",
      "\titers: 200, epoch: 2 | loss: 1.0670857\n",
      "\tspeed: 0.1385s/iter; left time: 1343.3079s\n",
      "Epoch: 2 cost time: 82.94997143745422\n",
      "Epoch: 2, Steps: 202 | Train Loss: 0.8840069 Vali Loss: 2.1151066 Test Loss: 6.8505507\n",
      "Validation loss decreased (2.118492 --> 2.115107).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.0161312\n",
      "\tspeed: 1.0586s/iter; left time: 10159.2736s\n",
      "\titers: 200, epoch: 3 | loss: 0.8123475\n",
      "\tspeed: 0.1331s/iter; left time: 1264.2757s\n",
      "Epoch: 3 cost time: 81.52730774879456\n",
      "Epoch: 3, Steps: 202 | Train Loss: 0.8725197 Vali Loss: 2.1499264 Test Loss: 6.8619528\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6102654\n",
      "\tspeed: 1.0286s/iter; left time: 9663.6416s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 4 | loss: 1.1013256\n",
      "\tspeed: 0.1397s/iter; left time: 1298.5112s\n",
      "Epoch: 4 cost time: 81.91245675086975\n",
      "Epoch: 4, Steps: 202 | Train Loss: 0.8565149 Vali Loss: 2.1160595 Test Loss: 6.9308090\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6474276\n",
      "\tspeed: 1.0664s/iter; left time: 9803.0880s\n",
      "\titers: 200, epoch: 5 | loss: 1.0497553\n",
      "\tspeed: 0.1383s/iter; left time: 1257.6325s\n",
      "Epoch: 5 cost time: 83.5029788017273\n",
      "Epoch: 5, Steps: 202 | Train Loss: 0.8510066 Vali Loss: 2.1033309 Test Loss: 6.9443541\n",
      "Validation loss decreased (2.115107 --> 2.103331).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7606266\n",
      "\tspeed: 1.1204s/iter; left time: 10073.5949s\n",
      "\titers: 200, epoch: 6 | loss: 1.1932918\n",
      "\tspeed: 0.1475s/iter; left time: 1311.6062s\n",
      "Epoch: 6 cost time: 85.14688634872437\n",
      "Epoch: 6, Steps: 202 | Train Loss: 0.8437252 Vali Loss: 2.1208043 Test Loss: 6.9368019\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6987701\n",
      "\tspeed: 1.1505s/iter; left time: 10111.4337s\n",
      "\titers: 200, epoch: 7 | loss: 0.5821503\n",
      "\tspeed: 0.1501s/iter; left time: 1304.4840s\n",
      "Epoch: 7 cost time: 92.2875235080719\n",
      "Epoch: 7, Steps: 202 | Train Loss: 0.8387266 Vali Loss: 2.1248202 Test Loss: 6.9352107\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.8167094\n",
      "\tspeed: 1.1364s/iter; left time: 9757.9280s\n",
      "\titers: 200, epoch: 8 | loss: 0.7950384\n",
      "\tspeed: 0.1503s/iter; left time: 1275.2426s\n",
      "Epoch: 8 cost time: 88.3644917011261\n",
      "Epoch: 8, Steps: 202 | Train Loss: 0.8389171 Vali Loss: 2.1312640 Test Loss: 6.9328628\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.0502717\n",
      "\tspeed: 1.0974s/iter; left time: 9201.6051s\n",
      "\titers: 200, epoch: 9 | loss: 0.6440721\n",
      "\tspeed: 0.1116s/iter; left time: 924.3071s\n",
      "Epoch: 9 cost time: 84.67640209197998\n",
      "Epoch: 9, Steps: 202 | Train Loss: 0.8357734 Vali Loss: 2.1198287 Test Loss: 6.9319687\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.0956091\n",
      "\tspeed: 1.1449s/iter; left time: 9368.8307s\n",
      "\titers: 200, epoch: 10 | loss: 1.3603737\n",
      "\tspeed: 0.1089s/iter; left time: 880.4414s\n",
      "Epoch: 10 cost time: 87.33440613746643\n",
      "Epoch: 10, Steps: 202 | Train Loss: 0.8381305 Vali Loss: 2.1233275 Test Loss: 6.9317441\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 462\n",
      "( accuracy, f1: 0.722 & 0.764 \n",
      "456, 456\n",
      "114912, 114912\n",
      " accuracy, f1: 0.493 & 0.439 \n",
      "######################  natural_gas_data_c.csv_Autoformer_14_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 0.8267218\n",
      "\tspeed: 0.7316s/iter; left time: 7280.1020s\n",
      "\titers: 200, epoch: 1 | loss: 0.9594196\n",
      "\tspeed: 0.1261s/iter; left time: 1242.5975s\n",
      "Epoch: 1 cost time: 87.55485463142395\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.8859203 Vali Loss: 2.1918590 Test Loss: 6.8731198\n",
      "Validation loss decreased (inf --> 2.191859).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7608266\n",
      "\tspeed: 0.9807s/iter; left time: 9561.7561s\n",
      "\titers: 200, epoch: 2 | loss: 0.6498645\n",
      "\tspeed: 0.1396s/iter; left time: 1347.5045s\n",
      "Epoch: 2 cost time: 80.5660936832428\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.8758932 Vali Loss: 2.1950514 Test Loss: 6.9184499\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.0153598\n",
      "\tspeed: 0.9290s/iter; left time: 8870.8537s\n",
      "\titers: 200, epoch: 3 | loss: 0.4697298\n",
      "\tspeed: 0.1208s/iter; left time: 1141.8814s\n",
      "Epoch: 3 cost time: 64.01073336601257\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.8551143 Vali Loss: 2.1691968 Test Loss: 6.9645438\n",
      "Validation loss decreased (2.191859 --> 2.169197).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.8443126\n",
      "\tspeed: 1.0977s/iter; left time: 10261.6615s\n",
      "\titers: 200, epoch: 4 | loss: 0.6662603\n",
      "\tspeed: 0.1698s/iter; left time: 1570.0422s\n",
      "Epoch: 4 cost time: 89.54157614707947\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.8360441 Vali Loss: 2.1618586 Test Loss: 7.0074148\n",
      "Validation loss decreased (2.169197 --> 2.161859).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7806166\n",
      "\tspeed: 1.1321s/iter; left time: 10354.9789s\n",
      "\titers: 200, epoch: 5 | loss: 0.6118258\n",
      "\tspeed: 0.1265s/iter; left time: 1144.6911s\n",
      "Epoch: 5 cost time: 87.38902854919434\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.8266449 Vali Loss: 2.1771712 Test Loss: 7.0294433\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8719085\n",
      "\tspeed: 1.2003s/iter; left time: 10737.8749s\n",
      "\titers: 200, epoch: 6 | loss: 0.7020702\n",
      "\tspeed: 0.1552s/iter; left time: 1373.0144s\n",
      "Epoch: 6 cost time: 93.73654127120972\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.8210333 Vali Loss: 2.1658037 Test Loss: 7.0224938\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5953166\n",
      "\tspeed: 1.1420s/iter; left time: 9986.6235s\n",
      "\titers: 200, epoch: 7 | loss: 0.5775612\n",
      "\tspeed: 0.1499s/iter; left time: 1296.0530s\n",
      "Epoch: 7 cost time: 90.5513288974762\n",
      "Epoch: 7, Steps: 201 | Train Loss: 0.8189422 Vali Loss: 2.1741602 Test Loss: 7.0245428\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.0039239\n",
      "\tspeed: 1.1444s/iter; left time: 9777.5047s\n",
      "\titers: 200, epoch: 8 | loss: 0.9125512\n",
      "\tspeed: 0.1553s/iter; left time: 1311.4424s\n",
      "Epoch: 8 cost time: 91.41383600234985\n",
      "Epoch: 8, Steps: 201 | Train Loss: 0.8173133 Vali Loss: 2.1664207 Test Loss: 7.0293207\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.6206883\n",
      "\tspeed: 1.1535s/iter; left time: 9623.8442s\n",
      "\titers: 200, epoch: 9 | loss: 0.9063144\n",
      "\tspeed: 0.1498s/iter; left time: 1234.5628s\n",
      "Epoch: 9 cost time: 92.38681125640869\n",
      "Epoch: 9, Steps: 201 | Train Loss: 0.8169816 Vali Loss: 2.1730049 Test Loss: 7.0297937\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "( accuracy, f1: 0.711 & 0.753 \n",
      "448, 448\n",
      "150528, 150528\n",
      " accuracy, f1: 0.488 & 0.435 \n",
      "######################  corn_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1648\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.5716894\n",
      "\tspeed: 0.9336s/iter; left time: 9523.8356s\n",
      "\titers: 200, epoch: 1 | loss: 0.8994051\n",
      "\tspeed: 0.1482s/iter; left time: 1496.5395s\n",
      "Epoch: 1 cost time: 110.97439789772034\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.6892097 Vali Loss: 0.8421608 Test Loss: 3.1136777\n",
      "Validation loss decreased (inf --> 0.842161).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.8685971\n",
      "\tspeed: 1.1689s/iter; left time: 11683.1518s\n",
      "\titers: 200, epoch: 2 | loss: 0.5842311\n",
      "\tspeed: 0.1374s/iter; left time: 1359.6753s\n",
      "Epoch: 2 cost time: 88.12695789337158\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.6773048 Vali Loss: 0.7917671 Test Loss: 3.1196277\n",
      "Validation loss decreased (0.842161 --> 0.791767).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6180912\n",
      "\tspeed: 1.1299s/iter; left time: 11060.2457s\n",
      "\titers: 200, epoch: 3 | loss: 0.5132647\n",
      "\tspeed: 0.1317s/iter; left time: 1276.0674s\n",
      "Epoch: 3 cost time: 87.6934266090393\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.6631646 Vali Loss: 0.7896443 Test Loss: 3.0858526\n",
      "Validation loss decreased (0.791767 --> 0.789644).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5644271\n",
      "\tspeed: 1.1030s/iter; left time: 10569.9643s\n",
      "\titers: 200, epoch: 4 | loss: 0.6272579\n",
      "\tspeed: 0.1273s/iter; left time: 1207.2873s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 cost time: 85.26020169258118\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.6541644 Vali Loss: 0.7800537 Test Loss: 3.0948226\n",
      "Validation loss decreased (0.789644 --> 0.780054).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7420205\n",
      "\tspeed: 1.1314s/iter; left time: 10609.4470s\n",
      "\titers: 200, epoch: 5 | loss: 0.5713568\n",
      "\tspeed: 0.1343s/iter; left time: 1245.8005s\n",
      "Epoch: 5 cost time: 88.55787682533264\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.6479821 Vali Loss: 0.7671589 Test Loss: 3.0893984\n",
      "Validation loss decreased (0.780054 --> 0.767159).  Saving model ...\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4323314\n",
      "\tspeed: 1.0755s/iter; left time: 9863.1836s\n",
      "\titers: 200, epoch: 6 | loss: 0.8161846\n",
      "\tspeed: 0.1323s/iter; left time: 1199.9450s\n",
      "Epoch: 6 cost time: 83.43831515312195\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.6458276 Vali Loss: 0.7821642 Test Loss: 3.1041033\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5564908\n",
      "\tspeed: 1.1143s/iter; left time: 9989.7467s\n",
      "\titers: 200, epoch: 7 | loss: 0.4741941\n",
      "\tspeed: 0.1315s/iter; left time: 1165.8375s\n",
      "Epoch: 7 cost time: 86.43438959121704\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.6436640 Vali Loss: 0.7880874 Test Loss: 3.0983033\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.1274379\n",
      "\tspeed: 1.0833s/iter; left time: 9488.9705s\n",
      "\titers: 200, epoch: 8 | loss: 0.4663250\n",
      "\tspeed: 0.1261s/iter; left time: 1091.7826s\n",
      "Epoch: 8 cost time: 83.45150566101074\n",
      "Epoch: 8, Steps: 206 | Train Loss: 0.6425347 Vali Loss: 0.7882536 Test Loss: 3.1007509\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.8451889\n",
      "\tspeed: 1.1049s/iter; left time: 9450.3009s\n",
      "\titers: 200, epoch: 9 | loss: 0.5684465\n",
      "\tspeed: 0.1368s/iter; left time: 1156.3181s\n",
      "Epoch: 9 cost time: 86.69731783866882\n",
      "Epoch: 9, Steps: 206 | Train Loss: 0.6422988 Vali Loss: 0.7901940 Test Loss: 3.1012933\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.5925614\n",
      "\tspeed: 1.1023s/iter; left time: 9201.2622s\n",
      "\titers: 200, epoch: 10 | loss: 0.5294463\n",
      "\tspeed: 0.1321s/iter; left time: 1089.5347s\n",
      "Epoch: 10 cost time: 85.85437107086182\n",
      "Epoch: 10, Steps: 206 | Train Loss: 0.6418433 Vali Loss: 0.7886466 Test Loss: 3.0999749\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "( accuracy, f1: 0.732 & 0.732 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.302 & 0.249 \n",
      "######################  corn_data_c.csv_Autoformer_14_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1634\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.6600450\n",
      "\tspeed: 0.6831s/iter; left time: 6900.3231s\n",
      "\titers: 200, epoch: 1 | loss: 0.6089766\n",
      "\tspeed: 0.1359s/iter; left time: 1358.9275s\n",
      "Epoch: 1 cost time: 84.12335634231567\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.6735030 Vali Loss: 0.8490772 Test Loss: 3.1071558\n",
      "Validation loss decreased (inf --> 0.849077).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6429510\n",
      "\tspeed: 1.0542s/iter; left time: 10433.7648s\n",
      "\titers: 200, epoch: 2 | loss: 0.4801410\n",
      "\tspeed: 0.1349s/iter; left time: 1321.5671s\n",
      "Epoch: 2 cost time: 82.46549320220947\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.6631518 Vali Loss: 0.7694239 Test Loss: 3.1150990\n",
      "Validation loss decreased (0.849077 --> 0.769424).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6210266\n",
      "\tspeed: 1.0049s/iter; left time: 9740.8758s\n",
      "\titers: 200, epoch: 3 | loss: 0.8700845\n",
      "\tspeed: 0.1522s/iter; left time: 1460.1440s\n",
      "Epoch: 3 cost time: 77.40357232093811\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.6521088 Vali Loss: 0.7875492 Test Loss: 3.1327970\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4394871\n",
      "\tspeed: 1.0888s/iter; left time: 10331.5859s\n",
      "\titers: 200, epoch: 4 | loss: 0.8002840\n",
      "\tspeed: 0.1592s/iter; left time: 1494.4332s\n",
      "Epoch: 4 cost time: 86.76073813438416\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.6428511 Vali Loss: 0.7765372 Test Loss: 3.1184719\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7426507\n",
      "\tspeed: 1.1162s/iter; left time: 10363.5419s\n",
      "\titers: 200, epoch: 5 | loss: 0.5320499\n",
      "\tspeed: 0.1447s/iter; left time: 1329.0496s\n",
      "Epoch: 5 cost time: 93.48767447471619\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.6366216 Vali Loss: 0.8233975 Test Loss: 3.1332440\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5628889\n",
      "\tspeed: 1.1344s/iter; left time: 10301.7744s\n",
      "\titers: 200, epoch: 6 | loss: 0.5734222\n",
      "\tspeed: 0.1403s/iter; left time: 1260.2731s\n",
      "Epoch: 6 cost time: 88.97431826591492\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.6335438 Vali Loss: 0.8097711 Test Loss: 3.1287298\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4755887\n",
      "\tspeed: 1.1243s/iter; left time: 9980.1220s\n",
      "\titers: 200, epoch: 7 | loss: 0.6370823\n",
      "\tspeed: 0.1404s/iter; left time: 1232.4967s\n",
      "Epoch: 7 cost time: 88.45399236679077\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.6317208 Vali Loss: 0.8113025 Test Loss: 3.1325953\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "( accuracy, f1: 0.702 & 0.737 \n",
      "472, 472\n",
      "79296, 79296\n",
      " accuracy, f1: 0.340 & 0.313 \n",
      "######################  corn_data_c.csv_Autoformer_14_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1620\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 0.5523278\n",
      "\tspeed: 0.7266s/iter; left time: 7266.9868s\n",
      "\titers: 200, epoch: 1 | loss: 0.5490260\n",
      "\tspeed: 0.1473s/iter; left time: 1457.9399s\n",
      "Epoch: 1 cost time: 89.39356684684753\n",
      "Epoch: 1, Steps: 202 | Train Loss: 0.6613955 Vali Loss: 0.8142790 Test Loss: 3.1450236\n",
      "Validation loss decreased (inf --> 0.814279).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.5170226\n",
      "\tspeed: 1.1188s/iter; left time: 10963.0811s\n",
      "\titers: 200, epoch: 2 | loss: 0.5001597\n",
      "\tspeed: 0.1504s/iter; left time: 1459.1098s\n",
      "Epoch: 2 cost time: 89.43383479118347\n",
      "Epoch: 2, Steps: 202 | Train Loss: 0.6456177 Vali Loss: 0.7909907 Test Loss: 3.1555872\n",
      "Validation loss decreased (0.814279 --> 0.790991).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4893062\n",
      "\tspeed: 1.1348s/iter; left time: 10890.9103s\n",
      "\titers: 200, epoch: 3 | loss: 0.7417264\n",
      "\tspeed: 0.1367s/iter; left time: 1298.4705s\n",
      "Epoch: 3 cost time: 88.44804215431213\n",
      "Epoch: 3, Steps: 202 | Train Loss: 0.6302652 Vali Loss: 0.8113233 Test Loss: 3.1797469\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.8723893\n",
      "\tspeed: 1.1113s/iter; left time: 10440.9456s\n",
      "\titers: 200, epoch: 4 | loss: 0.5269817\n",
      "\tspeed: 0.1449s/iter; left time: 1346.5482s\n",
      "Epoch: 4 cost time: 87.69376182556152\n",
      "Epoch: 4, Steps: 202 | Train Loss: 0.6206026 Vali Loss: 0.8004229 Test Loss: 3.1721838\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6910897\n",
      "\tspeed: 1.1228s/iter; left time: 10321.7228s\n",
      "\titers: 200, epoch: 5 | loss: 0.7379221\n",
      "\tspeed: 0.1434s/iter; left time: 1303.7892s\n",
      "Epoch: 5 cost time: 87.93376994132996\n",
      "Epoch: 5, Steps: 202 | Train Loss: 0.6168999 Vali Loss: 0.8045275 Test Loss: 3.1932414\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5642895\n",
      "\tspeed: 1.1318s/iter; left time: 10175.8990s\n",
      "\titers: 200, epoch: 6 | loss: 0.8915584\n",
      "\tspeed: 0.1467s/iter; left time: 1304.4527s\n",
      "Epoch: 6 cost time: 89.428471326828\n",
      "Epoch: 6, Steps: 202 | Train Loss: 0.6138009 Vali Loss: 0.8010612 Test Loss: 3.1777227\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 7 | loss: 0.5220477\n",
      "\tspeed: 1.1232s/iter; left time: 9871.9541s\n",
      "\titers: 200, epoch: 7 | loss: 0.4965756\n",
      "\tspeed: 0.1426s/iter; left time: 1239.2999s\n",
      "Epoch: 7 cost time: 87.72499418258667\n",
      "Epoch: 7, Steps: 202 | Train Loss: 0.6130215 Vali Loss: 0.8281145 Test Loss: 3.1801648\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "( accuracy, f1: 0.710 & 0.740 \n",
      "456, 456\n",
      "114912, 114912\n",
      " accuracy, f1: 0.309 & 0.268 \n",
      "######################  corn_data_c.csv_Autoformer_14_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1606\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.7646302\n",
      "\tspeed: 0.7183s/iter; left time: 7112.0518s\n",
      "\titers: 200, epoch: 1 | loss: 0.5589564\n",
      "\tspeed: 0.1544s/iter; left time: 1512.8698s\n",
      "Epoch: 1 cost time: 89.05857753753662\n",
      "Epoch: 1, Steps: 200 | Train Loss: 0.6569831 Vali Loss: 0.8404856 Test Loss: 3.1563778\n",
      "Validation loss decreased (inf --> 0.840486).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6309706\n",
      "\tspeed: 1.1044s/iter; left time: 10714.2324s\n",
      "\titers: 200, epoch: 2 | loss: 0.5400838\n",
      "\tspeed: 0.1502s/iter; left time: 1442.2171s\n",
      "Epoch: 2 cost time: 87.78525519371033\n",
      "Epoch: 2, Steps: 200 | Train Loss: 0.6414503 Vali Loss: 0.8290082 Test Loss: 3.2162850\n",
      "Validation loss decreased (0.840486 --> 0.829008).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.5147800\n",
      "\tspeed: 1.1036s/iter; left time: 10485.2647s\n",
      "\titers: 200, epoch: 3 | loss: 0.7775890\n",
      "\tspeed: 0.1475s/iter; left time: 1386.9556s\n",
      "Epoch: 3 cost time: 87.0591025352478\n",
      "Epoch: 3, Steps: 200 | Train Loss: 0.6122287 Vali Loss: 0.8520174 Test Loss: 3.2437203\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4918930\n",
      "\tspeed: 1.0999s/iter; left time: 10229.7562s\n",
      "\titers: 200, epoch: 4 | loss: 0.4842313\n",
      "\tspeed: 0.1472s/iter; left time: 1354.1568s\n",
      "Epoch: 4 cost time: 87.58279347419739\n",
      "Epoch: 4, Steps: 200 | Train Loss: 0.5939257 Vali Loss: 0.8349374 Test Loss: 3.2538297\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7263612\n",
      "\tspeed: 1.1009s/iter; left time: 10019.0393s\n",
      "\titers: 200, epoch: 5 | loss: 0.5540910\n",
      "\tspeed: 0.1413s/iter; left time: 1271.8435s\n",
      "Epoch: 5 cost time: 85.4847342967987\n",
      "Epoch: 5, Steps: 200 | Train Loss: 0.5833928 Vali Loss: 0.8726404 Test Loss: 3.2798262\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4572988\n",
      "\tspeed: 1.1101s/iter; left time: 9881.3302s\n",
      "\titers: 200, epoch: 6 | loss: 0.4918868\n",
      "\tspeed: 0.1439s/iter; left time: 1266.6735s\n",
      "Epoch: 6 cost time: 87.17656373977661\n",
      "Epoch: 6, Steps: 200 | Train Loss: 0.5783244 Vali Loss: 0.8742456 Test Loss: 3.3127682\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.4387982\n",
      "\tspeed: 1.0960s/iter; left time: 9536.2155s\n",
      "\titers: 200, epoch: 7 | loss: 0.9197835\n",
      "\tspeed: 0.1506s/iter; left time: 1295.6930s\n",
      "Epoch: 7 cost time: 87.43486857414246\n",
      "Epoch: 7, Steps: 200 | Train Loss: 0.5768036 Vali Loss: 0.8780898 Test Loss: 3.3048236\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "( accuracy, f1: 0.699 & 0.728 \n",
      "440, 440\n",
      "147840, 147840\n",
      " accuracy, f1: 0.263 & 0.193 \n",
      "######################  coffee_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 1.0079672\n",
      "\tspeed: 0.6878s/iter; left time: 7015.7642s\n",
      "\titers: 200, epoch: 1 | loss: 0.4791764\n",
      "\tspeed: 0.1331s/iter; left time: 1344.7247s\n",
      "Epoch: 1 cost time: 84.77891802787781\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.8792484 Vali Loss: 1.0029345 Test Loss: 4.3067064\n",
      "Validation loss decreased (inf --> 1.002934).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7745423\n",
      "\tspeed: 1.0873s/iter; left time: 10867.5278s\n",
      "\titers: 200, epoch: 2 | loss: 0.5478998\n",
      "\tspeed: 0.1312s/iter; left time: 1298.0970s\n",
      "Epoch: 2 cost time: 85.12514996528625\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.8628408 Vali Loss: 1.0296686 Test Loss: 4.3394132\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4053447\n",
      "\tspeed: 1.0800s/iter; left time: 10572.1860s\n",
      "\titers: 200, epoch: 3 | loss: 0.7977584\n",
      "\tspeed: 0.1296s/iter; left time: 1255.5976s\n",
      "Epoch: 3 cost time: 84.58542728424072\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.8498868 Vali Loss: 1.0486850 Test Loss: 4.3013387\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6749609\n",
      "\tspeed: 1.0613s/iter; left time: 10170.8926s\n",
      "\titers: 200, epoch: 4 | loss: 0.6981624\n",
      "\tspeed: 0.1298s/iter; left time: 1231.3651s\n",
      "Epoch: 4 cost time: 83.72071599960327\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.8406369 Vali Loss: 1.0839067 Test Loss: 4.3263488\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.8519122\n",
      "\tspeed: 1.0506s/iter; left time: 9851.5149s\n",
      "\titers: 200, epoch: 5 | loss: 0.9430788\n",
      "\tspeed: 0.1265s/iter; left time: 1173.9059s\n",
      "Epoch: 5 cost time: 82.04868054389954\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.8349028 Vali Loss: 1.0795196 Test Loss: 4.3118081\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5848197\n",
      "\tspeed: 1.0766s/iter; left time: 9873.4251s\n",
      "\titers: 200, epoch: 6 | loss: 0.6985955\n",
      "\tspeed: 0.1281s/iter; left time: 1162.3835s\n",
      "Epoch: 6 cost time: 83.9425117969513\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.8335436 Vali Loss: 1.0846374 Test Loss: 4.3152833\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "( accuracy, f1: 0.786 & 0.797 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.358 & 0.269 \n",
      "######################  coffee_data_c.csv_Autoformer_14_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1636\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.9586573\n",
      "\tspeed: 0.6811s/iter; left time: 6880.2392s\n",
      "\titers: 200, epoch: 1 | loss: 0.6465812\n",
      "\tspeed: 0.1388s/iter; left time: 1388.5848s\n",
      "Epoch: 1 cost time: 84.22484970092773\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.8912461 Vali Loss: 0.9938900 Test Loss: 4.3226523\n",
      "Validation loss decreased (inf --> 0.993890).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.1188838\n",
      "\tspeed: 1.0831s/iter; left time: 10719.5871s\n",
      "\titers: 200, epoch: 2 | loss: 0.6327516\n",
      "\tspeed: 0.1338s/iter; left time: 1310.5376s\n",
      "Epoch: 2 cost time: 84.23288774490356\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.8732297 Vali Loss: 0.9735013 Test Loss: 4.3197989\n",
      "Validation loss decreased (0.993890 --> 0.973501).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4851988\n",
      "\tspeed: 1.0835s/iter; left time: 10502.7591s\n",
      "\titers: 200, epoch: 3 | loss: 1.5227706\n",
      "\tspeed: 0.1346s/iter; left time: 1291.1457s\n",
      "Epoch: 3 cost time: 84.5701379776001\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.8604340 Vali Loss: 1.0138080 Test Loss: 4.3214741\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.7918581\n",
      "\tspeed: 1.0516s/iter; left time: 9979.0086s\n",
      "\titers: 200, epoch: 4 | loss: 0.7986732\n",
      "\tspeed: 0.1327s/iter; left time: 1246.1277s\n",
      "Epoch: 4 cost time: 83.03014922142029\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.8507228 Vali Loss: 1.0172694 Test Loss: 4.3159714\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 5 | loss: 0.8800078\n",
      "\tspeed: 1.0600s/iter; left time: 9841.9165s\n",
      "\titers: 200, epoch: 5 | loss: 0.7951154\n",
      "\tspeed: 0.1330s/iter; left time: 1221.2961s\n",
      "Epoch: 5 cost time: 82.7793653011322\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.8450002 Vali Loss: 1.0224820 Test Loss: 4.3075194\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8083376\n",
      "\tspeed: 1.0829s/iter; left time: 9834.1000s\n",
      "\titers: 200, epoch: 6 | loss: 0.8800920\n",
      "\tspeed: 0.1359s/iter; left time: 1220.8867s\n",
      "Epoch: 6 cost time: 84.97136664390564\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.8415262 Vali Loss: 1.0162466 Test Loss: 4.3193998\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.7635168\n",
      "\tspeed: 1.0770s/iter; left time: 9560.7007s\n",
      "\titers: 200, epoch: 7 | loss: 0.9790399\n",
      "\tspeed: 0.1520s/iter; left time: 1334.5310s\n",
      "Epoch: 7 cost time: 85.37327241897583\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.8394268 Vali Loss: 1.0295898 Test Loss: 4.2987008\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "( accuracy, f1: 0.762 & 0.706 \n",
      "472, 472\n",
      "79296, 79296\n",
      " accuracy, f1: 0.386 & 0.319 \n",
      "######################  coffee_data_c.csv_Autoformer_14_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1622\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 0.6990936\n",
      "\tspeed: 0.6299s/iter; left time: 6300.1161s\n",
      "\titers: 200, epoch: 1 | loss: 0.9480433\n",
      "\tspeed: 0.0751s/iter; left time: 743.3536s\n",
      "Epoch: 1 cost time: 71.53075909614563\n",
      "Epoch: 1, Steps: 202 | Train Loss: 0.9006545 Vali Loss: 1.0077631 Test Loss: 4.2798052\n",
      "Validation loss decreased (inf --> 1.007763).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7979143\n",
      "\tspeed: 0.5293s/iter; left time: 5186.7974s\n",
      "\titers: 200, epoch: 2 | loss: 0.8735740\n",
      "\tspeed: 0.0615s/iter; left time: 596.7183s\n",
      "Epoch: 2 cost time: 39.14496040344238\n",
      "Epoch: 2, Steps: 202 | Train Loss: 0.8828664 Vali Loss: 1.0243797 Test Loss: 4.2806807\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4477843\n",
      "\tspeed: 0.5002s/iter; left time: 4800.1696s\n",
      "\titers: 200, epoch: 3 | loss: 0.9216342\n",
      "\tspeed: 0.0605s/iter; left time: 574.6853s\n",
      "Epoch: 3 cost time: 38.15405893325806\n",
      "Epoch: 3, Steps: 202 | Train Loss: 0.8716487 Vali Loss: 0.9959686 Test Loss: 4.2932539\n",
      "Validation loss decreased (1.007763 --> 0.995969).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.9151384\n",
      "\tspeed: 0.4961s/iter; left time: 4660.4046s\n",
      "\titers: 200, epoch: 4 | loss: 0.7740363\n",
      "\tspeed: 0.0599s/iter; left time: 557.1095s\n",
      "Epoch: 4 cost time: 37.92940950393677\n",
      "Epoch: 4, Steps: 202 | Train Loss: 0.8618991 Vali Loss: 1.0362269 Test Loss: 4.2952919\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7153304\n",
      "\tspeed: 0.4907s/iter; left time: 4510.5592s\n",
      "\titers: 200, epoch: 5 | loss: 0.6880928\n",
      "\tspeed: 0.0594s/iter; left time: 540.1038s\n",
      "Epoch: 5 cost time: 37.58396363258362\n",
      "Epoch: 5, Steps: 202 | Train Loss: 0.8577385 Vali Loss: 1.0814197 Test Loss: 4.3095989\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.6572483\n",
      "\tspeed: 0.5694s/iter; left time: 5119.4328s\n",
      "\titers: 200, epoch: 6 | loss: 0.6545972\n",
      "\tspeed: 0.0642s/iter; left time: 571.1411s\n",
      "Epoch: 6 cost time: 42.167529821395874\n",
      "Epoch: 6, Steps: 202 | Train Loss: 0.8517434 Vali Loss: 1.0392076 Test Loss: 4.2884941\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.6944603\n",
      "\tspeed: 0.5408s/iter; left time: 4752.8155s\n",
      "\titers: 200, epoch: 7 | loss: 0.9096206\n",
      "\tspeed: 0.0616s/iter; left time: 535.3426s\n",
      "Epoch: 7 cost time: 40.341540575027466\n",
      "Epoch: 7, Steps: 202 | Train Loss: 0.8508745 Vali Loss: 1.0765471 Test Loss: 4.2985048\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.0304520\n",
      "\tspeed: 0.5331s/iter; left time: 4577.6847s\n",
      "\titers: 200, epoch: 8 | loss: 0.8093150\n",
      "\tspeed: 0.0643s/iter; left time: 545.8448s\n",
      "Epoch: 8 cost time: 42.35382652282715\n",
      "Epoch: 8, Steps: 202 | Train Loss: 0.8488507 Vali Loss: 1.0289236 Test Loss: 4.2967944\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftM_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "( accuracy, f1: 0.762 & 0.767 \n",
      "456, 456\n",
      "114912, 114912\n",
      " accuracy, f1: 0.363 & 0.284 \n",
      "######################  coffee_data_c.csv_Autoformer_14_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 0.9349032\n",
      "\tspeed: 0.3392s/iter; left time: 3375.4872s\n",
      "\titers: 200, epoch: 1 | loss: 0.5976114\n",
      "\tspeed: 0.0733s/iter; left time: 722.1587s\n",
      "Epoch: 1 cost time: 42.39172387123108\n",
      "Epoch: 1, Steps: 201 | Train Loss: 0.8947019 Vali Loss: 0.9975910 Test Loss: 4.2898021\n",
      "Validation loss decreased (inf --> 0.997591).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.0751328\n",
      "\tspeed: 0.5606s/iter; left time: 5465.6830s\n",
      "\titers: 200, epoch: 2 | loss: 0.8043227\n",
      "\tspeed: 0.0659s/iter; left time: 635.5637s\n",
      "Epoch: 2 cost time: 41.901551485061646\n",
      "Epoch: 2, Steps: 201 | Train Loss: 0.8780397 Vali Loss: 1.0693439 Test Loss: 4.2991590\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2588993\n",
      "\tspeed: 0.5527s/iter; left time: 5278.0290s\n",
      "\titers: 200, epoch: 3 | loss: 0.8176970\n",
      "\tspeed: 0.0655s/iter; left time: 618.8739s\n",
      "Epoch: 3 cost time: 41.19993042945862\n",
      "Epoch: 3, Steps: 201 | Train Loss: 0.8549984 Vali Loss: 1.0138640 Test Loss: 4.3531728\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.4192457\n",
      "\tspeed: 0.6193s/iter; left time: 5789.4590s\n",
      "\titers: 200, epoch: 4 | loss: 0.8205427\n",
      "\tspeed: 0.0667s/iter; left time: 616.6556s\n",
      "Epoch: 4 cost time: 44.04199838638306\n",
      "Epoch: 4, Steps: 201 | Train Loss: 0.8388739 Vali Loss: 1.0996556 Test Loss: 4.3391109\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6165416\n",
      "\tspeed: 0.5370s/iter; left time: 4912.1245s\n",
      "\titers: 200, epoch: 5 | loss: 0.6898355\n",
      "\tspeed: 0.0655s/iter; left time: 592.2585s\n",
      "Epoch: 5 cost time: 41.23868012428284\n",
      "Epoch: 5, Steps: 201 | Train Loss: 0.8260628 Vali Loss: 1.1019231 Test Loss: 4.3669286\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.6315756\n",
      "\tspeed: 0.5482s/iter; left time: 4903.8676s\n",
      "\titers: 200, epoch: 6 | loss: 0.7494220\n",
      "\tspeed: 0.0656s/iter; left time: 580.4971s\n",
      "Epoch: 6 cost time: 42.392452001571655\n",
      "Epoch: 6, Steps: 201 | Train Loss: 0.8215023 Vali Loss: 1.1299409 Test Loss: 4.3571849\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftM_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "( accuracy, f1: 0.723 & 0.771 \n",
      "440, 440\n",
      "147840, 147840\n",
      " accuracy, f1: 0.364 & 0.291 \n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "models = ['Autoformer', 'Informer', 'Transformer', 'DLinear', 'NLinear']\n",
    "data_paths = ['gold_data_c.csv', 'silver_data_c.csv', 'crude_oil_data_c.csv', 'natural_gas_data_c.csv', 'corn_data_c.csv', 'coffee_data_c.csv']\n",
    "label_lens = [14, 28, 42, 56]\n",
    "pred_lens = [14, 28, 42, 56]\n",
    "args = dotdict()\n",
    "\n",
    "for data_path in data_paths:\n",
    "    for pred_len in pred_lens:\n",
    "\n",
    "        # basic config\n",
    "        args.is_training = 1\n",
    "        args.model_id = 'classification'\n",
    "        args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "        # data loader\n",
    "        args.data = 'custom'\n",
    "        args.root_path = './dataset/commodity/'\n",
    "        args.data_path = data_path\n",
    "        args.features = 'M'\n",
    "        args.target = 'OT'\n",
    "        args.freq = 'd'\n",
    "        args.checkpoints = './checkpoints/'\n",
    "\n",
    "        # forecasting task\n",
    "        args.seq_len = 96\n",
    "        args.label_len = pred_len\n",
    "        args.pred_len = pred_len\n",
    "\n",
    "        # DLinear\n",
    "        args.individual = False\n",
    "\n",
    "        # Formers \n",
    "        args.embed_type = 0\n",
    "        args.enc_in = 6\n",
    "        args.dec_in = 6\n",
    "        args.c_out = 6\n",
    "        args.d_model = 512\n",
    "        args.n_heads = 8\n",
    "        args.e_layers = 2\n",
    "        args.d_layers = 1\n",
    "        args.d_ff = 2048\n",
    "        args.moving_avg = 25\n",
    "        args.factor = 3\n",
    "        args.distil = True\n",
    "        args.dropout = 0.05\n",
    "        args.embed = 'timeF'\n",
    "        args.do_predict = True\n",
    "\n",
    "        # optimization\n",
    "        args.num_workers = 10\n",
    "        args.itr = 1\n",
    "        args.train_epochs = 50\n",
    "        args.patience = 5\n",
    "        args.learning_rate = 0.0005\n",
    "        args.batch_size = 8\n",
    "        args.lradj = 'type1'\n",
    "        args.des = 'Exp'\n",
    "\n",
    "        # GPU\n",
    "        args.gpu = 0\n",
    "        args.devices = '0'\n",
    "        args.use_gpu = True\n",
    "        args.use_multi_gpu = False\n",
    "\n",
    "        # print('Args in experiment:')\n",
    "        # print(args)\n",
    "\n",
    "        Exp = Exp_Main\n",
    "        print(f'######################  {data_path}_{args.model}_{pred_len}_{pred_len}  ######################')\n",
    "        if args.is_training:\n",
    "            for ii in range(args.itr):\n",
    "                # setting record of experiments\n",
    "                setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                    args.data_path.split(\"_\")[0],\n",
    "                    args.model_id,\n",
    "                    args.model,\n",
    "                    args.data,\n",
    "                    args.features,\n",
    "                    args.seq_len,\n",
    "                    args.label_len,\n",
    "                    args.pred_len,\n",
    "                    args.d_model,\n",
    "                    args.n_heads,\n",
    "                    args.e_layers,\n",
    "                    args.d_layers,\n",
    "                    args.d_ff,\n",
    "                    args.factor,\n",
    "                    args.embed,\n",
    "                    args.distil,\n",
    "                    args.des, ii)\n",
    "\n",
    "                exp = Exp(args)  # set experiments\n",
    "                print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                exp.train(setting)\n",
    "\n",
    "                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.test(setting)\n",
    "\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################  gold_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 1.1885455\n",
      "\tspeed: 0.1483s/iter; left time: 1513.2607s\n",
      "\titers: 200, epoch: 1 | loss: 1.1154827\n",
      "\tspeed: 0.0522s/iter; left time: 526.9359s\n",
      "Epoch: 1 cost time: 21.398499965667725\n",
      "Epoch: 1, Steps: 206 | Train Loss: 1.2541343 Vali Loss: 1.4313439 Test Loss: 1.3314521\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.4439062\n",
      "\tspeed: 0.3182s/iter; left time: 3180.6061s\n",
      "\titers: 200, epoch: 2 | loss: 1.1518065\n",
      "\tspeed: 0.0575s/iter; left time: 569.3947s\n",
      "Epoch: 2 cost time: 18.805977821350098\n",
      "Epoch: 2, Steps: 206 | Train Loss: 1.2498268 Vali Loss: 1.4357511 Test Loss: 1.3314862\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.3159962\n",
      "\tspeed: 0.3191s/iter; left time: 3123.4437s\n",
      "\titers: 200, epoch: 3 | loss: 1.3706958\n",
      "\tspeed: 0.0536s/iter; left time: 518.8572s\n",
      "Epoch: 3 cost time: 19.554500102996826\n",
      "Epoch: 3, Steps: 206 | Train Loss: 1.2490942 Vali Loss: 1.4353628 Test Loss: 1.3310817\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2611365\n",
      "\tspeed: 0.3264s/iter; left time: 3128.3495s\n",
      "\titers: 200, epoch: 4 | loss: 1.1699156\n",
      "\tspeed: 0.0529s/iter; left time: 501.2727s\n",
      "Epoch: 4 cost time: 19.77127766609192\n",
      "Epoch: 4, Steps: 206 | Train Loss: 1.2493818 Vali Loss: 1.4309461 Test Loss: 1.3310711\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.1152043\n",
      "\tspeed: 0.3651s/iter; left time: 3423.7322s\n",
      "\titers: 200, epoch: 5 | loss: 1.2246528\n",
      "\tspeed: 0.0568s/iter; left time: 526.4970s\n",
      "Epoch: 5 cost time: 20.488709211349487\n",
      "Epoch: 5, Steps: 206 | Train Loss: 1.2490070 Vali Loss: 1.4346985 Test Loss: 1.3310515\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2246288\n",
      "\tspeed: 0.3554s/iter; left time: 3258.9762s\n",
      "\titers: 200, epoch: 6 | loss: 1.2794007\n",
      "\tspeed: 0.0572s/iter; left time: 518.8608s\n",
      "Epoch: 6 cost time: 20.53505229949951\n",
      "Epoch: 6, Steps: 206 | Train Loss: 1.2490812 Vali Loss: 1.4334333 Test Loss: 1.3310446\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.4435550\n",
      "\tspeed: 0.3620s/iter; left time: 3245.1350s\n",
      "\titers: 200, epoch: 7 | loss: 1.3158722\n",
      "\tspeed: 0.0545s/iter; left time: 483.0606s\n",
      "Epoch: 7 cost time: 19.93435311317444\n",
      "Epoch: 7, Steps: 206 | Train Loss: 1.2492504 Vali Loss: 1.4340520 Test Loss: 1.3310337\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2976043\n",
      "\tspeed: 0.3290s/iter; left time: 2882.1050s\n",
      "\titers: 200, epoch: 8 | loss: 1.2063844\n",
      "\tspeed: 0.0524s/iter; left time: 454.0895s\n",
      "Epoch: 8 cost time: 18.056467056274414\n",
      "Epoch: 8, Steps: 206 | Train Loss: 1.2490712 Vali Loss: 1.4296485 Test Loss: 1.3310337\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2611083\n",
      "\tspeed: 0.3081s/iter; left time: 2635.2368s\n",
      "\titers: 200, epoch: 9 | loss: 1.2428664\n",
      "\tspeed: 0.0530s/iter; left time: 448.0676s\n",
      "Epoch: 9 cost time: 18.14570379257202\n",
      "Epoch: 9, Steps: 206 | Train Loss: 1.2491576 Vali Loss: 1.4346797 Test Loss: 1.3310322\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2976140\n",
      "\tspeed: 0.3095s/iter; left time: 2583.2265s\n",
      "\titers: 200, epoch: 10 | loss: 1.2428710\n",
      "\tspeed: 0.0531s/iter; left time: 437.8933s\n",
      "Epoch: 10 cost time: 18.258214712142944\n",
      "Epoch: 10, Steps: 206 | Train Loss: 1.2490677 Vali Loss: 1.4327916 Test Loss: 1.3310317\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.1151648\n",
      "\tspeed: 0.3099s/iter; left time: 2522.4927s\n",
      "\titers: 200, epoch: 11 | loss: 1.3341054\n",
      "\tspeed: 0.0534s/iter; left time: 429.2463s\n",
      "Epoch: 11 cost time: 18.351166248321533\n",
      "Epoch: 11, Steps: 206 | Train Loss: 1.2496880 Vali Loss: 1.4334201 Test Loss: 1.3310307\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.0421957\n",
      "\tspeed: 0.3125s/iter; left time: 2479.8414s\n",
      "\titers: 200, epoch: 12 | loss: 1.1334019\n",
      "\tspeed: 0.0536s/iter; left time: 420.3455s\n",
      "Epoch: 12 cost time: 18.702641010284424\n",
      "Epoch: 12, Steps: 206 | Train Loss: 1.2492444 Vali Loss: 1.4346782 Test Loss: 1.3310307\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.2428489\n",
      "\tspeed: 0.3108s/iter; left time: 2402.3638s\n",
      "\titers: 200, epoch: 13 | loss: 1.2793550\n",
      "\tspeed: 0.0543s/iter; left time: 414.1385s\n",
      "Epoch: 13 cost time: 18.490435123443604\n",
      "Epoch: 13, Steps: 206 | Train Loss: 1.2494220 Vali Loss: 1.4346784 Test Loss: 1.3310310\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "accuracy, f1: 0.696 & 0.730 \n",
      "488, 488\n",
      "######################  gold_data_c.csv_Autoformer_28_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1636\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 1.4530008\n",
      "\tspeed: 0.1222s/iter; left time: 1234.3278s\n",
      "\titers: 200, epoch: 1 | loss: 1.2066293\n",
      "\tspeed: 0.0615s/iter; left time: 614.8372s\n",
      "Epoch: 1 cost time: 19.497562170028687\n",
      "Epoch: 1, Steps: 204 | Train Loss: 1.2520813 Vali Loss: 1.4432735 Test Loss: 1.3301126\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2794904\n",
      "\tspeed: 0.3177s/iter; left time: 3143.8235s\n",
      "\titers: 200, epoch: 2 | loss: 1.3250000\n",
      "\tspeed: 0.0579s/iter; left time: 567.7335s\n",
      "Epoch: 2 cost time: 19.14606523513794\n",
      "Epoch: 2, Steps: 204 | Train Loss: 1.2495602 Vali Loss: 1.4422525 Test Loss: 1.3300683\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1608127\n",
      "\tspeed: 0.3193s/iter; left time: 3094.5730s\n",
      "\titers: 200, epoch: 3 | loss: 1.2519848\n",
      "\tspeed: 0.0575s/iter; left time: 551.4773s\n",
      "Epoch: 3 cost time: 18.926342487335205\n",
      "Epoch: 3, Steps: 204 | Train Loss: 1.2499333 Vali Loss: 1.4428589 Test Loss: 1.3300256\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2246503\n",
      "\tspeed: 0.3150s/iter; left time: 2989.1992s\n",
      "\titers: 200, epoch: 4 | loss: 1.3341256\n",
      "\tspeed: 0.0579s/iter; left time: 544.0293s\n",
      "Epoch: 4 cost time: 19.093878507614136\n",
      "Epoch: 4, Steps: 204 | Train Loss: 1.2495432 Vali Loss: 1.4421890 Test Loss: 1.3300065\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2246323\n",
      "\tspeed: 0.3133s/iter; left time: 2908.9861s\n",
      "\titers: 200, epoch: 5 | loss: 1.1881497\n",
      "\tspeed: 0.0578s/iter; left time: 531.0315s\n",
      "Epoch: 5 cost time: 19.00815200805664\n",
      "Epoch: 5, Steps: 204 | Train Loss: 1.2496630 Vali Loss: 1.4441417 Test Loss: 1.3300064\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2337445\n",
      "\tspeed: 0.3153s/iter; left time: 2863.5017s\n",
      "\titers: 200, epoch: 6 | loss: 1.2793589\n",
      "\tspeed: 0.0581s/iter; left time: 521.4377s\n",
      "Epoch: 6 cost time: 19.20172142982483\n",
      "Epoch: 6, Steps: 204 | Train Loss: 1.2495243 Vali Loss: 1.4415296 Test Loss: 1.3300000\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.1881006\n",
      "\tspeed: 0.3175s/iter; left time: 2818.2185s\n",
      "\titers: 200, epoch: 7 | loss: 1.3340863\n",
      "\tspeed: 0.0580s/iter; left time: 508.7427s\n",
      "Epoch: 7 cost time: 19.152726650238037\n",
      "Epoch: 7, Steps: 204 | Train Loss: 1.2497452 Vali Loss: 1.4425076 Test Loss: 1.3300008\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.3067421\n",
      "\tspeed: 0.3153s/iter; left time: 2734.3600s\n",
      "\titers: 200, epoch: 8 | loss: 1.2884748\n",
      "\tspeed: 0.0581s/iter; left time: 497.9548s\n",
      "Epoch: 8 cost time: 19.15811061859131\n",
      "Epoch: 8, Steps: 204 | Train Loss: 1.2494754 Vali Loss: 1.4431572 Test Loss: 1.3299986\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.1698751\n",
      "\tspeed: 0.3146s/iter; left time: 2663.9696s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 9 | loss: 1.2154955\n",
      "\tspeed: 0.0580s/iter; left time: 485.2764s\n",
      "Epoch: 9 cost time: 19.14796018600464\n",
      "Epoch: 9, Steps: 204 | Train Loss: 1.2493841 Vali Loss: 1.4418538 Test Loss: 1.3299989\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2154937\n",
      "\tspeed: 0.3161s/iter; left time: 2612.9649s\n",
      "\titers: 200, epoch: 10 | loss: 1.3431932\n",
      "\tspeed: 0.0581s/iter; left time: 474.1312s\n",
      "Epoch: 10 cost time: 19.135037660598755\n",
      "Epoch: 10, Steps: 204 | Train Loss: 1.2492937 Vali Loss: 1.4421794 Test Loss: 1.3299983\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.3796924\n",
      "\tspeed: 0.3142s/iter; left time: 2532.7286s\n",
      "\titers: 200, epoch: 11 | loss: 1.3431909\n",
      "\tspeed: 0.0578s/iter; left time: 460.2885s\n",
      "Epoch: 11 cost time: 19.048155069351196\n",
      "Epoch: 11, Steps: 204 | Train Loss: 1.2496076 Vali Loss: 1.4425050 Test Loss: 1.3299981\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "accuracy, f1: 0.690 & 0.711 \n",
      "472, 472\n",
      "######################  gold_data_c.csv_Autoformer_42_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1622\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 1.2733901\n",
      "\tspeed: 0.1247s/iter; left time: 1247.1503s\n",
      "\titers: 200, epoch: 1 | loss: 1.2673255\n",
      "\tspeed: 0.0617s/iter; left time: 611.1843s\n",
      "Epoch: 1 cost time: 19.61358094215393\n",
      "Epoch: 1, Steps: 202 | Train Loss: 1.2512194 Vali Loss: 1.4479765 Test Loss: 1.3339622\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2855505\n",
      "\tspeed: 0.3165s/iter; left time: 3101.0000s\n",
      "\titers: 200, epoch: 2 | loss: 1.2551116\n",
      "\tspeed: 0.0618s/iter; left time: 599.3120s\n",
      "Epoch: 2 cost time: 19.675692319869995\n",
      "Epoch: 2, Steps: 202 | Train Loss: 1.2490335 Vali Loss: 1.4477735 Test Loss: 1.3339911\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2246245\n",
      "\tspeed: 0.3158s/iter; left time: 3030.6097s\n",
      "\titers: 200, epoch: 3 | loss: 1.1881292\n",
      "\tspeed: 0.0624s/iter; left time: 592.3920s\n",
      "Epoch: 3 cost time: 19.69719171524048\n",
      "Epoch: 3, Steps: 202 | Train Loss: 1.2491406 Vali Loss: 1.4486440 Test Loss: 1.3339287\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2611182\n",
      "\tspeed: 0.3171s/iter; left time: 2979.6088s\n",
      "\titers: 200, epoch: 4 | loss: 1.2185292\n",
      "\tspeed: 0.0621s/iter; left time: 576.9979s\n",
      "Epoch: 4 cost time: 19.802579879760742\n",
      "Epoch: 4, Steps: 202 | Train Loss: 1.2490276 Vali Loss: 1.4493406 Test Loss: 1.3339214\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.3036695\n",
      "\tspeed: 0.3165s/iter; left time: 2909.9690s\n",
      "\titers: 200, epoch: 5 | loss: 1.2246021\n",
      "\tspeed: 0.0620s/iter; left time: 563.3323s\n",
      "Epoch: 5 cost time: 19.710768222808838\n",
      "Epoch: 5, Steps: 202 | Train Loss: 1.2493494 Vali Loss: 1.4493370 Test Loss: 1.3339196\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2975806\n",
      "\tspeed: 0.3166s/iter; left time: 2846.9968s\n",
      "\titers: 200, epoch: 6 | loss: 1.1637777\n",
      "\tspeed: 0.0618s/iter; left time: 549.1378s\n",
      "Epoch: 6 cost time: 19.719042778015137\n",
      "Epoch: 6, Steps: 202 | Train Loss: 1.2489828 Vali Loss: 1.4490979 Test Loss: 1.3339137\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.1941805\n",
      "\tspeed: 0.3177s/iter; left time: 2792.6561s\n",
      "\titers: 200, epoch: 7 | loss: 1.3401384\n",
      "\tspeed: 0.0623s/iter; left time: 540.9958s\n",
      "Epoch: 7 cost time: 19.804646730422974\n",
      "Epoch: 7, Steps: 202 | Train Loss: 1.2489802 Vali Loss: 1.4488652 Test Loss: 1.3339145\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "accuracy, f1: 0.710 & 0.746 \n",
      "456, 456\n",
      "######################  gold_data_c.csv_Autoformer_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 1.2567449\n",
      "\tspeed: 0.1290s/iter; left time: 1284.0295s\n",
      "\titers: 200, epoch: 1 | loss: 1.2977041\n",
      "\tspeed: 0.0669s/iter; left time: 659.0521s\n",
      "Epoch: 1 cost time: 20.510886907577515\n",
      "Epoch: 1, Steps: 201 | Train Loss: 1.2514755 Vali Loss: 1.4540163 Test Loss: 1.3350794\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2793927\n",
      "\tspeed: 0.3206s/iter; left time: 3125.3933s\n",
      "\titers: 200, epoch: 2 | loss: 1.2337687\n",
      "\tspeed: 0.0671s/iter; left time: 647.8333s\n",
      "Epoch: 2 cost time: 20.565881967544556\n",
      "Epoch: 2, Steps: 201 | Train Loss: 1.2494607 Vali Loss: 1.4543381 Test Loss: 1.3350371\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2474200\n",
      "\tspeed: 0.3195s/iter; left time: 3050.5610s\n",
      "\titers: 200, epoch: 3 | loss: 1.3204029\n",
      "\tspeed: 0.0675s/iter; left time: 638.0712s\n",
      "Epoch: 3 cost time: 20.71726679801941\n",
      "Epoch: 3, Steps: 201 | Train Loss: 1.2493977 Vali Loss: 1.4522471 Test Loss: 1.3350370\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2017834\n",
      "\tspeed: 0.3213s/iter; left time: 3003.0791s\n",
      "\titers: 200, epoch: 4 | loss: 1.2245922\n",
      "\tspeed: 0.0676s/iter; left time: 624.7758s\n",
      "Epoch: 4 cost time: 20.635961771011353\n",
      "Epoch: 4, Steps: 201 | Train Loss: 1.2493784 Vali Loss: 1.4539391 Test Loss: 1.3350177\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2793292\n",
      "\tspeed: 0.3205s/iter; left time: 2931.7607s\n",
      "\titers: 200, epoch: 5 | loss: 1.2565230\n",
      "\tspeed: 0.0671s/iter; left time: 607.0493s\n",
      "Epoch: 5 cost time: 20.5942862033844\n",
      "Epoch: 5, Steps: 201 | Train Loss: 1.2493710 Vali Loss: 1.4545050 Test Loss: 1.3350137\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.3066782\n",
      "\tspeed: 0.3217s/iter; left time: 2878.2355s\n",
      "\titers: 200, epoch: 6 | loss: 1.2747735\n",
      "\tspeed: 0.0672s/iter; left time: 594.4218s\n",
      "Epoch: 6 cost time: 20.65384602546692\n",
      "Epoch: 6, Steps: 201 | Train Loss: 1.2493678 Vali Loss: 1.4535543 Test Loss: 1.3350135\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2154634\n",
      "\tspeed: 0.3212s/iter; left time: 2808.8728s\n",
      "\titers: 200, epoch: 7 | loss: 1.3386099\n",
      "\tspeed: 0.0671s/iter; left time: 580.4628s\n",
      "Epoch: 7 cost time: 20.659252882003784\n",
      "Epoch: 7, Steps: 201 | Train Loss: 1.2493657 Vali Loss: 1.4541229 Test Loss: 1.3350120\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2838792\n",
      "\tspeed: 0.3237s/iter; left time: 2765.2833s\n",
      "\titers: 200, epoch: 8 | loss: 1.1789756\n",
      "\tspeed: 0.0668s/iter; left time: 564.2315s\n",
      "Epoch: 8 cost time: 20.85700011253357\n",
      "Epoch: 8, Steps: 201 | Train Loss: 1.2493650 Vali Loss: 1.4531721 Test Loss: 1.3350111\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.708 & 0.742 \n",
      "440, 440\n",
      "######################  silver_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1649\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 1.2538544\n",
      "\tspeed: 0.1185s/iter; left time: 1208.6811s\n",
      "\titers: 200, epoch: 1 | loss: 1.3262566\n",
      "\tspeed: 0.0541s/iter; left time: 546.8480s\n",
      "Epoch: 1 cost time: 18.42830514907837\n",
      "Epoch: 1, Steps: 206 | Train Loss: 1.2499989 Vali Loss: 1.3560820 Test Loss: 1.2832497\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.1998698\n",
      "\tspeed: 0.3130s/iter; left time: 3128.6773s\n",
      "\titers: 200, epoch: 2 | loss: 1.1997875\n",
      "\tspeed: 0.0541s/iter; left time: 535.2067s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 cost time: 18.46051335334778\n",
      "Epoch: 2, Steps: 206 | Train Loss: 1.2471112 Vali Loss: 1.3566152 Test Loss: 1.2831415\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2895266\n",
      "\tspeed: 0.3124s/iter; left time: 3057.6536s\n",
      "\titers: 200, epoch: 3 | loss: 1.2715768\n",
      "\tspeed: 0.0539s/iter; left time: 522.6767s\n",
      "Epoch: 3 cost time: 18.463436365127563\n",
      "Epoch: 3, Steps: 206 | Train Loss: 1.2467942 Vali Loss: 1.3619884 Test Loss: 1.2829658\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2535319\n",
      "\tspeed: 0.3123s/iter; left time: 2992.8935s\n",
      "\titers: 200, epoch: 4 | loss: 1.3074936\n",
      "\tspeed: 0.0538s/iter; left time: 510.5664s\n",
      "Epoch: 4 cost time: 18.46105456352234\n",
      "Epoch: 4, Steps: 206 | Train Loss: 1.2463994 Vali Loss: 1.3607258 Test Loss: 1.2829438\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2356154\n",
      "\tspeed: 0.3100s/iter; left time: 2907.1586s\n",
      "\titers: 200, epoch: 5 | loss: 1.2355934\n",
      "\tspeed: 0.0541s/iter; left time: 501.6835s\n",
      "Epoch: 5 cost time: 18.433610677719116\n",
      "Epoch: 5, Steps: 206 | Train Loss: 1.2464680 Vali Loss: 1.3644273 Test Loss: 1.2829263\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.3254240\n",
      "\tspeed: 0.3112s/iter; left time: 2854.1413s\n",
      "\titers: 200, epoch: 6 | loss: 1.1456389\n",
      "\tspeed: 0.0539s/iter; left time: 489.3590s\n",
      "Epoch: 6 cost time: 18.455845832824707\n",
      "Epoch: 6, Steps: 206 | Train Loss: 1.2465447 Vali Loss: 1.3613245 Test Loss: 1.2829231\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "accuracy, f1: 0.696 & 0.726 \n",
      "488, 488\n",
      "######################  silver_data_c.csv_Autoformer_28_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1635\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 1.3351723\n",
      "\tspeed: 0.1210s/iter; left time: 1222.5280s\n",
      "\titers: 200, epoch: 1 | loss: 1.1999201\n",
      "\tspeed: 0.0574s/iter; left time: 574.0038s\n",
      "Epoch: 1 cost time: 18.929566621780396\n",
      "Epoch: 1, Steps: 204 | Train Loss: 1.2519986 Vali Loss: 1.3664180 Test Loss: 1.2815806\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.1998619\n",
      "\tspeed: 0.3153s/iter; left time: 3120.7072s\n",
      "\titers: 200, epoch: 2 | loss: 1.1547678\n",
      "\tspeed: 0.0576s/iter; left time: 564.4345s\n",
      "Epoch: 2 cost time: 19.040209531784058\n",
      "Epoch: 2, Steps: 204 | Train Loss: 1.2497575 Vali Loss: 1.3678287 Test Loss: 1.2814132\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1816187\n",
      "\tspeed: 0.3154s/iter; left time: 3057.0119s\n",
      "\titers: 200, epoch: 3 | loss: 1.2355934\n",
      "\tspeed: 0.0578s/iter; left time: 554.4299s\n",
      "Epoch: 3 cost time: 19.069520235061646\n",
      "Epoch: 3, Steps: 204 | Train Loss: 1.2498145 Vali Loss: 1.3668270 Test Loss: 1.2813735\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2715087\n",
      "\tspeed: 0.3163s/iter; left time: 3001.7766s\n",
      "\titers: 200, epoch: 4 | loss: 1.2265748\n",
      "\tspeed: 0.0580s/iter; left time: 544.3762s\n",
      "Epoch: 4 cost time: 19.09670066833496\n",
      "Epoch: 4, Steps: 204 | Train Loss: 1.2495208 Vali Loss: 1.3667946 Test Loss: 1.2813447\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2265440\n",
      "\tspeed: 0.3160s/iter; left time: 2933.7106s\n",
      "\titers: 200, epoch: 5 | loss: 1.2355263\n",
      "\tspeed: 0.0579s/iter; left time: 531.8834s\n",
      "Epoch: 5 cost time: 19.127950191497803\n",
      "Epoch: 5, Steps: 204 | Train Loss: 1.2497664 Vali Loss: 1.3664588 Test Loss: 1.2813290\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2625055\n",
      "\tspeed: 0.3196s/iter; left time: 2902.6172s\n",
      "\titers: 200, epoch: 6 | loss: 1.3343972\n",
      "\tspeed: 0.0579s/iter; left time: 519.8447s\n",
      "Epoch: 6 cost time: 19.139734506607056\n",
      "Epoch: 6, Steps: 204 | Train Loss: 1.2498490 Vali Loss: 1.3674241 Test Loss: 1.2813314\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "accuracy, f1: 0.702 & 0.725 \n",
      "472, 472\n",
      "######################  silver_data_c.csv_Autoformer_42_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1621\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 1.2776247\n",
      "\tspeed: 0.1277s/iter; left time: 1276.7824s\n",
      "\titers: 200, epoch: 1 | loss: 1.2356550\n",
      "\tspeed: 0.0614s/iter; left time: 607.8746s\n",
      "Epoch: 1 cost time: 19.89739155769348\n",
      "Epoch: 1, Steps: 202 | Train Loss: 1.2527458 Vali Loss: 1.3727176 Test Loss: 1.2834063\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2776452\n",
      "\tspeed: 0.3240s/iter; left time: 3174.8339s\n",
      "\titers: 200, epoch: 2 | loss: 1.1878448\n",
      "\tspeed: 0.0619s/iter; left time: 600.7173s\n",
      "Epoch: 2 cost time: 20.14943838119507\n",
      "Epoch: 2, Steps: 202 | Train Loss: 1.2508856 Vali Loss: 1.3732682 Test Loss: 1.2834996\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2715156\n",
      "\tspeed: 0.3186s/iter; left time: 3057.3330s\n",
      "\titers: 200, epoch: 3 | loss: 1.2055999\n",
      "\tspeed: 0.0624s/iter; left time: 592.7533s\n",
      "Epoch: 3 cost time: 19.76438069343567\n",
      "Epoch: 3, Steps: 202 | Train Loss: 1.2508085 Vali Loss: 1.3731109 Test Loss: 1.2833378\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.3733737\n",
      "\tspeed: 0.3189s/iter; left time: 2995.8035s\n",
      "\titers: 200, epoch: 4 | loss: 1.2595096\n",
      "\tspeed: 0.0621s/iter; left time: 577.6579s\n",
      "Epoch: 4 cost time: 19.87201452255249\n",
      "Epoch: 4, Steps: 202 | Train Loss: 1.2507763 Vali Loss: 1.3717040 Test Loss: 1.2833158\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2954525\n",
      "\tspeed: 0.3193s/iter; left time: 2935.4094s\n",
      "\titers: 200, epoch: 5 | loss: 1.3074381\n",
      "\tspeed: 0.0623s/iter; left time: 566.4099s\n",
      "Epoch: 5 cost time: 19.75814390182495\n",
      "Epoch: 5, Steps: 202 | Train Loss: 1.2509444 Vali Loss: 1.3703129 Test Loss: 1.2833086\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2714636\n",
      "\tspeed: 0.3228s/iter; left time: 2901.9172s\n",
      "\titers: 200, epoch: 6 | loss: 1.3254030\n",
      "\tspeed: 0.0623s/iter; left time: 553.5072s\n",
      "Epoch: 6 cost time: 19.855290412902832\n",
      "Epoch: 6, Steps: 202 | Train Loss: 1.2507620 Vali Loss: 1.3714625 Test Loss: 1.2833056\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2834504\n",
      "\tspeed: 0.3200s/iter; left time: 2812.3351s\n",
      "\titers: 200, epoch: 7 | loss: 1.2415087\n",
      "\tspeed: 0.0623s/iter; left time: 541.4685s\n",
      "Epoch: 7 cost time: 19.876875162124634\n",
      "Epoch: 7, Steps: 202 | Train Loss: 1.2506695 Vali Loss: 1.3716903 Test Loss: 1.2833027\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2055491\n",
      "\tspeed: 0.3198s/iter; left time: 2745.9822s\n",
      "\titers: 200, epoch: 8 | loss: 1.1815602\n",
      "\tspeed: 0.0622s/iter; left time: 527.6307s\n",
      "Epoch: 8 cost time: 19.866031169891357\n",
      "Epoch: 8, Steps: 202 | Train Loss: 1.2508187 Vali Loss: 1.3733037 Test Loss: 1.2833028\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2774602\n",
      "\tspeed: 0.3213s/iter; left time: 2694.3865s\n",
      "\titers: 200, epoch: 9 | loss: 1.2295182\n",
      "\tspeed: 0.0625s/iter; left time: 518.1621s\n",
      "Epoch: 9 cost time: 19.97977375984192\n",
      "Epoch: 9, Steps: 202 | Train Loss: 1.2506093 Vali Loss: 1.3703068 Test Loss: 1.2833027\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2774751\n",
      "\tspeed: 0.3207s/iter; left time: 2624.4509s\n",
      "\titers: 200, epoch: 10 | loss: 1.2235087\n",
      "\tspeed: 0.0622s/iter; left time: 502.9828s\n",
      "Epoch: 10 cost time: 19.85795760154724\n",
      "Epoch: 10, Steps: 202 | Train Loss: 1.2508467 Vali Loss: 1.3700763 Test Loss: 1.2833024\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.3433734\n",
      "\tspeed: 0.3214s/iter; left time: 2564.7106s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 200, epoch: 11 | loss: 1.1516013\n",
      "\tspeed: 0.0623s/iter; left time: 491.3727s\n",
      "Epoch: 11 cost time: 19.880714893341064\n",
      "Epoch: 11, Steps: 202 | Train Loss: 1.2507270 Vali Loss: 1.3712285 Test Loss: 1.2833022\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.2115302\n",
      "\tspeed: 0.3194s/iter; left time: 2484.3669s\n",
      "\titers: 200, epoch: 12 | loss: 1.3253894\n",
      "\tspeed: 0.0623s/iter; left time: 478.5355s\n",
      "Epoch: 12 cost time: 19.905675888061523\n",
      "Epoch: 12, Steps: 202 | Train Loss: 1.2506082 Vali Loss: 1.3714588 Test Loss: 1.2833019\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.2474918\n",
      "\tspeed: 0.3196s/iter; left time: 2421.3871s\n",
      "\titers: 200, epoch: 13 | loss: 1.3254161\n",
      "\tspeed: 0.0623s/iter; left time: 466.1447s\n",
      "Epoch: 13 cost time: 19.849976062774658\n",
      "Epoch: 13, Steps: 202 | Train Loss: 1.2508457 Vali Loss: 1.3700758 Test Loss: 1.2833021\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 1.2534840\n",
      "\tspeed: 0.3200s/iter; left time: 2359.7071s\n",
      "\titers: 200, epoch: 14 | loss: 1.2355189\n",
      "\tspeed: 0.0624s/iter; left time: 454.2180s\n",
      "Epoch: 14 cost time: 19.8952796459198\n",
      "Epoch: 14, Steps: 202 | Train Loss: 1.2502812 Vali Loss: 1.3709978 Test Loss: 1.2833021\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 1.2954295\n",
      "\tspeed: 0.3201s/iter; left time: 2296.3954s\n",
      "\titers: 200, epoch: 15 | loss: 1.1815680\n",
      "\tspeed: 0.0623s/iter; left time: 440.9088s\n",
      "Epoch: 15 cost time: 19.89981436729431\n",
      "Epoch: 15, Steps: 202 | Train Loss: 1.2504001 Vali Loss: 1.3728417 Test Loss: 1.2833019\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 1.2175144\n",
      "\tspeed: 0.3188s/iter; left time: 2222.3847s\n",
      "\titers: 200, epoch: 16 | loss: 1.2654648\n",
      "\tspeed: 0.0625s/iter; left time: 429.6532s\n",
      "Epoch: 16 cost time: 19.871041536331177\n",
      "Epoch: 16, Steps: 202 | Train Loss: 1.2510232 Vali Loss: 1.3735335 Test Loss: 1.2833019\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.52587890625e-08\n",
      "\titers: 100, epoch: 17 | loss: 1.2295096\n",
      "\tspeed: 0.3181s/iter; left time: 2152.8947s\n",
      "\titers: 200, epoch: 17 | loss: 1.2175242\n",
      "\tspeed: 0.0622s/iter; left time: 415.0055s\n",
      "Epoch: 17 cost time: 19.782567262649536\n",
      "Epoch: 17, Steps: 202 | Train Loss: 1.2504001 Vali Loss: 1.3712281 Test Loss: 1.2833019\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.62939453125e-09\n",
      "\titers: 100, epoch: 18 | loss: 1.2354995\n",
      "\tspeed: 0.3196s/iter; left time: 2098.6745s\n",
      "\titers: 200, epoch: 18 | loss: 1.2055298\n",
      "\tspeed: 0.0621s/iter; left time: 401.8263s\n",
      "Epoch: 18 cost time: 19.822492599487305\n",
      "Epoch: 18, Steps: 202 | Train Loss: 1.2503407 Vali Loss: 1.3707671 Test Loss: 1.2833019\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "accuracy, f1: 0.718 & 0.758 \n",
      "456, 456\n",
      "######################  silver_data_c.csv_Autoformer_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1607\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 1.2896197\n",
      "\tspeed: 0.1302s/iter; left time: 1289.1231s\n",
      "\titers: 200, epoch: 1 | loss: 1.2356718\n",
      "\tspeed: 0.0667s/iter; left time: 653.9343s\n",
      "Epoch: 1 cost time: 20.561622142791748\n",
      "Epoch: 1, Steps: 200 | Train Loss: 1.2526726 Vali Loss: 1.3721033 Test Loss: 1.2850233\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.1906202\n",
      "\tspeed: 0.3225s/iter; left time: 3128.8157s\n",
      "\titers: 200, epoch: 2 | loss: 1.2535256\n",
      "\tspeed: 0.0673s/iter; left time: 646.0123s\n",
      "Epoch: 2 cost time: 20.570938110351562\n",
      "Epoch: 2, Steps: 200 | Train Loss: 1.2514559 Vali Loss: 1.3735262 Test Loss: 1.2849361\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1770952\n",
      "\tspeed: 0.3231s/iter; left time: 3069.7605s\n",
      "\titers: 200, epoch: 3 | loss: 1.3119235\n",
      "\tspeed: 0.0671s/iter; left time: 630.7256s\n",
      "Epoch: 3 cost time: 20.644184350967407\n",
      "Epoch: 3, Steps: 200 | Train Loss: 1.2514091 Vali Loss: 1.3725672 Test Loss: 1.2849195\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2355039\n",
      "\tspeed: 0.3238s/iter; left time: 3011.8153s\n",
      "\titers: 200, epoch: 4 | loss: 1.2130191\n",
      "\tspeed: 0.0673s/iter; left time: 619.2123s\n",
      "Epoch: 4 cost time: 20.737060070037842\n",
      "Epoch: 4, Steps: 200 | Train Loss: 1.2513442 Vali Loss: 1.3742431 Test Loss: 1.2849089\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.3793292\n",
      "\tspeed: 0.3259s/iter; left time: 2966.3229s\n",
      "\titers: 200, epoch: 5 | loss: 1.2265090\n",
      "\tspeed: 0.0672s/iter; left time: 604.7952s\n",
      "Epoch: 5 cost time: 20.674004316329956\n",
      "Epoch: 5, Steps: 200 | Train Loss: 1.2512260 Vali Loss: 1.3742418 Test Loss: 1.2849073\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2714490\n",
      "\tspeed: 0.3239s/iter; left time: 2882.6112s\n",
      "\titers: 200, epoch: 6 | loss: 1.2849289\n",
      "\tspeed: 0.0671s/iter; left time: 590.6793s\n",
      "Epoch: 6 cost time: 20.70676827430725\n",
      "Epoch: 6, Steps: 200 | Train Loss: 1.2511998 Vali Loss: 1.3723669 Test Loss: 1.2849054\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.726 & 0.766 \n",
      "440, 440\n",
      "######################  crude_oil_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1651\n",
      "val 239\n",
      "test 490\n",
      "\titers: 100, epoch: 1 | loss: 1.1042845\n",
      "\tspeed: 0.1196s/iter; left time: 1220.0458s\n",
      "\titers: 200, epoch: 1 | loss: 1.3223366\n",
      "\tspeed: 0.0543s/iter; left time: 548.2184s\n",
      "Epoch: 1 cost time: 18.577513694763184\n",
      "Epoch: 1, Steps: 206 | Train Loss: 1.2539043 Vali Loss: 1.2786942 Test Loss: 1.3864005\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.0707743\n",
      "\tspeed: 0.3141s/iter; left time: 3139.5367s\n",
      "\titers: 200, epoch: 2 | loss: 1.1880682\n",
      "\tspeed: 0.0541s/iter; left time: 534.9341s\n",
      "Epoch: 2 cost time: 18.54166889190674\n",
      "Epoch: 2, Steps: 206 | Train Loss: 1.2497180 Vali Loss: 1.2838031 Test Loss: 1.3862848\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2381366\n",
      "\tspeed: 0.3129s/iter; left time: 3062.5999s\n",
      "\titers: 200, epoch: 3 | loss: 1.2884079\n",
      "\tspeed: 0.0543s/iter; left time: 526.3734s\n",
      "Epoch: 3 cost time: 18.533082723617554\n",
      "Epoch: 3, Steps: 206 | Train Loss: 1.2494736 Vali Loss: 1.2813805 Test Loss: 1.3861836\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.3219473\n",
      "\tspeed: 0.3140s/iter; left time: 3009.3815s\n",
      "\titers: 200, epoch: 4 | loss: 1.3890662\n",
      "\tspeed: 0.0540s/iter; left time: 512.4795s\n",
      "Epoch: 4 cost time: 18.51874566078186\n",
      "Epoch: 4, Steps: 206 | Train Loss: 1.2491688 Vali Loss: 1.2813534 Test Loss: 1.3861549\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.3387631\n",
      "\tspeed: 0.3129s/iter; left time: 2934.0774s\n",
      "\titers: 200, epoch: 5 | loss: 1.1710048\n",
      "\tspeed: 0.0542s/iter; left time: 502.5036s\n",
      "Epoch: 5 cost time: 18.4980525970459\n",
      "Epoch: 5, Steps: 206 | Train Loss: 1.2494740 Vali Loss: 1.2761399 Test Loss: 1.3861467\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.1710023\n",
      "\tspeed: 0.3145s/iter; left time: 2884.2686s\n",
      "\titers: 200, epoch: 6 | loss: 1.2381114\n",
      "\tspeed: 0.0541s/iter; left time: 490.8120s\n",
      "Epoch: 6 cost time: 18.5567307472229\n",
      "Epoch: 6, Steps: 206 | Train Loss: 1.2490529 Vali Loss: 1.2819211 Test Loss: 1.3861454\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2380513\n",
      "\tspeed: 0.3131s/iter; left time: 2806.7420s\n",
      "\titers: 200, epoch: 7 | loss: 1.2548414\n",
      "\tspeed: 0.0542s/iter; left time: 480.4263s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 cost time: 18.545409440994263\n",
      "Epoch: 7, Steps: 206 | Train Loss: 1.2492098 Vali Loss: 1.2761319 Test Loss: 1.3861380\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2213056\n",
      "\tspeed: 0.3156s/iter; left time: 2764.2800s\n",
      "\titers: 200, epoch: 8 | loss: 1.2044894\n",
      "\tspeed: 0.0539s/iter; left time: 466.9292s\n",
      "Epoch: 8 cost time: 18.54739284515381\n",
      "Epoch: 8, Steps: 206 | Train Loss: 1.2491285 Vali Loss: 1.2830718 Test Loss: 1.3861393\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.3219464\n",
      "\tspeed: 0.3145s/iter; left time: 2689.7348s\n",
      "\titers: 200, epoch: 9 | loss: 1.3386514\n",
      "\tspeed: 0.0540s/iter; left time: 456.5714s\n",
      "Epoch: 9 cost time: 18.557457447052002\n",
      "Epoch: 9, Steps: 206 | Train Loss: 1.2494500 Vali Loss: 1.2836471 Test Loss: 1.3861362\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2883538\n",
      "\tspeed: 0.3122s/iter; left time: 2605.5689s\n",
      "\titers: 200, epoch: 10 | loss: 1.1709656\n",
      "\tspeed: 0.0539s/iter; left time: 444.5702s\n",
      "Epoch: 10 cost time: 18.460111618041992\n",
      "Epoch: 10, Steps: 206 | Train Loss: 1.2494493 Vali Loss: 1.2761308 Test Loss: 1.3861368\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.3218915\n",
      "\tspeed: 0.3181s/iter; left time: 2590.0404s\n",
      "\titers: 200, epoch: 11 | loss: 1.1877187\n",
      "\tspeed: 0.0539s/iter; left time: 433.5045s\n",
      "Epoch: 11 cost time: 18.49186372756958\n",
      "Epoch: 11, Steps: 206 | Train Loss: 1.2493700 Vali Loss: 1.2871165 Test Loss: 1.3861361\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.1709611\n",
      "\tspeed: 0.3141s/iter; left time: 2492.7020s\n",
      "\titers: 200, epoch: 12 | loss: 1.4560568\n",
      "\tspeed: 0.0538s/iter; left time: 421.2237s\n",
      "Epoch: 12 cost time: 18.490822792053223\n",
      "Epoch: 12, Steps: 206 | Train Loss: 1.2495276 Vali Loss: 1.2790217 Test Loss: 1.3861363\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.0535856\n",
      "\tspeed: 0.3142s/iter; left time: 2428.2226s\n",
      "\titers: 200, epoch: 13 | loss: 1.2548528\n",
      "\tspeed: 0.0541s/iter; left time: 412.6816s\n",
      "Epoch: 13 cost time: 18.542014837265015\n",
      "Epoch: 13, Steps: 206 | Train Loss: 1.2499374 Vali Loss: 1.2784432 Test Loss: 1.3861361\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 1.1206779\n",
      "\tspeed: 0.3121s/iter; left time: 2347.6676s\n",
      "\titers: 200, epoch: 14 | loss: 1.4058268\n",
      "\tspeed: 0.0539s/iter; left time: 400.1481s\n",
      "Epoch: 14 cost time: 18.47426414489746\n",
      "Epoch: 14, Steps: 206 | Train Loss: 1.2488773 Vali Loss: 1.2842255 Test Loss: 1.3861359\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 1.2548075\n",
      "\tspeed: 0.3128s/iter; left time: 2288.4462s\n",
      "\titers: 200, epoch: 15 | loss: 1.4728439\n",
      "\tspeed: 0.0539s/iter; left time: 388.6405s\n",
      "Epoch: 15 cost time: 18.47396492958069\n",
      "Epoch: 15, Steps: 206 | Train Loss: 1.2496129 Vali Loss: 1.2819127 Test Loss: 1.3861362\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 490\n",
      "accuracy, f1: 0.696 & 0.734 \n",
      "488, 488\n",
      "######################  crude_oil_data_c.csv_Autoformer_28_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1637\n",
      "val 225\n",
      "test 476\n",
      "\titers: 100, epoch: 1 | loss: 1.3138692\n",
      "\tspeed: 0.1215s/iter; left time: 1227.5333s\n",
      "\titers: 200, epoch: 1 | loss: 1.1545588\n",
      "\tspeed: 0.0577s/iter; left time: 577.2467s\n",
      "Epoch: 1 cost time: 19.017807960510254\n",
      "Epoch: 1, Steps: 204 | Train Loss: 1.2530882 Vali Loss: 1.2785615 Test Loss: 1.3886399\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.1042305\n",
      "\tspeed: 0.3159s/iter; left time: 3126.1763s\n",
      "\titers: 200, epoch: 2 | loss: 1.3723069\n",
      "\tspeed: 0.0579s/iter; left time: 567.5528s\n",
      "Epoch: 2 cost time: 19.086235761642456\n",
      "Epoch: 2, Steps: 204 | Train Loss: 1.2510542 Vali Loss: 1.2784637 Test Loss: 1.3885244\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.4057440\n",
      "\tspeed: 0.3154s/iter; left time: 3056.8554s\n",
      "\titers: 200, epoch: 3 | loss: 1.2297457\n",
      "\tspeed: 0.0582s/iter; left time: 557.9517s\n",
      "Epoch: 3 cost time: 19.099826097488403\n",
      "Epoch: 3, Steps: 204 | Train Loss: 1.2506043 Vali Loss: 1.2790145 Test Loss: 1.3884847\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.3218826\n",
      "\tspeed: 0.3173s/iter; left time: 3011.2578s\n",
      "\titers: 200, epoch: 4 | loss: 1.2296797\n",
      "\tspeed: 0.0586s/iter; left time: 550.0084s\n",
      "Epoch: 4 cost time: 19.214849948883057\n",
      "Epoch: 4, Steps: 204 | Train Loss: 1.2508615 Vali Loss: 1.2801942 Test Loss: 1.3884648\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.3051134\n",
      "\tspeed: 0.3173s/iter; left time: 2946.1886s\n",
      "\titers: 200, epoch: 5 | loss: 1.1374038\n",
      "\tspeed: 0.0585s/iter; left time: 537.5326s\n",
      "Epoch: 5 cost time: 19.368418216705322\n",
      "Epoch: 5, Steps: 204 | Train Loss: 1.2506836 Vali Loss: 1.2774982 Test Loss: 1.3884637\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2883133\n",
      "\tspeed: 0.3193s/iter; left time: 2899.8562s\n",
      "\titers: 200, epoch: 6 | loss: 1.2463979\n",
      "\tspeed: 0.0590s/iter; left time: 530.1848s\n",
      "Epoch: 6 cost time: 19.302252531051636\n",
      "Epoch: 6, Steps: 204 | Train Loss: 1.2508853 Vali Loss: 1.2807864 Test Loss: 1.3884583\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.1290139\n",
      "\tspeed: 0.3179s/iter; left time: 2822.3493s\n",
      "\titers: 200, epoch: 7 | loss: 1.1960934\n",
      "\tspeed: 0.0583s/iter; left time: 511.8646s\n",
      "Epoch: 7 cost time: 19.248993635177612\n",
      "Epoch: 7, Steps: 204 | Train Loss: 1.2506769 Vali Loss: 1.2777908 Test Loss: 1.3884571\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2128541\n",
      "\tspeed: 0.3188s/iter; left time: 2765.0883s\n",
      "\titers: 200, epoch: 8 | loss: 1.2799367\n",
      "\tspeed: 0.0582s/iter; left time: 499.0174s\n",
      "Epoch: 8 cost time: 19.27535915374756\n",
      "Epoch: 8, Steps: 204 | Train Loss: 1.2510036 Vali Loss: 1.2792870 Test Loss: 1.3884562\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.3134751\n",
      "\tspeed: 0.3187s/iter; left time: 2699.1559s\n",
      "\titers: 200, epoch: 9 | loss: 1.3050840\n",
      "\tspeed: 0.0582s/iter; left time: 486.8874s\n",
      "Epoch: 9 cost time: 19.284693002700806\n",
      "Epoch: 9, Steps: 204 | Train Loss: 1.2505910 Vali Loss: 1.2798851 Test Loss: 1.3884554\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.1960942\n",
      "\tspeed: 0.3213s/iter; left time: 2655.5712s\n",
      "\titers: 200, epoch: 10 | loss: 1.3553977\n",
      "\tspeed: 0.0582s/iter; left time: 474.8916s\n",
      "Epoch: 10 cost time: 19.255741834640503\n",
      "Epoch: 10, Steps: 204 | Train Loss: 1.2510834 Vali Loss: 1.2795860 Test Loss: 1.3884555\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 476\n",
      "accuracy, f1: 0.690 & 0.714 \n",
      "472, 472\n",
      "######################  crude_oil_data_c.csv_Autoformer_42_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1623\n",
      "val 211\n",
      "test 462\n",
      "\titers: 100, epoch: 1 | loss: 1.1599504\n",
      "\tspeed: 0.1246s/iter; left time: 1245.6376s\n",
      "\titers: 200, epoch: 1 | loss: 1.2270080\n",
      "\tspeed: 0.0618s/iter; left time: 612.2222s\n",
      "Epoch: 1 cost time: 19.624507904052734\n",
      "Epoch: 1, Steps: 202 | Train Loss: 1.2532339 Vali Loss: 1.2785208 Test Loss: 1.3897985\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2772706\n",
      "\tspeed: 0.3200s/iter; left time: 3135.9848s\n",
      "\titers: 200, epoch: 2 | loss: 1.2102289\n",
      "\tspeed: 0.0627s/iter; left time: 608.1291s\n",
      "Epoch: 2 cost time: 19.91123867034912\n",
      "Epoch: 2, Steps: 202 | Train Loss: 1.2515287 Vali Loss: 1.2738037 Test Loss: 1.3898151\n",
      "Updating learning rate to 0.00025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 3 | loss: 1.3721827\n",
      "\tspeed: 0.3213s/iter; left time: 3083.2910s\n",
      "\titers: 200, epoch: 3 | loss: 1.2827518\n",
      "\tspeed: 0.0620s/iter; left time: 589.2069s\n",
      "Epoch: 3 cost time: 19.74759316444397\n",
      "Epoch: 3, Steps: 202 | Train Loss: 1.2510322 Vali Loss: 1.2743198 Test Loss: 1.3896923\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.1877104\n",
      "\tspeed: 0.3182s/iter; left time: 2989.4943s\n",
      "\titers: 200, epoch: 4 | loss: 1.3218490\n",
      "\tspeed: 0.0620s/iter; left time: 576.7524s\n",
      "Epoch: 4 cost time: 19.76295232772827\n",
      "Epoch: 4, Steps: 202 | Train Loss: 1.2511122 Vali Loss: 1.2760290 Test Loss: 1.3896793\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.3218576\n",
      "\tspeed: 0.3196s/iter; left time: 2937.9008s\n",
      "\titers: 200, epoch: 5 | loss: 1.2547786\n",
      "\tspeed: 0.0621s/iter; left time: 564.8115s\n",
      "Epoch: 5 cost time: 19.851348638534546\n",
      "Epoch: 5, Steps: 202 | Train Loss: 1.2510193 Vali Loss: 1.2751591 Test Loss: 1.3896695\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2994971\n",
      "\tspeed: 0.3201s/iter; left time: 2877.9049s\n",
      "\titers: 200, epoch: 6 | loss: 1.3106725\n",
      "\tspeed: 0.0618s/iter; left time: 549.1692s\n",
      "Epoch: 6 cost time: 19.7162868976593\n",
      "Epoch: 6, Steps: 202 | Train Loss: 1.2512628 Vali Loss: 1.2753735 Test Loss: 1.3896691\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.3162522\n",
      "\tspeed: 0.3221s/iter; left time: 2830.7846s\n",
      "\titers: 200, epoch: 7 | loss: 1.2435969\n",
      "\tspeed: 0.0621s/iter; left time: 539.6390s\n",
      "Epoch: 7 cost time: 20.048284769058228\n",
      "Epoch: 7, Steps: 202 | Train Loss: 1.2512059 Vali Loss: 1.2736518 Test Loss: 1.3896669\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.1876940\n",
      "\tspeed: 0.3213s/iter; left time: 2759.4136s\n",
      "\titers: 200, epoch: 8 | loss: 1.3386171\n",
      "\tspeed: 0.0619s/iter; left time: 525.3981s\n",
      "Epoch: 8 cost time: 19.779728651046753\n",
      "Epoch: 8, Steps: 202 | Train Loss: 1.2512593 Vali Loss: 1.2747266 Test Loss: 1.3896669\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2659504\n",
      "\tspeed: 0.3220s/iter; left time: 2699.7887s\n",
      "\titers: 200, epoch: 9 | loss: 1.3609976\n",
      "\tspeed: 0.0621s/iter; left time: 514.4997s\n",
      "Epoch: 9 cost time: 20.121062994003296\n",
      "Epoch: 9, Steps: 202 | Train Loss: 1.2513969 Vali Loss: 1.2751553 Test Loss: 1.3896658\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.1988817\n",
      "\tspeed: 0.3191s/iter; left time: 2611.4822s\n",
      "\titers: 200, epoch: 10 | loss: 1.3050779\n",
      "\tspeed: 0.0620s/iter; left time: 500.8711s\n",
      "Epoch: 10 cost time: 19.78587579727173\n",
      "Epoch: 10, Steps: 202 | Train Loss: 1.2513135 Vali Loss: 1.2751553 Test Loss: 1.3896658\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.2380035\n",
      "\tspeed: 0.3197s/iter; left time: 2551.8345s\n",
      "\titers: 200, epoch: 11 | loss: 1.3162469\n",
      "\tspeed: 0.0618s/iter; left time: 486.8485s\n",
      "Epoch: 11 cost time: 19.784435510635376\n",
      "Epoch: 11, Steps: 202 | Train Loss: 1.2513131 Vali Loss: 1.2755849 Test Loss: 1.3896655\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.2938877\n",
      "\tspeed: 0.3191s/iter; left time: 2482.5080s\n",
      "\titers: 200, epoch: 12 | loss: 1.3442012\n",
      "\tspeed: 0.0622s/iter; left time: 477.4309s\n",
      "Epoch: 12 cost time: 19.827226161956787\n",
      "Epoch: 12, Steps: 202 | Train Loss: 1.2513693 Vali Loss: 1.2773048 Test Loss: 1.3896656\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 462\n",
      "accuracy, f1: 0.706 & 0.741 \n",
      "456, 456\n",
      "######################  crude_oil_data_c.csv_Autoformer_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 1.2675463\n",
      "\tspeed: 0.1303s/iter; left time: 1296.7395s\n",
      "\titers: 200, epoch: 1 | loss: 1.2884545\n",
      "\tspeed: 0.0664s/iter; left time: 654.4726s\n",
      "Epoch: 1 cost time: 20.608436584472656\n",
      "Epoch: 1, Steps: 201 | Train Loss: 1.2534162 Vali Loss: 1.2737094 Test Loss: 1.3893517\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2548811\n",
      "\tspeed: 0.3252s/iter; left time: 3170.9389s\n",
      "\titers: 200, epoch: 2 | loss: 1.3051611\n",
      "\tspeed: 0.0672s/iter; left time: 648.3186s\n",
      "Epoch: 2 cost time: 20.70976734161377\n",
      "Epoch: 2, Steps: 201 | Train Loss: 1.2516153 Vali Loss: 1.2701679 Test Loss: 1.3893200\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1541889\n",
      "\tspeed: 0.3276s/iter; left time: 3128.3615s\n",
      "\titers: 200, epoch: 3 | loss: 1.2380333\n",
      "\tspeed: 0.0706s/iter; left time: 666.8020s\n",
      "Epoch: 3 cost time: 21.32085347175598\n",
      "Epoch: 3, Steps: 201 | Train Loss: 1.2516981 Vali Loss: 1.2685465 Test Loss: 1.3892696\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2463858\n",
      "\tspeed: 0.3555s/iter; left time: 3323.5410s\n",
      "\titers: 200, epoch: 4 | loss: 1.2044635\n",
      "\tspeed: 0.0715s/iter; left time: 661.3597s\n",
      "Epoch: 4 cost time: 22.019673347473145\n",
      "Epoch: 4, Steps: 201 | Train Loss: 1.2514651 Vali Loss: 1.2701069 Test Loss: 1.3892591\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2212434\n",
      "\tspeed: 0.3720s/iter; left time: 3402.5752s\n",
      "\titers: 200, epoch: 5 | loss: 1.2841209\n",
      "\tspeed: 0.0692s/iter; left time: 626.1042s\n",
      "Epoch: 5 cost time: 22.27286696434021\n",
      "Epoch: 5, Steps: 201 | Train Loss: 1.2516648 Vali Loss: 1.2722007 Test Loss: 1.3892568\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2589661\n",
      "\tspeed: 0.3757s/iter; left time: 3361.0340s\n",
      "\titers: 200, epoch: 6 | loss: 1.2715397\n",
      "\tspeed: 0.0715s/iter; left time: 632.4622s\n",
      "Epoch: 6 cost time: 22.64948868751526\n",
      "Epoch: 6, Steps: 201 | Train Loss: 1.2513896 Vali Loss: 1.2708006 Test Loss: 1.3892542\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2631564\n",
      "\tspeed: 0.3856s/iter; left time: 3371.8182s\n",
      "\titers: 200, epoch: 7 | loss: 1.3008778\n",
      "\tspeed: 0.0719s/iter; left time: 621.8865s\n",
      "Epoch: 7 cost time: 23.8246853351593\n",
      "Epoch: 7, Steps: 201 | Train Loss: 1.2515539 Vali Loss: 1.2667826 Test Loss: 1.3892535\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.1876929\n",
      "\tspeed: 0.3734s/iter; left time: 3190.0136s\n",
      "\titers: 200, epoch: 8 | loss: 1.2631615\n",
      "\tspeed: 0.0714s/iter; left time: 602.6022s\n",
      "Epoch: 8 cost time: 22.353846788406372\n",
      "Epoch: 8, Steps: 201 | Train Loss: 1.2515537 Vali Loss: 1.2721976 Test Loss: 1.3892536\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2296178\n",
      "\tspeed: 0.3528s/iter; left time: 2943.5307s\n",
      "\titers: 200, epoch: 9 | loss: 1.2799132\n",
      "\tspeed: 0.0672s/iter; left time: 554.1467s\n",
      "Epoch: 9 cost time: 20.78474497795105\n",
      "Epoch: 9, Steps: 201 | Train Loss: 1.2514692 Vali Loss: 1.2704505 Test Loss: 1.3892529\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2924898\n",
      "\tspeed: 0.3290s/iter; left time: 2678.6418s\n",
      "\titers: 200, epoch: 10 | loss: 1.3637586\n",
      "\tspeed: 0.0673s/iter; left time: 541.2806s\n",
      "Epoch: 10 cost time: 20.93036127090454\n",
      "Epoch: 10, Steps: 201 | Train Loss: 1.2513233 Vali Loss: 1.2678300 Test Loss: 1.3892527\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.2422047\n",
      "\tspeed: 0.3291s/iter; left time: 2613.7680s\n",
      "\titers: 200, epoch: 11 | loss: 1.2715349\n",
      "\tspeed: 0.0675s/iter; left time: 529.2543s\n",
      "Epoch: 11 cost time: 20.871574878692627\n",
      "Epoch: 11, Steps: 201 | Train Loss: 1.2515523 Vali Loss: 1.2702752 Test Loss: 1.3892525\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.1583432\n",
      "\tspeed: 0.3302s/iter; left time: 2555.9404s\n",
      "\titers: 200, epoch: 12 | loss: 1.2379894\n",
      "\tspeed: 0.0675s/iter; left time: 515.5316s\n",
      "Epoch: 12 cost time: 20.911033153533936\n",
      "Epoch: 12, Steps: 201 | Train Loss: 1.2514682 Vali Loss: 1.2697513 Test Loss: 1.3892527\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, f1: 0.714 & 0.750 \n",
      "448, 448\n",
      "######################  natural_gas_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1651\n",
      "val 239\n",
      "test 490\n",
      "\titers: 100, epoch: 1 | loss: 1.4431866\n",
      "\tspeed: 0.1211s/iter; left time: 1235.5437s\n",
      "\titers: 200, epoch: 1 | loss: 1.2428716\n",
      "\tspeed: 0.0549s/iter; left time: 554.0548s\n",
      "Epoch: 1 cost time: 18.774941205978394\n",
      "Epoch: 1, Steps: 206 | Train Loss: 1.2521652 Vali Loss: 1.2512732 Test Loss: 1.3512181\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2242521\n",
      "\tspeed: 0.3517s/iter; left time: 3515.6319s\n",
      "\titers: 200, epoch: 2 | loss: 1.2242854\n",
      "\tspeed: 0.0578s/iter; left time: 572.0463s\n",
      "Epoch: 2 cost time: 20.762283086776733\n",
      "Epoch: 2, Steps: 206 | Train Loss: 1.2480028 Vali Loss: 1.2536728 Test Loss: 1.3510987\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2971210\n",
      "\tspeed: 0.3340s/iter; left time: 3269.3092s\n",
      "\titers: 200, epoch: 3 | loss: 1.3334712\n",
      "\tspeed: 0.0552s/iter; left time: 534.4713s\n",
      "Epoch: 3 cost time: 19.182259798049927\n",
      "Epoch: 3, Steps: 206 | Train Loss: 1.2475249 Vali Loss: 1.2504503 Test Loss: 1.3510208\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.4609857\n",
      "\tspeed: 0.3279s/iter; left time: 3142.2078s\n",
      "\titers: 200, epoch: 4 | loss: 1.2788043\n",
      "\tspeed: 0.0551s/iter; left time: 522.5088s\n",
      "Epoch: 4 cost time: 19.112241506576538\n",
      "Epoch: 4, Steps: 206 | Train Loss: 1.2478289 Vali Loss: 1.2522892 Test Loss: 1.3509769\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2970064\n",
      "\tspeed: 0.3319s/iter; left time: 3112.6131s\n",
      "\titers: 200, epoch: 5 | loss: 1.2423329\n",
      "\tspeed: 0.0555s/iter; left time: 514.5959s\n",
      "Epoch: 5 cost time: 19.187525033950806\n",
      "Epoch: 5, Steps: 206 | Train Loss: 1.2475365 Vali Loss: 1.2485285 Test Loss: 1.3509841\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.3698726\n",
      "\tspeed: 0.3312s/iter; left time: 3037.4498s\n",
      "\titers: 200, epoch: 6 | loss: 1.1512002\n",
      "\tspeed: 0.0554s/iter; left time: 502.1840s\n",
      "Epoch: 6 cost time: 19.185738801956177\n",
      "Epoch: 6, Steps: 206 | Train Loss: 1.2478816 Vali Loss: 1.2516510 Test Loss: 1.3509676\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.3516285\n",
      "\tspeed: 0.3306s/iter; left time: 2963.9464s\n",
      "\titers: 200, epoch: 7 | loss: 1.3516247\n",
      "\tspeed: 0.0556s/iter; left time: 492.9085s\n",
      "Epoch: 7 cost time: 19.274237155914307\n",
      "Epoch: 7, Steps: 206 | Train Loss: 1.2476099 Vali Loss: 1.2541659 Test Loss: 1.3509690\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2058848\n",
      "\tspeed: 0.3281s/iter; left time: 2874.0475s\n",
      "\titers: 200, epoch: 8 | loss: 1.2787602\n",
      "\tspeed: 0.0553s/iter; left time: 479.1373s\n",
      "Epoch: 8 cost time: 19.1667160987854\n",
      "Epoch: 8, Steps: 206 | Train Loss: 1.2475178 Vali Loss: 1.2541642 Test Loss: 1.3509665\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.1147302\n",
      "\tspeed: 0.3337s/iter; left time: 2854.1931s\n",
      "\titers: 200, epoch: 9 | loss: 1.2422764\n",
      "\tspeed: 0.0552s/iter; left time: 467.0226s\n",
      "Epoch: 9 cost time: 19.21833372116089\n",
      "Epoch: 9, Steps: 206 | Train Loss: 1.2470756 Vali Loss: 1.2547909 Test Loss: 1.3509651\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.1875948\n",
      "\tspeed: 0.3331s/iter; left time: 2780.3214s\n",
      "\titers: 200, epoch: 10 | loss: 1.3151815\n",
      "\tspeed: 0.0552s/iter; left time: 455.2275s\n",
      "Epoch: 10 cost time: 19.552525281906128\n",
      "Epoch: 10, Steps: 206 | Train Loss: 1.2473368 Vali Loss: 1.2560472 Test Loss: 1.3509642\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 490\n",
      "accuracy, f1: 0.759 & 0.794 \n",
      "488, 488\n",
      "######################  natural_gas_data_c.csv_Autoformer_28_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1637\n",
      "val 225\n",
      "test 476\n",
      "\titers: 100, epoch: 1 | loss: 1.2703654\n",
      "\tspeed: 0.1250s/iter; left time: 1262.9390s\n",
      "\titers: 200, epoch: 1 | loss: 1.2971869\n",
      "\tspeed: 0.0588s/iter; left time: 588.2410s\n",
      "Epoch: 1 cost time: 19.518210887908936\n",
      "Epoch: 1, Steps: 204 | Train Loss: 1.2523214 Vali Loss: 1.2504702 Test Loss: 1.3592416\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2425623\n",
      "\tspeed: 0.3258s/iter; left time: 3224.3685s\n",
      "\titers: 200, epoch: 2 | loss: 1.3517697\n",
      "\tspeed: 0.0594s/iter; left time: 582.2601s\n",
      "Epoch: 2 cost time: 19.61788535118103\n",
      "Epoch: 2, Steps: 204 | Train Loss: 1.2508139 Vali Loss: 1.2488657 Test Loss: 1.3592603\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.3061490\n",
      "\tspeed: 0.3294s/iter; left time: 3192.8163s\n",
      "\titers: 200, epoch: 3 | loss: 1.1603463\n",
      "\tspeed: 0.0590s/iter; left time: 565.9964s\n",
      "Epoch: 3 cost time: 19.668933629989624\n",
      "Epoch: 3, Steps: 204 | Train Loss: 1.2504345 Vali Loss: 1.2507063 Test Loss: 1.3591549\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2878512\n",
      "\tspeed: 0.3264s/iter; left time: 3097.2318s\n",
      "\titers: 200, epoch: 4 | loss: 1.2240715\n",
      "\tspeed: 0.0597s/iter; left time: 560.2498s\n",
      "Epoch: 4 cost time: 19.720121383666992\n",
      "Epoch: 4, Steps: 204 | Train Loss: 1.2508355 Vali Loss: 1.2487432 Test Loss: 1.3591422\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2422793\n",
      "\tspeed: 0.3278s/iter; left time: 3043.5403s\n",
      "\titers: 200, epoch: 5 | loss: 1.2331600\n",
      "\tspeed: 0.0596s/iter; left time: 547.7731s\n",
      "Epoch: 5 cost time: 19.703434944152832\n",
      "Epoch: 5, Steps: 204 | Train Loss: 1.2506412 Vali Loss: 1.2487352 Test Loss: 1.3591348\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2058384\n",
      "\tspeed: 0.3537s/iter; left time: 3212.3210s\n",
      "\titers: 200, epoch: 6 | loss: 1.2331822\n",
      "\tspeed: 0.0588s/iter; left time: 527.9411s\n",
      "Epoch: 6 cost time: 20.1878445148468\n",
      "Epoch: 6, Steps: 204 | Train Loss: 1.2502798 Vali Loss: 1.2497065 Test Loss: 1.3591299\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.1056252\n",
      "\tspeed: 0.3203s/iter; left time: 2843.0510s\n",
      "\titers: 200, epoch: 7 | loss: 1.1967232\n",
      "\tspeed: 0.0589s/iter; left time: 517.3650s\n",
      "Epoch: 7 cost time: 19.498655796051025\n",
      "Epoch: 7, Steps: 204 | Train Loss: 1.2501871 Vali Loss: 1.2487290 Test Loss: 1.3591284\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2513806\n",
      "\tspeed: 0.3200s/iter; left time: 2775.5230s\n",
      "\titers: 200, epoch: 8 | loss: 1.2969391\n",
      "\tspeed: 0.0591s/iter; left time: 506.5340s\n",
      "Epoch: 8 cost time: 19.37018346786499\n",
      "Epoch: 8, Steps: 204 | Train Loss: 1.2504530 Vali Loss: 1.2497057 Test Loss: 1.3591291\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.3242906\n",
      "\tspeed: 0.3372s/iter; left time: 2855.7028s\n",
      "\titers: 200, epoch: 9 | loss: 1.2969642\n",
      "\tspeed: 0.0664s/iter; left time: 555.9713s\n",
      "Epoch: 9 cost time: 22.122247219085693\n",
      "Epoch: 9, Steps: 204 | Train Loss: 1.2509424 Vali Loss: 1.2487277 Test Loss: 1.3591272\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2605070\n",
      "\tspeed: 0.3727s/iter; left time: 3080.0904s\n",
      "\titers: 200, epoch: 10 | loss: 1.2422705\n",
      "\tspeed: 0.0620s/iter; left time: 506.5622s\n",
      "Epoch: 10 cost time: 21.344444751739502\n",
      "Epoch: 10, Steps: 204 | Train Loss: 1.2503614 Vali Loss: 1.2497044 Test Loss: 1.3591276\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.3151836\n",
      "\tspeed: 0.3517s/iter; left time: 2835.1358s\n",
      "\titers: 200, epoch: 11 | loss: 1.3425100\n",
      "\tspeed: 0.0613s/iter; left time: 487.9280s\n",
      "Epoch: 11 cost time: 20.786060333251953\n",
      "Epoch: 11, Steps: 204 | Train Loss: 1.2504509 Vali Loss: 1.2487274 Test Loss: 1.3591272\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.2422637\n",
      "\tspeed: 0.3556s/iter; left time: 2793.6157s\n",
      "\titers: 200, epoch: 12 | loss: 1.3060503\n",
      "\tspeed: 0.0612s/iter; left time: 474.7616s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 cost time: 20.811339139938354\n",
      "Epoch: 12, Steps: 204 | Train Loss: 1.2504958 Vali Loss: 1.2503544 Test Loss: 1.3591268\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.2331532\n",
      "\tspeed: 0.3619s/iter; left time: 2769.6801s\n",
      "\titers: 200, epoch: 13 | loss: 1.1694152\n",
      "\tspeed: 0.0615s/iter; left time: 464.8184s\n",
      "Epoch: 13 cost time: 21.152787685394287\n",
      "Epoch: 13, Steps: 204 | Train Loss: 1.2508541 Vali Loss: 1.2503545 Test Loss: 1.3591268\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 1.2058322\n",
      "\tspeed: 0.3565s/iter; left time: 2655.2929s\n",
      "\titers: 200, epoch: 14 | loss: 1.1784996\n",
      "\tspeed: 0.0612s/iter; left time: 449.5456s\n",
      "Epoch: 14 cost time: 20.950096130371094\n",
      "Epoch: 14, Steps: 204 | Train Loss: 1.2503162 Vali Loss: 1.2490526 Test Loss: 1.3591267\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.103515625e-08\n",
      "\titers: 100, epoch: 15 | loss: 1.0964940\n",
      "\tspeed: 0.4661s/iter; left time: 3376.9040s\n",
      "\titers: 200, epoch: 15 | loss: 1.2969434\n",
      "\tspeed: 0.1783s/iter; left time: 1274.1276s\n",
      "Epoch: 15 cost time: 38.8109827041626\n",
      "Epoch: 15, Steps: 204 | Train Loss: 1.2500940 Vali Loss: 1.2493781 Test Loss: 1.3591268\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.0517578125e-08\n",
      "\titers: 100, epoch: 16 | loss: 1.2969353\n",
      "\tspeed: 0.8368s/iter; left time: 5891.7858s\n",
      "\titers: 200, epoch: 16 | loss: 1.2787223\n",
      "\tspeed: 0.1128s/iter; left time: 782.6604s\n",
      "Epoch: 16 cost time: 37.198434829711914\n",
      "Epoch: 16, Steps: 204 | Train Loss: 1.2507632 Vali Loss: 1.2490526 Test Loss: 1.3591268\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 476\n",
      "accuracy, f1: 0.732 & 0.776 \n",
      "472, 472\n",
      "######################  natural_gas_data_c.csv_Autoformer_42_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1623\n",
      "val 211\n",
      "test 462\n",
      "\titers: 100, epoch: 1 | loss: 1.2363948\n",
      "\tspeed: 0.2516s/iter; left time: 2516.5999s\n",
      "\titers: 200, epoch: 1 | loss: 1.2667375\n",
      "\tspeed: 0.1385s/iter; left time: 1371.2995s\n",
      "Epoch: 1 cost time: 40.833192348480225\n",
      "Epoch: 1, Steps: 202 | Train Loss: 1.2535052 Vali Loss: 1.2481782 Test Loss: 1.3623457\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2241238\n",
      "\tspeed: 0.6284s/iter; left time: 6158.0389s\n",
      "\titers: 200, epoch: 2 | loss: 1.2484262\n",
      "\tspeed: 0.1419s/iter; left time: 1375.8822s\n",
      "Epoch: 2 cost time: 42.21056389808655\n",
      "Epoch: 2, Steps: 202 | Train Loss: 1.2514391 Vali Loss: 1.2492843 Test Loss: 1.3622764\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2423316\n",
      "\tspeed: 0.6293s/iter; left time: 6039.2917s\n",
      "\titers: 200, epoch: 3 | loss: 1.1815397\n",
      "\tspeed: 0.1389s/iter; left time: 1318.9077s\n",
      "Epoch: 3 cost time: 41.76796770095825\n",
      "Epoch: 3, Steps: 202 | Train Loss: 1.2516093 Vali Loss: 1.2487850 Test Loss: 1.3622414\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.3212612\n",
      "\tspeed: 0.6317s/iter; left time: 5935.1549s\n",
      "\titers: 200, epoch: 4 | loss: 1.1815403\n",
      "\tspeed: 0.1260s/iter; left time: 1171.5553s\n",
      "Epoch: 4 cost time: 40.75913953781128\n",
      "Epoch: 4, Steps: 202 | Train Loss: 1.2511843 Vali Loss: 1.2492338 Test Loss: 1.3622223\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2058203\n",
      "\tspeed: 0.6037s/iter; left time: 5549.9804s\n",
      "\titers: 200, epoch: 5 | loss: 1.2240487\n",
      "\tspeed: 0.1332s/iter; left time: 1211.6245s\n",
      "Epoch: 5 cost time: 40.65697479248047\n",
      "Epoch: 5, Steps: 202 | Train Loss: 1.2515362 Vali Loss: 1.2490010 Test Loss: 1.3622221\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2483370\n",
      "\tspeed: 0.6089s/iter; left time: 5474.6884s\n",
      "\titers: 200, epoch: 6 | loss: 1.2908626\n",
      "\tspeed: 0.1318s/iter; left time: 1171.6031s\n",
      "Epoch: 6 cost time: 40.077433824539185\n",
      "Epoch: 6, Steps: 202 | Train Loss: 1.2515007 Vali Loss: 1.2494628 Test Loss: 1.3622178\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 462\n",
      "accuracy, f1: 0.718 & 0.763 \n",
      "456, 456\n",
      "######################  natural_gas_data_c.csv_Autoformer_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1609\n",
      "val 197\n",
      "test 448\n",
      "\titers: 100, epoch: 1 | loss: 1.2242599\n",
      "\tspeed: 0.3445s/iter; left time: 3427.6492s\n",
      "\titers: 200, epoch: 1 | loss: 1.2378460\n",
      "\tspeed: 0.1732s/iter; left time: 1706.5428s\n",
      "Epoch: 1 cost time: 54.814263343811035\n",
      "Epoch: 1, Steps: 201 | Train Loss: 1.2536115 Vali Loss: 1.2502422 Test Loss: 1.3628320\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2013673\n",
      "\tspeed: 0.6615s/iter; left time: 6449.8691s\n",
      "\titers: 200, epoch: 2 | loss: 1.2241168\n",
      "\tspeed: 0.1741s/iter; left time: 1680.3266s\n",
      "Epoch: 2 cost time: 41.70107054710388\n",
      "Epoch: 2, Steps: 201 | Train Loss: 1.2519062 Vali Loss: 1.2494930 Test Loss: 1.3628434\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1693895\n",
      "\tspeed: 0.7586s/iter; left time: 7243.4078s\n",
      "\titers: 200, epoch: 3 | loss: 1.2696261\n",
      "\tspeed: 0.1909s/iter; left time: 1804.2329s\n",
      "Epoch: 3 cost time: 53.58716630935669\n",
      "Epoch: 3, Steps: 201 | Train Loss: 1.2518137 Vali Loss: 1.2471914 Test Loss: 1.3628172\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2513790\n",
      "\tspeed: 0.9536s/iter; left time: 8914.5161s\n",
      "\titers: 200, epoch: 4 | loss: 1.2741469\n",
      "\tspeed: 0.1930s/iter; left time: 1784.4765s\n",
      "Epoch: 4 cost time: 64.34940695762634\n",
      "Epoch: 4, Steps: 201 | Train Loss: 1.2517237 Vali Loss: 1.2503916 Test Loss: 1.3627943\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2650439\n",
      "\tspeed: 0.9903s/iter; left time: 9058.5100s\n",
      "\titers: 200, epoch: 5 | loss: 1.2832634\n",
      "\tspeed: 0.1096s/iter; left time: 991.4364s\n",
      "Epoch: 5 cost time: 53.85191869735718\n",
      "Epoch: 5, Steps: 201 | Train Loss: 1.2518961 Vali Loss: 1.2496258 Test Loss: 1.3627886\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2604812\n",
      "\tspeed: 0.3650s/iter; left time: 3265.6021s\n",
      "\titers: 200, epoch: 6 | loss: 1.2969291\n",
      "\tspeed: 0.0685s/iter; left time: 605.6785s\n",
      "Epoch: 6 cost time: 21.72387671470642\n",
      "Epoch: 6, Steps: 201 | Train Loss: 1.2517786 Vali Loss: 1.2482966 Test Loss: 1.3627882\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.3242549\n",
      "\tspeed: 0.3468s/iter; left time: 3032.5048s\n",
      "\titers: 200, epoch: 7 | loss: 1.2650403\n",
      "\tspeed: 0.0692s/iter; left time: 598.2371s\n",
      "Epoch: 7 cost time: 21.84110713005066\n",
      "Epoch: 7, Steps: 201 | Train Loss: 1.2518223 Vali Loss: 1.2481052 Test Loss: 1.3627864\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2468181\n",
      "\tspeed: 0.3434s/iter; left time: 2934.3279s\n",
      "\titers: 200, epoch: 8 | loss: 1.2012551\n",
      "\tspeed: 0.0677s/iter; left time: 571.5147s\n",
      "Epoch: 8 cost time: 21.397338390350342\n",
      "Epoch: 8, Steps: 201 | Train Loss: 1.2517984 Vali Loss: 1.2492443 Test Loss: 1.3627865\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 448\n",
      "accuracy, f1: 0.711 & 0.753 \n",
      "448, 448\n",
      "######################  corn_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1648\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 1.0070145\n",
      "\tspeed: 0.1287s/iter; left time: 1313.2735s\n",
      "\titers: 200, epoch: 1 | loss: 1.2444471\n",
      "\tspeed: 0.0561s/iter; left time: 566.7467s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 19.645917654037476\n",
      "Epoch: 1, Steps: 206 | Train Loss: 1.2526686 Vali Loss: 1.1838048 Test Loss: 1.3621862\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.3460201\n",
      "\tspeed: 0.3276s/iter; left time: 3274.4456s\n",
      "\titers: 200, epoch: 2 | loss: 1.0404321\n",
      "\tspeed: 0.0554s/iter; left time: 548.1711s\n",
      "Epoch: 2 cost time: 19.252249002456665\n",
      "Epoch: 2, Steps: 206 | Train Loss: 1.2503389 Vali Loss: 1.1833932 Test Loss: 1.3623773\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1080586\n",
      "\tspeed: 0.3321s/iter; left time: 3250.8430s\n",
      "\titers: 200, epoch: 3 | loss: 1.2268645\n",
      "\tspeed: 0.0553s/iter; left time: 536.0946s\n",
      "Epoch: 3 cost time: 19.109108448028564\n",
      "Epoch: 3, Steps: 206 | Train Loss: 1.2499438 Vali Loss: 1.1840209 Test Loss: 1.3618450\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2777907\n",
      "\tspeed: 0.3282s/iter; left time: 3144.9771s\n",
      "\titers: 200, epoch: 4 | loss: 1.2098114\n",
      "\tspeed: 0.0550s/iter; left time: 521.8433s\n",
      "Epoch: 4 cost time: 19.184967517852783\n",
      "Epoch: 4, Steps: 206 | Train Loss: 1.2498737 Vali Loss: 1.1834038 Test Loss: 1.3618175\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.1249156\n",
      "\tspeed: 0.3285s/iter; left time: 3080.3665s\n",
      "\titers: 200, epoch: 5 | loss: 1.2438916\n",
      "\tspeed: 0.0554s/iter; left time: 513.5299s\n",
      "Epoch: 5 cost time: 19.14893674850464\n",
      "Epoch: 5, Steps: 206 | Train Loss: 1.2498464 Vali Loss: 1.1875045 Test Loss: 1.3618214\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2778084\n",
      "\tspeed: 0.3252s/iter; left time: 2982.7212s\n",
      "\titers: 200, epoch: 6 | loss: 1.0229743\n",
      "\tspeed: 0.0546s/iter; left time: 495.0340s\n",
      "Epoch: 6 cost time: 18.868663549423218\n",
      "Epoch: 6, Steps: 206 | Train Loss: 1.2498357 Vali Loss: 1.1851420 Test Loss: 1.3618010\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2948036\n",
      "\tspeed: 0.3549s/iter; left time: 3181.8801s\n",
      "\titers: 200, epoch: 7 | loss: 1.2098174\n",
      "\tspeed: 0.0576s/iter; left time: 510.3006s\n",
      "Epoch: 7 cost time: 21.274640321731567\n",
      "Epoch: 7, Steps: 206 | Train Loss: 1.2498308 Vali Loss: 1.1822112 Test Loss: 1.3617988\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.0569556\n",
      "\tspeed: 0.3521s/iter; left time: 3084.4427s\n",
      "\titers: 200, epoch: 8 | loss: 1.2607803\n",
      "\tspeed: 0.0566s/iter; left time: 490.4640s\n",
      "Epoch: 8 cost time: 20.219080448150635\n",
      "Epoch: 8, Steps: 206 | Train Loss: 1.2498251 Vali Loss: 1.1798667 Test Loss: 1.3617970\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2098236\n",
      "\tspeed: 0.3524s/iter; left time: 3013.9247s\n",
      "\titers: 200, epoch: 9 | loss: 1.1589212\n",
      "\tspeed: 0.0571s/iter; left time: 483.0423s\n",
      "Epoch: 9 cost time: 20.06808853149414\n",
      "Epoch: 9, Steps: 206 | Train Loss: 1.2498246 Vali Loss: 1.1804514 Test Loss: 1.3617961\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.1758591\n",
      "\tspeed: 0.3574s/iter; left time: 2982.8685s\n",
      "\titers: 200, epoch: 10 | loss: 1.1928456\n",
      "\tspeed: 0.0595s/iter; left time: 490.5486s\n",
      "Epoch: 10 cost time: 20.616902351379395\n",
      "Epoch: 10, Steps: 206 | Train Loss: 1.2498229 Vali Loss: 1.1827950 Test Loss: 1.3617965\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.2098325\n",
      "\tspeed: 0.3591s/iter; left time: 2923.6151s\n",
      "\titers: 200, epoch: 11 | loss: 1.2268189\n",
      "\tspeed: 0.0564s/iter; left time: 453.4076s\n",
      "Epoch: 11 cost time: 20.645472764968872\n",
      "Epoch: 11, Steps: 206 | Train Loss: 1.2498241 Vali Loss: 1.1810366 Test Loss: 1.3617957\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.2607679\n",
      "\tspeed: 0.3678s/iter; left time: 2918.7778s\n",
      "\titers: 200, epoch: 12 | loss: 1.2098236\n",
      "\tspeed: 0.0569s/iter; left time: 445.5514s\n",
      "Epoch: 12 cost time: 21.225921392440796\n",
      "Epoch: 12, Steps: 206 | Train Loss: 1.2498242 Vali Loss: 1.1839658 Test Loss: 1.3617958\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.1928461\n",
      "\tspeed: 0.4164s/iter; left time: 3218.6216s\n",
      "\titers: 200, epoch: 13 | loss: 1.3627092\n",
      "\tspeed: 0.0755s/iter; left time: 575.8566s\n",
      "Epoch: 13 cost time: 24.936675310134888\n",
      "Epoch: 13, Steps: 206 | Train Loss: 1.2498254 Vali Loss: 1.1816220 Test Loss: 1.3617952\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "accuracy, f1: 0.696 & 0.734 \n",
      "488, 488\n",
      "######################  corn_data_c.csv_Autoformer_28_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1634\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 1.2439982\n",
      "\tspeed: 0.1837s/iter; left time: 1855.5919s\n",
      "\titers: 200, epoch: 1 | loss: 1.3034083\n",
      "\tspeed: 0.0870s/iter; left time: 869.6584s\n",
      "Epoch: 1 cost time: 28.77053737640381\n",
      "Epoch: 1, Steps: 204 | Train Loss: 1.2523907 Vali Loss: 1.1860236 Test Loss: 1.3604215\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2440001\n",
      "\tspeed: 0.4707s/iter; left time: 4658.0327s\n",
      "\titers: 200, epoch: 2 | loss: 1.1335530\n",
      "\tspeed: 0.0712s/iter; left time: 697.1759s\n",
      "Epoch: 2 cost time: 25.57905340194702\n",
      "Epoch: 2, Steps: 204 | Train Loss: 1.2494876 Vali Loss: 1.1862009 Test Loss: 1.3602986\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.0824383\n",
      "\tspeed: 0.4288s/iter; left time: 4156.5426s\n",
      "\titers: 200, epoch: 3 | loss: 1.1673596\n",
      "\tspeed: 0.0599s/iter; left time: 574.8442s\n",
      "Epoch: 3 cost time: 21.180409908294678\n",
      "Epoch: 3, Steps: 204 | Train Loss: 1.2496191 Vali Loss: 1.1870089 Test Loss: 1.3602052\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2438039\n",
      "\tspeed: 0.3407s/iter; left time: 3232.6040s\n",
      "\titers: 200, epoch: 4 | loss: 1.2183049\n",
      "\tspeed: 0.0648s/iter; left time: 608.0601s\n",
      "Epoch: 4 cost time: 20.786274433135986\n",
      "Epoch: 4, Steps: 204 | Train Loss: 1.2495785 Vali Loss: 1.1860914 Test Loss: 1.3601966\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2522548\n",
      "\tspeed: 0.3903s/iter; left time: 3624.1044s\n",
      "\titers: 200, epoch: 5 | loss: 1.2012829\n",
      "\tspeed: 0.0773s/iter; left time: 710.4492s\n",
      "Epoch: 5 cost time: 22.123589992523193\n",
      "Epoch: 5, Steps: 204 | Train Loss: 1.2494396 Vali Loss: 1.1854715 Test Loss: 1.3601846\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.3032211\n",
      "\tspeed: 0.3526s/iter; left time: 3201.7187s\n",
      "\titers: 200, epoch: 6 | loss: 1.3032167\n",
      "\tspeed: 0.0600s/iter; left time: 539.2196s\n",
      "Epoch: 6 cost time: 19.89274311065674\n",
      "Epoch: 6, Steps: 204 | Train Loss: 1.2493909 Vali Loss: 1.1851677 Test Loss: 1.3601835\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2437558\n",
      "\tspeed: 0.3754s/iter; left time: 3332.0501s\n",
      "\titers: 200, epoch: 7 | loss: 1.2352628\n",
      "\tspeed: 0.0779s/iter; left time: 683.8114s\n",
      "Epoch: 7 cost time: 23.453133583068848\n",
      "Epoch: 7, Steps: 204 | Train Loss: 1.2493871 Vali Loss: 1.1860759 Test Loss: 1.3601819\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2097863\n",
      "\tspeed: 0.3926s/iter; left time: 3405.0710s\n",
      "\titers: 200, epoch: 8 | loss: 1.2522578\n",
      "\tspeed: 0.0593s/iter; left time: 508.1784s\n",
      "Epoch: 8 cost time: 20.226044178009033\n",
      "Epoch: 8, Steps: 204 | Train Loss: 1.2494268 Vali Loss: 1.1854681 Test Loss: 1.3601804\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2352750\n",
      "\tspeed: 0.3413s/iter; left time: 2890.7829s\n",
      "\titers: 200, epoch: 9 | loss: 1.1843137\n",
      "\tspeed: 0.0594s/iter; left time: 497.2458s\n",
      "Epoch: 9 cost time: 20.208975315093994\n",
      "Epoch: 9, Steps: 204 | Train Loss: 1.2494259 Vali Loss: 1.1851643 Test Loss: 1.3601803\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2182814\n",
      "\tspeed: 0.3375s/iter; left time: 2789.3802s\n",
      "\titers: 200, epoch: 10 | loss: 1.2607630\n",
      "\tspeed: 0.0589s/iter; left time: 481.3129s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 cost time: 19.958136796951294\n",
      "Epoch: 10, Steps: 204 | Train Loss: 1.2492582 Vali Loss: 1.1869843 Test Loss: 1.3601803\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.2182776\n",
      "\tspeed: 0.3714s/iter; left time: 2993.8253s\n",
      "\titers: 200, epoch: 11 | loss: 1.1418313\n",
      "\tspeed: 0.0816s/iter; left time: 649.5441s\n",
      "Epoch: 11 cost time: 25.833982467651367\n",
      "Epoch: 11, Steps: 204 | Train Loss: 1.2494248 Vali Loss: 1.1857711 Test Loss: 1.3601801\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.1928020\n",
      "\tspeed: 0.4386s/iter; left time: 3445.8054s\n",
      "\titers: 200, epoch: 12 | loss: 1.1758101\n",
      "\tspeed: 0.0758s/iter; left time: 588.0045s\n",
      "Epoch: 12 cost time: 25.567073345184326\n",
      "Epoch: 12, Steps: 204 | Train Loss: 1.2492584 Vali Loss: 1.1869843 Test Loss: 1.3601800\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.2097988\n",
      "\tspeed: 0.4365s/iter; left time: 3340.8675s\n",
      "\titers: 200, epoch: 13 | loss: 1.2522568\n",
      "\tspeed: 0.0766s/iter; left time: 578.4705s\n",
      "Epoch: 13 cost time: 25.7575466632843\n",
      "Epoch: 13, Steps: 204 | Train Loss: 1.2494241 Vali Loss: 1.1857711 Test Loss: 1.3601804\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 1.3201983\n",
      "\tspeed: 0.4191s/iter; left time: 3121.5537s\n",
      "\titers: 200, epoch: 14 | loss: 1.2862438\n",
      "\tspeed: 0.0703s/iter; left time: 516.7054s\n",
      "Epoch: 14 cost time: 24.053663969039917\n",
      "Epoch: 14, Steps: 204 | Train Loss: 1.2496317 Vali Loss: 1.1854675 Test Loss: 1.3601800\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "accuracy, f1: 0.702 & 0.737 \n",
      "472, 472\n",
      "######################  corn_data_c.csv_Autoformer_42_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1620\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 1.1590991\n",
      "\tspeed: 0.1864s/iter; left time: 1863.7195s\n",
      "\titers: 200, epoch: 1 | loss: 1.3118560\n",
      "\tspeed: 0.0782s/iter; left time: 774.7358s\n",
      "Epoch: 1 cost time: 27.71736764907837\n",
      "Epoch: 1, Steps: 202 | Train Loss: 1.2516251 Vali Loss: 1.1795745 Test Loss: 1.3607454\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2326038\n",
      "\tspeed: 0.4341s/iter; left time: 4253.6063s\n",
      "\titers: 200, epoch: 2 | loss: 1.2098752\n",
      "\tspeed: 0.0750s/iter; left time: 727.2942s\n",
      "Epoch: 2 cost time: 25.309521436691284\n",
      "Epoch: 2, Steps: 202 | Train Loss: 1.2495555 Vali Loss: 1.1788777 Test Loss: 1.3607060\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2607698\n",
      "\tspeed: 0.4133s/iter; left time: 3966.3976s\n",
      "\titers: 200, epoch: 3 | loss: 1.2551033\n",
      "\tspeed: 0.0759s/iter; left time: 721.2566s\n",
      "Epoch: 3 cost time: 25.110411167144775\n",
      "Epoch: 3, Steps: 202 | Train Loss: 1.2492327 Vali Loss: 1.1781734 Test Loss: 1.3606565\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2551036\n",
      "\tspeed: 0.4182s/iter; left time: 3929.3227s\n",
      "\titers: 200, epoch: 4 | loss: 1.2097969\n",
      "\tspeed: 0.0719s/iter; left time: 668.7495s\n",
      "Epoch: 4 cost time: 24.72977304458618\n",
      "Epoch: 4, Steps: 202 | Train Loss: 1.2492920 Vali Loss: 1.1809932 Test Loss: 1.3606460\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2664199\n",
      "\tspeed: 0.4118s/iter; left time: 3785.8911s\n",
      "\titers: 200, epoch: 5 | loss: 1.2381051\n",
      "\tspeed: 0.0721s/iter; left time: 655.7073s\n",
      "Epoch: 5 cost time: 24.074316024780273\n",
      "Epoch: 5, Steps: 202 | Train Loss: 1.2492850 Vali Loss: 1.1818625 Test Loss: 1.3606439\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2041188\n",
      "\tspeed: 0.4041s/iter; left time: 3632.9473s\n",
      "\titers: 200, epoch: 6 | loss: 1.3343558\n",
      "\tspeed: 0.0759s/iter; left time: 675.0202s\n",
      "Epoch: 6 cost time: 24.284393072128296\n",
      "Epoch: 6, Steps: 202 | Train Loss: 1.2494763 Vali Loss: 1.1772870 Test Loss: 1.3606427\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2380893\n",
      "\tspeed: 0.4093s/iter; left time: 3597.2227s\n",
      "\titers: 200, epoch: 7 | loss: 1.2720683\n",
      "\tspeed: 0.0766s/iter; left time: 665.4729s\n",
      "Epoch: 7 cost time: 24.492052793502808\n",
      "Epoch: 7, Steps: 202 | Train Loss: 1.2495870 Vali Loss: 1.1792463 Test Loss: 1.3606417\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2437527\n",
      "\tspeed: 0.4265s/iter; left time: 3662.5875s\n",
      "\titers: 200, epoch: 8 | loss: 1.3230274\n",
      "\tspeed: 0.0725s/iter; left time: 615.1983s\n",
      "Epoch: 8 cost time: 23.99146556854248\n",
      "Epoch: 8, Steps: 202 | Train Loss: 1.2495573 Vali Loss: 1.1801163 Test Loss: 1.3606405\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2494143\n",
      "\tspeed: 0.4049s/iter; left time: 3394.6962s\n",
      "\titers: 200, epoch: 9 | loss: 1.3003719\n",
      "\tspeed: 0.0734s/iter; left time: 607.8900s\n",
      "Epoch: 9 cost time: 24.092550039291382\n",
      "Epoch: 9, Steps: 202 | Train Loss: 1.2495845 Vali Loss: 1.1772851 Test Loss: 1.3606405\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.1701379\n",
      "\tspeed: 0.4341s/iter; left time: 3552.6304s\n",
      "\titers: 200, epoch: 10 | loss: 1.2097844\n",
      "\tspeed: 0.0746s/iter; left time: 603.3072s\n",
      "Epoch: 10 cost time: 25.1554274559021\n",
      "Epoch: 10, Steps: 202 | Train Loss: 1.2495565 Vali Loss: 1.1816404 Test Loss: 1.3606400\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.3570040\n",
      "\tspeed: 0.3730s/iter; left time: 2976.8576s\n",
      "\titers: 200, epoch: 11 | loss: 1.2720630\n",
      "\tspeed: 0.0638s/iter; left time: 503.1652s\n",
      "Epoch: 11 cost time: 20.80180859565735\n",
      "Epoch: 11, Steps: 202 | Train Loss: 1.2493600 Vali Loss: 1.1796801 Test Loss: 1.3606400\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.1984493\n",
      "\tspeed: 0.3509s/iter; left time: 2729.9175s\n",
      "\titers: 200, epoch: 12 | loss: 1.2494285\n",
      "\tspeed: 0.0633s/iter; left time: 486.4330s\n",
      "Epoch: 12 cost time: 20.660948514938354\n",
      "Epoch: 12, Steps: 202 | Train Loss: 1.2492191 Vali Loss: 1.1779377 Test Loss: 1.3606400\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.3060406\n",
      "\tspeed: 0.3554s/iter; left time: 2692.6964s\n",
      "\titers: 200, epoch: 13 | loss: 1.2324233\n",
      "\tspeed: 0.0645s/iter; left time: 482.0786s\n",
      "Epoch: 13 cost time: 21.339387893676758\n",
      "Epoch: 13, Steps: 202 | Train Loss: 1.2495567 Vali Loss: 1.1775020 Test Loss: 1.3606397\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 1.3286966\n",
      "\tspeed: 0.3407s/iter; left time: 2512.2938s\n",
      "\titers: 200, epoch: 14 | loss: 1.2777349\n",
      "\tspeed: 0.0620s/iter; left time: 450.9080s\n",
      "Epoch: 14 cost time: 20.48677968978882\n",
      "Epoch: 14, Steps: 202 | Train Loss: 1.2493591 Vali Loss: 1.1775022 Test Loss: 1.3606397\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "accuracy, f1: 0.710 & 0.740 \n",
      "456, 456\n",
      "######################  corn_data_c.csv_Autoformer_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1606\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 1.2057850\n",
      "\tspeed: 0.1319s/iter; left time: 1306.0238s\n",
      "\titers: 200, epoch: 1 | loss: 1.3245625\n",
      "\tspeed: 0.0677s/iter; left time: 663.4704s\n",
      "Epoch: 1 cost time: 20.9675395488739\n",
      "Epoch: 1, Steps: 200 | Train Loss: 1.2524164 Vali Loss: 1.1732455 Test Loss: 1.3550421\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2608784\n",
      "\tspeed: 0.3644s/iter; left time: 3534.7678s\n",
      "\titers: 200, epoch: 2 | loss: 1.1843966\n",
      "\tspeed: 0.0786s/iter; left time: 754.3397s\n",
      "Epoch: 2 cost time: 23.31570315361023\n",
      "Epoch: 2, Steps: 200 | Train Loss: 1.2504527 Vali Loss: 1.1698618 Test Loss: 1.3550117\n",
      "Updating learning rate to 0.00025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\titers: 100, epoch: 3 | loss: 1.1418583\n",
      "\tspeed: 0.4291s/iter; left time: 4077.1205s\n",
      "\titers: 200, epoch: 3 | loss: 1.2182988\n",
      "\tspeed: 0.0685s/iter; left time: 644.2877s\n",
      "Epoch: 3 cost time: 21.920875549316406\n",
      "Epoch: 3, Steps: 200 | Train Loss: 1.2507080 Vali Loss: 1.1701096 Test Loss: 1.3549172\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2692412\n",
      "\tspeed: 0.3747s/iter; left time: 3484.7841s\n",
      "\titers: 200, epoch: 4 | loss: 1.2564930\n",
      "\tspeed: 0.0711s/iter; left time: 654.4168s\n",
      "Epoch: 4 cost time: 24.17754030227661\n",
      "Epoch: 4, Steps: 200 | Train Loss: 1.2505799 Vali Loss: 1.1708080 Test Loss: 1.3549076\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2479906\n",
      "\tspeed: 0.3930s/iter; left time: 3576.8410s\n",
      "\titers: 200, epoch: 5 | loss: 1.2692292\n",
      "\tspeed: 0.0746s/iter; left time: 671.4797s\n",
      "Epoch: 5 cost time: 23.653388738632202\n",
      "Epoch: 5, Steps: 200 | Train Loss: 1.2506358 Vali Loss: 1.1708045 Test Loss: 1.3549036\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2182629\n",
      "\tspeed: 0.3941s/iter; left time: 3507.8463s\n",
      "\titers: 200, epoch: 6 | loss: 1.3032045\n",
      "\tspeed: 0.0709s/iter; left time: 624.1053s\n",
      "Epoch: 6 cost time: 23.24052858352661\n",
      "Epoch: 6, Steps: 200 | Train Loss: 1.2506517 Vali Loss: 1.1731046 Test Loss: 1.3549035\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2225078\n",
      "\tspeed: 0.4023s/iter; left time: 3500.0355s\n",
      "\titers: 200, epoch: 7 | loss: 1.2012753\n",
      "\tspeed: 0.0710s/iter; left time: 610.9317s\n",
      "Epoch: 7 cost time: 23.452418088912964\n",
      "Epoch: 7, Steps: 200 | Train Loss: 1.2507989 Vali Loss: 1.1715112 Test Loss: 1.3549032\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.699 & 0.728 \n",
      "440, 440\n",
      "######################  coffee_data_c.csv_Autoformer_14_14  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 1.2533730\n",
      "\tspeed: 0.1426s/iter; left time: 1454.5117s\n",
      "\titers: 200, epoch: 1 | loss: 1.2896384\n",
      "\tspeed: 0.0608s/iter; left time: 613.9554s\n",
      "Epoch: 1 cost time: 21.801740169525146\n",
      "Epoch: 1, Steps: 206 | Train Loss: 1.2573936 Vali Loss: 1.2586156 Test Loss: 1.2748779\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.3070698\n",
      "\tspeed: 0.4088s/iter; left time: 4086.1993s\n",
      "\titers: 200, epoch: 2 | loss: 1.3070872\n",
      "\tspeed: 0.0660s/iter; left time: 653.4963s\n",
      "Epoch: 2 cost time: 24.146167278289795\n",
      "Epoch: 2, Steps: 206 | Train Loss: 1.2537582 Vali Loss: 1.2574925 Test Loss: 1.2743832\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.1991376\n",
      "\tspeed: 0.4081s/iter; left time: 3994.6686s\n",
      "\titers: 200, epoch: 3 | loss: 1.2529877\n",
      "\tspeed: 0.0605s/iter; left time: 586.1723s\n",
      "Epoch: 3 cost time: 22.517277479171753\n",
      "Epoch: 3, Steps: 206 | Train Loss: 1.2536505 Vali Loss: 1.2553785 Test Loss: 1.2740989\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2529826\n",
      "\tspeed: 0.4164s/iter; left time: 3990.4278s\n",
      "\titers: 200, epoch: 4 | loss: 1.1452078\n",
      "\tspeed: 0.0660s/iter; left time: 625.6597s\n",
      "Epoch: 4 cost time: 24.6467227935791\n",
      "Epoch: 4, Steps: 206 | Train Loss: 1.2529832 Vali Loss: 1.2615718 Test Loss: 1.2740983\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2708950\n",
      "\tspeed: 0.3894s/iter; left time: 3651.6221s\n",
      "\titers: 200, epoch: 5 | loss: 1.1811576\n",
      "\tspeed: 0.0685s/iter; left time: 635.6502s\n",
      "Epoch: 5 cost time: 23.824846267700195\n",
      "Epoch: 5, Steps: 206 | Train Loss: 1.2534904 Vali Loss: 1.2559799 Test Loss: 1.2740791\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.1811227\n",
      "\tspeed: 0.4094s/iter; left time: 3754.7184s\n",
      "\titers: 200, epoch: 6 | loss: 1.3068188\n",
      "\tspeed: 0.0688s/iter; left time: 624.1720s\n",
      "Epoch: 6 cost time: 23.614455223083496\n",
      "Epoch: 6, Steps: 206 | Train Loss: 1.2535669 Vali Loss: 1.2652586 Test Loss: 1.2740688\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.3068116\n",
      "\tspeed: 0.4047s/iter; left time: 3628.0125s\n",
      "\titers: 200, epoch: 7 | loss: 1.2350060\n",
      "\tspeed: 0.0689s/iter; left time: 610.6297s\n",
      "Epoch: 7 cost time: 23.735065698623657\n",
      "Epoch: 7, Steps: 206 | Train Loss: 1.2532095 Vali Loss: 1.2596833 Test Loss: 1.2740675\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2170203\n",
      "\tspeed: 0.4063s/iter; left time: 3558.5428s\n",
      "\titers: 200, epoch: 8 | loss: 1.2350031\n",
      "\tspeed: 0.0584s/iter; left time: 505.4818s\n",
      "Epoch: 8 cost time: 21.345926523208618\n",
      "Epoch: 8, Steps: 206 | Train Loss: 1.2535557 Vali Loss: 1.2528708 Test Loss: 1.2740667\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2708721\n",
      "\tspeed: 0.3759s/iter; left time: 3215.0474s\n",
      "\titers: 200, epoch: 9 | loss: 1.1272205\n",
      "\tspeed: 0.0653s/iter; left time: 552.2256s\n",
      "Epoch: 9 cost time: 22.34718346595764\n",
      "Epoch: 9, Steps: 206 | Train Loss: 1.2534655 Vali Loss: 1.2596812 Test Loss: 1.2740655\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.3068101\n",
      "\tspeed: 0.4332s/iter; left time: 3616.2985s\n",
      "\titers: 200, epoch: 10 | loss: 1.2349678\n",
      "\tspeed: 0.0614s/iter; left time: 506.1768s\n",
      "Epoch: 10 cost time: 26.78035616874695\n",
      "Epoch: 10, Steps: 206 | Train Loss: 1.2534666 Vali Loss: 1.2584434 Test Loss: 1.2740657\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.2888653\n",
      "\tspeed: 0.4195s/iter; left time: 3415.1804s\n",
      "\titers: 200, epoch: 11 | loss: 1.4145604\n",
      "\tspeed: 0.0622s/iter; left time: 500.4507s\n",
      "Epoch: 11 cost time: 22.224408388137817\n",
      "Epoch: 11, Steps: 206 | Train Loss: 1.2532028 Vali Loss: 1.2603000 Test Loss: 1.2740650\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.3786314\n",
      "\tspeed: 0.3331s/iter; left time: 2642.8380s\n",
      "\titers: 200, epoch: 12 | loss: 1.2708762\n",
      "\tspeed: 0.0531s/iter; left time: 416.4053s\n",
      "Epoch: 12 cost time: 18.698498249053955\n",
      "Epoch: 12, Steps: 206 | Train Loss: 1.2532896 Vali Loss: 1.2609193 Test Loss: 1.2740648\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.4145380\n",
      "\tspeed: 0.3252s/iter; left time: 2513.8065s\n",
      "\titers: 200, epoch: 13 | loss: 1.3247648\n",
      "\tspeed: 0.0533s/iter; left time: 406.2812s\n",
      "Epoch: 13 cost time: 18.435195207595825\n",
      "Epoch: 13, Steps: 206 | Train Loss: 1.2533760 Vali Loss: 1.2596807 Test Loss: 1.2740647\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftS_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "accuracy, f1: 0.768 & 0.809 \n",
      "488, 488\n",
      "######################  coffee_data_c.csv_Autoformer_28_28  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1636\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 1.2351646\n",
      "\tspeed: 0.1231s/iter; left time: 1243.1196s\n",
      "\titers: 200, epoch: 1 | loss: 1.1993132\n",
      "\tspeed: 0.0575s/iter; left time: 575.0262s\n",
      "Epoch: 1 cost time: 19.06452751159668\n",
      "Epoch: 1, Steps: 204 | Train Loss: 1.2534171 Vali Loss: 1.2520635 Test Loss: 1.2723722\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2709789\n",
      "\tspeed: 0.3316s/iter; left time: 3282.2952s\n",
      "\titers: 200, epoch: 2 | loss: 1.1454465\n",
      "\tspeed: 0.0588s/iter; left time: 576.2430s\n",
      "Epoch: 2 cost time: 20.35444402694702\n",
      "Epoch: 2, Steps: 204 | Train Loss: 1.2522801 Vali Loss: 1.2520477 Test Loss: 1.2723354\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2529671\n",
      "\tspeed: 0.3486s/iter; left time: 3378.8097s\n",
      "\titers: 200, epoch: 3 | loss: 1.2529290\n",
      "\tspeed: 0.0583s/iter; left time: 559.4260s\n",
      "Epoch: 3 cost time: 19.909533739089966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Steps: 204 | Train Loss: 1.2518308 Vali Loss: 1.2538821 Test Loss: 1.2722517\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.3247528\n",
      "\tspeed: 0.3426s/iter; left time: 3251.3951s\n",
      "\titers: 200, epoch: 4 | loss: 1.1631510\n",
      "\tspeed: 0.0573s/iter; left time: 538.2481s\n",
      "Epoch: 4 cost time: 20.41754698753357\n",
      "Epoch: 4, Steps: 204 | Train Loss: 1.2516593 Vali Loss: 1.2541685 Test Loss: 1.2722149\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.1900766\n",
      "\tspeed: 0.3132s/iter; left time: 2907.8345s\n",
      "\titers: 200, epoch: 5 | loss: 1.2349852\n",
      "\tspeed: 0.0601s/iter; left time: 552.3234s\n",
      "Epoch: 5 cost time: 19.51910376548767\n",
      "Epoch: 5, Steps: 204 | Train Loss: 1.2521745 Vali Loss: 1.2515863 Test Loss: 1.2721981\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2439302\n",
      "\tspeed: 0.3323s/iter; left time: 3017.8695s\n",
      "\titers: 200, epoch: 6 | loss: 1.2708809\n",
      "\tspeed: 0.0594s/iter; left time: 533.5687s\n",
      "Epoch: 6 cost time: 19.97522234916687\n",
      "Epoch: 6, Steps: 204 | Train Loss: 1.2518602 Vali Loss: 1.2522268 Test Loss: 1.2721967\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.3067921\n",
      "\tspeed: 0.3308s/iter; left time: 2936.6201s\n",
      "\titers: 200, epoch: 7 | loss: 1.2978144\n",
      "\tspeed: 0.0602s/iter; left time: 528.3357s\n",
      "Epoch: 7 cost time: 19.371347427368164\n",
      "Epoch: 7, Steps: 204 | Train Loss: 1.2516822 Vali Loss: 1.2525451 Test Loss: 1.2721941\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.1900482\n",
      "\tspeed: 0.3612s/iter; left time: 3132.9956s\n",
      "\titers: 200, epoch: 8 | loss: 1.2618980\n",
      "\tspeed: 0.0688s/iter; left time: 589.7318s\n",
      "Epoch: 8 cost time: 21.51217746734619\n",
      "Epoch: 8, Steps: 204 | Train Loss: 1.2515464 Vali Loss: 1.2531862 Test Loss: 1.2721940\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.1990387\n",
      "\tspeed: 0.3483s/iter; left time: 2949.5544s\n",
      "\titers: 200, epoch: 9 | loss: 1.2708639\n",
      "\tspeed: 0.0618s/iter; left time: 516.9233s\n",
      "Epoch: 9 cost time: 22.12159276008606\n",
      "Epoch: 9, Steps: 204 | Train Loss: 1.2516334 Vali Loss: 1.2506198 Test Loss: 1.2721928\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.2080090\n",
      "\tspeed: 0.3620s/iter; left time: 2992.1289s\n",
      "\titers: 200, epoch: 10 | loss: 1.2259604\n",
      "\tspeed: 0.0614s/iter; left time: 501.5006s\n",
      "Epoch: 10 cost time: 21.50375533103943\n",
      "Epoch: 10, Steps: 204 | Train Loss: 1.2517209 Vali Loss: 1.2538261 Test Loss: 1.2721924\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.1631100\n",
      "\tspeed: 0.4048s/iter; left time: 3262.7781s\n",
      "\titers: 200, epoch: 11 | loss: 1.3337302\n",
      "\tspeed: 0.0760s/iter; left time: 604.8381s\n",
      "Epoch: 11 cost time: 25.55975651741028\n",
      "Epoch: 11, Steps: 204 | Train Loss: 1.2516332 Vali Loss: 1.2519020 Test Loss: 1.2721922\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.2349522\n",
      "\tspeed: 0.4833s/iter; left time: 3797.5521s\n",
      "\titers: 200, epoch: 12 | loss: 1.3517067\n",
      "\tspeed: 0.0885s/iter; left time: 686.8352s\n",
      "Epoch: 12 cost time: 29.158740758895874\n",
      "Epoch: 12, Steps: 204 | Train Loss: 1.2516337 Vali Loss: 1.2515813 Test Loss: 1.2721922\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.2080008\n",
      "\tspeed: 0.4052s/iter; left time: 3100.9491s\n",
      "\titers: 200, epoch: 13 | loss: 1.2349441\n",
      "\tspeed: 0.0617s/iter; left time: 466.0584s\n",
      "Epoch: 13 cost time: 20.82951331138611\n",
      "Epoch: 13, Steps: 204 | Train Loss: 1.2515008 Vali Loss: 1.2515814 Test Loss: 1.2721922\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 1.3067794\n",
      "\tspeed: 0.3840s/iter; left time: 2860.5444s\n",
      "\titers: 200, epoch: 14 | loss: 1.2169857\n",
      "\tspeed: 0.0721s/iter; left time: 529.8908s\n",
      "Epoch: 14 cost time: 25.155259370803833\n",
      "Epoch: 14, Steps: 204 | Train Loss: 1.2510166 Vali Loss: 1.2512606 Test Loss: 1.2721922\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftS_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "accuracy, f1: 0.738 & 0.800 \n",
      "472, 472\n",
      "######################  coffee_data_c.csv_Autoformer_42_42  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1622\n",
      "val 211\n",
      "test 461\n",
      "\titers: 100, epoch: 1 | loss: 1.2531103\n",
      "\tspeed: 0.1427s/iter; left time: 1426.9421s\n",
      "\titers: 200, epoch: 1 | loss: 1.2051044\n",
      "\tspeed: 0.0660s/iter; left time: 653.0210s\n",
      "Epoch: 1 cost time: 21.841360569000244\n",
      "Epoch: 1, Steps: 202 | Train Loss: 1.2510725 Vali Loss: 1.2464975 Test Loss: 1.2740511\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.2111136\n",
      "\tspeed: 0.3715s/iter; left time: 3639.9509s\n",
      "\titers: 200, epoch: 2 | loss: 1.1452465\n",
      "\tspeed: 0.0715s/iter; left time: 693.1769s\n",
      "Epoch: 2 cost time: 24.616490125656128\n",
      "Epoch: 2, Steps: 202 | Train Loss: 1.2491838 Vali Loss: 1.2441967 Test Loss: 1.2740541\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.3606780\n",
      "\tspeed: 0.3673s/iter; left time: 3524.8142s\n",
      "\titers: 200, epoch: 3 | loss: 1.3127916\n",
      "\tspeed: 0.0626s/iter; left time: 594.3058s\n",
      "Epoch: 3 cost time: 20.6877281665802\n",
      "Epoch: 3, Steps: 202 | Train Loss: 1.2489712 Vali Loss: 1.2443649 Test Loss: 1.2739936\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.2110081\n",
      "\tspeed: 0.3294s/iter; left time: 3094.5978s\n",
      "\titers: 200, epoch: 4 | loss: 1.3008156\n",
      "\tspeed: 0.0620s/iter; left time: 576.1911s\n",
      "Epoch: 4 cost time: 20.049814224243164\n",
      "Epoch: 4, Steps: 202 | Train Loss: 1.2492424 Vali Loss: 1.2455069 Test Loss: 1.2739848\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2888314\n",
      "\tspeed: 0.3275s/iter; left time: 3010.7353s\n",
      "\titers: 200, epoch: 5 | loss: 1.2349445\n",
      "\tspeed: 0.0624s/iter; left time: 567.1171s\n",
      "Epoch: 5 cost time: 20.267255306243896\n",
      "Epoch: 5, Steps: 202 | Train Loss: 1.2488179 Vali Loss: 1.2459581 Test Loss: 1.2739745\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2409285\n",
      "\tspeed: 0.3263s/iter; left time: 2933.6297s\n",
      "\titers: 200, epoch: 6 | loss: 1.2349486\n",
      "\tspeed: 0.0619s/iter; left time: 550.5092s\n",
      "Epoch: 6 cost time: 20.415356874465942\n",
      "Epoch: 6, Steps: 202 | Train Loss: 1.2493765 Vali Loss: 1.2457280 Test Loss: 1.2739749\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 1.2169805\n",
      "\tspeed: 0.3255s/iter; left time: 2861.1631s\n",
      "\titers: 200, epoch: 7 | loss: 1.2050072\n",
      "\tspeed: 0.0630s/iter; left time: 547.6209s\n",
      "Epoch: 7 cost time: 20.067973852157593\n",
      "Epoch: 7, Steps: 202 | Train Loss: 1.2491365 Vali Loss: 1.2441150 Test Loss: 1.2739733\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 1.2349331\n",
      "\tspeed: 0.3324s/iter; left time: 2853.9901s\n",
      "\titers: 200, epoch: 8 | loss: 1.2409214\n",
      "\tspeed: 0.0620s/iter; left time: 526.3076s\n",
      "Epoch: 8 cost time: 19.982157945632935\n",
      "Epoch: 8, Steps: 202 | Train Loss: 1.2491949 Vali Loss: 1.2436541 Test Loss: 1.2739727\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.2708548\n",
      "\tspeed: 0.3313s/iter; left time: 2778.1589s\n",
      "\titers: 200, epoch: 9 | loss: 1.1870574\n",
      "\tspeed: 0.0623s/iter; left time: 516.0998s\n",
      "Epoch: 9 cost time: 20.314671277999878\n",
      "Epoch: 9, Steps: 202 | Train Loss: 1.2491348 Vali Loss: 1.2464168 Test Loss: 1.2739727\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 1.3127503\n",
      "\tspeed: 0.3487s/iter; left time: 2853.2217s\n",
      "\titers: 200, epoch: 10 | loss: 1.2409205\n",
      "\tspeed: 0.0630s/iter; left time: 509.5859s\n",
      "Epoch: 10 cost time: 20.963019371032715\n",
      "Epoch: 10, Steps: 202 | Train Loss: 1.2491637 Vali Loss: 1.2448050 Test Loss: 1.2739725\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 1.2528931\n",
      "\tspeed: 0.3526s/iter; left time: 2814.1939s\n",
      "\titers: 200, epoch: 11 | loss: 1.2888166\n",
      "\tspeed: 0.0628s/iter; left time: 494.6634s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 cost time: 21.233067274093628\n",
      "Epoch: 11, Steps: 202 | Train Loss: 1.2493415 Vali Loss: 1.2452654 Test Loss: 1.2739725\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 1.1870648\n",
      "\tspeed: 0.3489s/iter; left time: 2714.1449s\n",
      "\titers: 200, epoch: 12 | loss: 1.2888125\n",
      "\tspeed: 0.0628s/iter; left time: 481.9692s\n",
      "Epoch: 12 cost time: 20.30795121192932\n",
      "Epoch: 12, Steps: 202 | Train Loss: 1.2493407 Vali Loss: 1.2464163 Test Loss: 1.2739725\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 1.1930286\n",
      "\tspeed: 0.3454s/iter; left time: 2617.2281s\n",
      "\titers: 200, epoch: 13 | loss: 1.1451429\n",
      "\tspeed: 0.0660s/iter; left time: 493.5053s\n",
      "Epoch: 13 cost time: 20.98323678970337\n",
      "Epoch: 13, Steps: 202 | Train Loss: 1.2491039 Vali Loss: 1.2459559 Test Loss: 1.2739725\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftS_sl96_ll42_pl42_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 461\n",
      "accuracy, f1: 0.738 & 0.787 \n",
      "456, 456\n",
      "######################  coffee_data_c.csv_Autoformer_56_56  ######################\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1608\n",
      "val 197\n",
      "test 447\n",
      "\titers: 100, epoch: 1 | loss: 1.2486271\n",
      "\tspeed: 0.1379s/iter; left time: 1372.4198s\n",
      "\titers: 200, epoch: 1 | loss: 1.2620671\n",
      "\tspeed: 0.0675s/iter; left time: 665.0159s\n",
      "Epoch: 1 cost time: 21.520387887954712\n",
      "Epoch: 1, Steps: 201 | Train Loss: 1.2491580 Vali Loss: 1.2347121 Test Loss: 1.2715514\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.1811390\n",
      "\tspeed: 0.3491s/iter; left time: 3404.0615s\n",
      "\titers: 200, epoch: 2 | loss: 1.1631913\n",
      "\tspeed: 0.0716s/iter; left time: 691.2342s\n",
      "Epoch: 2 cost time: 22.20294713973999\n",
      "Epoch: 2, Steps: 201 | Train Loss: 1.2475941 Vali Loss: 1.2353467 Test Loss: 1.2714672\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 1.2214992\n",
      "\tspeed: 0.3325s/iter; left time: 3174.9504s\n",
      "\titers: 200, epoch: 3 | loss: 1.2170132\n",
      "\tspeed: 0.0673s/iter; left time: 636.3855s\n",
      "Epoch: 3 cost time: 20.93079948425293\n",
      "Epoch: 3, Steps: 201 | Train Loss: 1.2474807 Vali Loss: 1.2373672 Test Loss: 1.2714274\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.1720963\n",
      "\tspeed: 0.3307s/iter; left time: 3091.0241s\n",
      "\titers: 200, epoch: 4 | loss: 1.3023050\n",
      "\tspeed: 0.0708s/iter; left time: 654.9444s\n",
      "Epoch: 4 cost time: 21.763680934906006\n",
      "Epoch: 4, Steps: 201 | Train Loss: 1.2474614 Vali Loss: 1.2386566 Test Loss: 1.2714036\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.2214743\n",
      "\tspeed: 0.3590s/iter; left time: 3284.0191s\n",
      "\titers: 200, epoch: 5 | loss: 1.2528995\n",
      "\tspeed: 0.0729s/iter; left time: 659.7979s\n",
      "Epoch: 5 cost time: 22.385125875473022\n",
      "Epoch: 5, Steps: 201 | Train Loss: 1.2474524 Vali Loss: 1.2386497 Test Loss: 1.2713969\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.2259549\n",
      "\tspeed: 0.3602s/iter; left time: 3222.4492s\n",
      "\titers: 200, epoch: 6 | loss: 1.3157566\n",
      "\tspeed: 0.0696s/iter; left time: 616.0872s\n",
      "Epoch: 6 cost time: 22.48705744743347\n",
      "Epoch: 6, Steps: 201 | Train Loss: 1.2474479 Vali Loss: 1.2382736 Test Loss: 1.2713946\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 447\n",
      "accuracy, f1: 0.723 & 0.771 \n",
      "440, 440\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "models = ['Autoformer', 'Informer', 'Transformer', 'DLinear', 'NLinear']\n",
    "data_paths = ['gold_data_c.csv', 'silver_data_c.csv', 'crude_oil_data_c.csv', 'natural_gas_data_c.csv', 'corn_data_c.csv', 'coffee_data_c.csv']\n",
    "label_lens = [14, 28, 42, 56]\n",
    "pred_lens = [14, 28, 42, 56]\n",
    "args = dotdict()\n",
    "\n",
    "for data_path in data_paths:\n",
    "    for pred_len in pred_lens:\n",
    "\n",
    "        # basic config\n",
    "        args.is_training = 1\n",
    "        args.model_id = 'classification'\n",
    "        args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "        # data loader\n",
    "        args.data = 'custom'\n",
    "        args.root_path = './dataset/commodity/'\n",
    "        args.data_path = data_path\n",
    "        args.features = 'S'\n",
    "        args.target = 'OT'\n",
    "        args.freq = 'd'\n",
    "        args.checkpoints = './checkpoints/'\n",
    "\n",
    "        # forecasting task\n",
    "        args.seq_len = 96\n",
    "        args.label_len = pred_len\n",
    "        args.pred_len = pred_len\n",
    "\n",
    "        # DLinear\n",
    "        args.individual = False\n",
    "\n",
    "        # Formers \n",
    "        args.embed_type = 0\n",
    "        args.enc_in = 1\n",
    "        args.dec_in = 1\n",
    "        args.c_out = 2\n",
    "        args.d_model = 512\n",
    "        args.n_heads = 8\n",
    "        args.e_layers = 2\n",
    "        args.d_layers = 1\n",
    "        args.d_ff = 2048\n",
    "        args.moving_avg = 25\n",
    "        args.factor = 3\n",
    "        args.distil = True\n",
    "        args.dropout = 0.05\n",
    "        args.embed = 'timeF'\n",
    "        args.do_predict = True\n",
    "\n",
    "        # optimization\n",
    "        args.num_workers = 10\n",
    "        args.itr = 1\n",
    "        args.train_epochs = 50\n",
    "        args.patience = 5\n",
    "        args.learning_rate = 0.0005\n",
    "        args.batch_size = 8\n",
    "        args.lradj = 'type1'\n",
    "        args.des = 'Exp'\n",
    "\n",
    "        # GPU\n",
    "        args.gpu = 0\n",
    "        args.devices = '0'\n",
    "        args.use_gpu = True\n",
    "        args.use_multi_gpu = False\n",
    "\n",
    "        # print('Args in experiment:')\n",
    "        # print(args)\n",
    "\n",
    "        Exp = Exp_Main\n",
    "        print(f'######################  {data_path}_{args.model}_{pred_len}_{pred_len}  ######################')\n",
    "        if args.is_training:\n",
    "            for ii in range(args.itr):\n",
    "                # setting record of experiments\n",
    "                setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                    args.data_path.split(\"_\")[0],\n",
    "                    args.model_id,\n",
    "                    args.model,\n",
    "                    args.data,\n",
    "                    args.features,\n",
    "                    args.seq_len,\n",
    "                    args.label_len,\n",
    "                    args.pred_len,\n",
    "                    args.d_model,\n",
    "                    args.n_heads,\n",
    "                    args.e_layers,\n",
    "                    args.d_layers,\n",
    "                    args.d_ff,\n",
    "                    args.factor,\n",
    "                    args.embed,\n",
    "                    args.distil,\n",
    "                    args.des, ii)\n",
    "\n",
    "                exp = Exp(args)  # set experiments\n",
    "                print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                exp.train(setting)\n",
    "\n",
    "                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.test(setting)\n",
    "\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "models = ['Autoformer', 'Informer', 'Transformer', 'DLinear', 'NLinear']\n",
    "data_paths = ['gold_data_c.csv', 'silver_data_c.csv', 'crude_oil_data_c.csv', 'natural_gas_data_c.csv', 'corn_data_c.csv', 'coffee_data_c.csv']\n",
    "label_lens = [14, 28, 42, 56]\n",
    "pred_lens = [14, 28, 42, 56]\n",
    "args = dotdict()\n",
    "\n",
    "for data_path in data_paths:\n",
    "    for pred_len in pred_lens:\n",
    "\n",
    "        # basic config\n",
    "        args.is_training = 1\n",
    "        args.model_id = 'classification'\n",
    "        args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "        # data loader\n",
    "        args.data = 'custom'\n",
    "        args.root_path = './dataset/commodity/'\n",
    "        args.data_path = data_path\n",
    "        args.features = 'M'\n",
    "        args.target = 'OT'\n",
    "        args.freq = 'd'\n",
    "        args.checkpoints = './checkpoints/'\n",
    "\n",
    "        # forecasting task\n",
    "        args.seq_len = 96\n",
    "        args.label_len = pred_len\n",
    "        args.pred_len = pred_len\n",
    "\n",
    "        # DLinear\n",
    "        args.individual = False\n",
    "\n",
    "        # Formers \n",
    "        args.embed_type = 0\n",
    "        args.enc_in = 6\n",
    "        args.dec_in = 6\n",
    "        args.c_out = 6\n",
    "        args.d_model = 512\n",
    "        args.n_heads = 8\n",
    "        args.e_layers = 2\n",
    "        args.d_layers = 1\n",
    "        args.d_ff = 2048\n",
    "        args.moving_avg = 25\n",
    "        args.factor = 3\n",
    "        args.distil = True\n",
    "        args.dropout = 0.05\n",
    "        args.embed = 'timeF'\n",
    "        args.do_predict = True\n",
    "\n",
    "        # optimization\n",
    "        args.num_workers = 10\n",
    "        args.itr = 1\n",
    "        args.train_epochs = 50\n",
    "        args.patience = 5\n",
    "        args.learning_rate = 0.0005\n",
    "        args.batch_size = 8\n",
    "        args.lradj = 'type1'\n",
    "        args.des = 'Exp'\n",
    "\n",
    "        # GPU\n",
    "        args.gpu = 0\n",
    "        args.devices = '0'\n",
    "        args.use_gpu = True\n",
    "        args.use_multi_gpu = False\n",
    "\n",
    "        # print('Args in experiment:')\n",
    "        # print(args)\n",
    "\n",
    "        Exp = Exp_Main\n",
    "        print(f'######################  {data_path}_{args.model}_{pred_len}_{pred_len}  ######################')\n",
    "        if args.is_training:\n",
    "            for ii in range(args.itr):\n",
    "                # setting record of experiments\n",
    "                setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "                    args.data_path.split(\"_\")[0],\n",
    "                    args.model_id,\n",
    "                    args.model,\n",
    "                    args.data,\n",
    "                    args.features,\n",
    "                    args.seq_len,\n",
    "                    args.label_len,\n",
    "                    args.pred_len,\n",
    "                    args.d_model,\n",
    "                    args.n_heads,\n",
    "                    args.e_layers,\n",
    "                    args.d_layers,\n",
    "                    args.d_ff,\n",
    "                    args.factor,\n",
    "                    args.embed,\n",
    "                    args.distil,\n",
    "                    args.des, ii)\n",
    "\n",
    "                exp = Exp(args)  # set experiments\n",
    "                print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "                exp.train(setting)\n",
    "\n",
    "                print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "                exp.test(setting)\n",
    "\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'gold_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 14, 'pred_len': 14, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='gold_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 14\n",
    "args.pred_len = 14\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.7150675\n",
      "\tspeed: 0.2773s/iter; left time: 2828.8428s\n",
      "\titers: 200, epoch: 1 | loss: 1.0009717\n",
      "\tspeed: 0.0509s/iter; left time: 513.9020s\n",
      "Epoch: 1 cost time: 33.884782552719116\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.7102846 Vali Loss: 5.8019609 Test Loss: 8.5066147\n",
      "Validation loss decreased (inf --> 5.801961).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6824962\n",
      "\tspeed: 0.4324s/iter; left time: 4321.6925s\n",
      "\titers: 200, epoch: 2 | loss: 0.5746744\n",
      "\tspeed: 0.0515s/iter; left time: 510.0212s\n",
      "Epoch: 2 cost time: 33.46830439567566\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.7020987 Vali Loss: 5.6443119 Test Loss: 8.4638128\n",
      "Validation loss decreased (5.801961 --> 5.644312).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7148576\n",
      "\tspeed: 0.4470s/iter; left time: 4375.4979s\n",
      "\titers: 200, epoch: 3 | loss: 0.6398650\n",
      "\tspeed: 0.0520s/iter; left time: 503.3437s\n",
      "Epoch: 3 cost time: 34.34434747695923\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.6855648 Vali Loss: 5.7093654 Test Loss: 8.4993229\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4827015\n",
      "\tspeed: 0.4521s/iter; left time: 4332.9349s\n",
      "\titers: 200, epoch: 4 | loss: 0.5259620\n",
      "\tspeed: 0.0522s/iter; left time: 495.2633s\n",
      "Epoch: 4 cost time: 34.52096629142761\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.6806992 Vali Loss: 5.7225533 Test Loss: 8.4656172\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5274728\n",
      "\tspeed: 0.4580s/iter; left time: 4294.6635s\n",
      "\titers: 200, epoch: 5 | loss: 0.9052879\n",
      "\tspeed: 0.0539s/iter; left time: 499.9872s\n",
      "Epoch: 5 cost time: 34.99981641769409\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.6768525 Vali Loss: 5.7834997 Test Loss: 8.4701185\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5503940\n",
      "\tspeed: 0.4675s/iter; left time: 4287.4217s\n",
      "\titers: 200, epoch: 6 | loss: 0.5321650\n",
      "\tspeed: 0.0527s/iter; left time: 478.0681s\n",
      "Epoch: 6 cost time: 35.38303065299988\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.6739516 Vali Loss: 5.6728654 Test Loss: 8.4779854\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6875799\n",
      "\tspeed: 0.4681s/iter; left time: 4196.7739s\n",
      "\titers: 200, epoch: 7 | loss: 0.4752099\n",
      "\tspeed: 0.0533s/iter; left time: 472.8388s\n",
      "Epoch: 7 cost time: 35.59603428840637\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.6720031 Vali Loss: 5.6672688 Test Loss: 8.4728003\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "( accuracy, f1: 0.714 & 0.714 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.291 & 0.183 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'silver_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 14, 'pred_len': 14, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 1, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='silver_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 14\n",
    "args.pred_len = 14\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 1\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1649\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.4747304\n",
      "\tspeed: 0.3702s/iter; left time: 39.6114s\n",
      "\titers: 200, epoch: 1 | loss: 0.5830474\n",
      "\tspeed: 0.0606s/iter; left time: 0.4242s\n",
      "Epoch: 1 cost time: 44.29926419258118\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.6441659 Vali Loss: 1.0920039 Test Loss: 1.9572040\n",
      "Validation loss decreased (inf --> 1.092004).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "( accuracy, f1: 0.696 & 0.726 \n",
      "488, 488\n",
      "40992, 40992\n",
      " accuracy, f1: 0.404 & 0.370 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'crude_oil_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 14, 'pred_len': 14, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='crude_oil_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 14\n",
    "args.pred_len = 14\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1651\n",
      "val 239\n",
      "test 490\n",
      "\titers: 100, epoch: 1 | loss: 0.7196439\n",
      "\tspeed: 0.2879s/iter; left time: 2937.3040s\n",
      "\titers: 200, epoch: 1 | loss: 0.7413711\n",
      "\tspeed: 0.0513s/iter; left time: 517.7594s\n",
      "Epoch: 1 cost time: 35.02819299697876\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.8208497 Vali Loss: 1.4612168 Test Loss: 1.4606243\n",
      "Validation loss decreased (inf --> 1.461217).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.8188974\n",
      "\tspeed: 0.4571s/iter; left time: 4568.6189s\n",
      "\titers: 200, epoch: 2 | loss: 0.8233802\n",
      "\tspeed: 0.0512s/iter; left time: 506.9894s\n",
      "Epoch: 2 cost time: 35.022679567337036\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.8001896 Vali Loss: 1.4590780 Test Loss: 1.3978621\n",
      "Validation loss decreased (1.461217 --> 1.459078).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.9032504\n",
      "\tspeed: 0.4581s/iter; left time: 4484.8215s\n",
      "\titers: 200, epoch: 3 | loss: 0.6599706\n",
      "\tspeed: 0.0513s/iter; left time: 496.8838s\n",
      "Epoch: 3 cost time: 35.1739501953125\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.7876654 Vali Loss: 1.5504272 Test Loss: 1.4852965\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6836134\n",
      "\tspeed: 0.4719s/iter; left time: 4521.7741s\n",
      "\titers: 200, epoch: 4 | loss: 0.9231550\n",
      "\tspeed: 0.0519s/iter; left time: 491.9433s\n",
      "Epoch: 4 cost time: 35.32384943962097\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.7800981 Vali Loss: 1.4713809 Test Loss: 1.4646264\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6956918\n",
      "\tspeed: 0.4638s/iter; left time: 4348.7517s\n",
      "\titers: 200, epoch: 5 | loss: 0.6025602\n",
      "\tspeed: 0.0516s/iter; left time: 478.8465s\n",
      "Epoch: 5 cost time: 35.61347246170044\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.7747160 Vali Loss: 1.4872669 Test Loss: 1.4446075\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7605589\n",
      "\tspeed: 0.4613s/iter; left time: 4230.8780s\n",
      "\titers: 200, epoch: 6 | loss: 0.7899059\n",
      "\tspeed: 0.0516s/iter; left time: 468.4084s\n",
      "Epoch: 6 cost time: 35.259695529937744\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.7726013 Vali Loss: 1.4798889 Test Loss: 1.4578391\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.7636039\n",
      "\tspeed: 0.4602s/iter; left time: 4125.3593s\n",
      "\titers: 200, epoch: 7 | loss: 0.8509037\n",
      "\tspeed: 0.0515s/iter; left time: 456.5433s\n",
      "Epoch: 7 cost time: 35.26242756843567\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.7724088 Vali Loss: 1.4819461 Test Loss: 1.4573078\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 490\n",
      "488, 488\n",
      "6832, 6832\n",
      " accuracy, f1: 0.020 & 0.039 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'natural_gas_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 14, 'pred_len': 14, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='natural_gas_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 14\n",
    "args.pred_len = 14\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1651\n",
      "val 239\n",
      "test 490\n",
      "\titers: 100, epoch: 1 | loss: 0.6901675\n",
      "\tspeed: 0.2896s/iter; left time: 2954.0653s\n",
      "\titers: 200, epoch: 1 | loss: 1.2720895\n",
      "\tspeed: 0.0518s/iter; left time: 523.3331s\n",
      "Epoch: 1 cost time: 35.24260234832764\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.8955525 Vali Loss: 2.0291462 Test Loss: 7.2189631\n",
      "Validation loss decreased (inf --> 2.029146).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.8486303\n",
      "\tspeed: 0.4605s/iter; left time: 4602.9429s\n",
      "\titers: 200, epoch: 2 | loss: 1.0460898\n",
      "\tspeed: 0.0517s/iter; left time: 511.5081s\n",
      "Epoch: 2 cost time: 35.08385729789734\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.8843704 Vali Loss: 2.0378432 Test Loss: 7.2751784\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6472185\n",
      "\tspeed: 0.4622s/iter; left time: 4524.9365s\n",
      "\titers: 200, epoch: 3 | loss: 0.5629636\n",
      "\tspeed: 0.0517s/iter; left time: 500.5209s\n",
      "Epoch: 3 cost time: 35.39165711402893\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.8705124 Vali Loss: 2.0221300 Test Loss: 7.2289691\n",
      "Validation loss decreased (2.029146 --> 2.022130).  Saving model ...\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.0136191\n",
      "\tspeed: 0.4632s/iter; left time: 4438.7425s\n",
      "\titers: 200, epoch: 4 | loss: 0.7220510\n",
      "\tspeed: 0.0519s/iter; left time: 491.8727s\n",
      "Epoch: 4 cost time: 35.36789512634277\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.8618818 Vali Loss: 2.0061517 Test Loss: 7.2246528\n",
      "Validation loss decreased (2.022130 --> 2.006152).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.6637847\n",
      "\tspeed: 0.4638s/iter; left time: 4349.5124s\n",
      "\titers: 200, epoch: 5 | loss: 0.7709203\n",
      "\tspeed: 0.0517s/iter; left time: 480.0394s\n",
      "Epoch: 5 cost time: 35.38854360580444\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.8548267 Vali Loss: 2.0303063 Test Loss: 7.2158175\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8350360\n",
      "\tspeed: 0.4607s/iter; left time: 4225.2544s\n",
      "\titers: 200, epoch: 6 | loss: 0.7539808\n",
      "\tspeed: 0.0516s/iter; left time: 467.7258s\n",
      "Epoch: 6 cost time: 35.31524181365967\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.8514068 Vali Loss: 2.0328045 Test Loss: 7.2233639\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.8902128\n",
      "\tspeed: 0.4609s/iter; left time: 4132.2004s\n",
      "\titers: 200, epoch: 7 | loss: 0.7714928\n",
      "\tspeed: 0.0517s/iter; left time: 458.4708s\n",
      "Epoch: 7 cost time: 35.2288281917572\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.8495075 Vali Loss: 2.0373228 Test Loss: 7.2238126\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.8336563\n",
      "\tspeed: 0.4619s/iter; left time: 4045.7758s\n",
      "\titers: 200, epoch: 8 | loss: 0.9053488\n",
      "\tspeed: 0.0518s/iter; left time: 448.2167s\n",
      "Epoch: 8 cost time: 35.35645842552185\n",
      "Epoch: 8, Steps: 206 | Train Loss: 0.8485301 Vali Loss: 2.0475054 Test Loss: 7.2238626\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 1.0833203\n",
      "\tspeed: 0.4642s/iter; left time: 3970.1498s\n",
      "\titers: 200, epoch: 9 | loss: 1.3682619\n",
      "\tspeed: 0.0517s/iter; left time: 437.1469s\n",
      "Epoch: 9 cost time: 35.452627420425415\n",
      "Epoch: 9, Steps: 206 | Train Loss: 0.8479546 Vali Loss: 2.0298574 Test Loss: 7.2234597\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 490\n",
      "488, 488\n",
      "6832, 6832\n",
      " accuracy, f1: 0.368 & 0.538 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'corn_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 14, 'pred_len': 14, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='corn_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 14\n",
    "args.pred_len = 14\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1648\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.6302173\n",
      "\tspeed: 0.2924s/iter; left time: 2982.4118s\n",
      "\titers: 200, epoch: 1 | loss: 0.5111011\n",
      "\tspeed: 0.0517s/iter; left time: 521.9231s\n",
      "Epoch: 1 cost time: 35.497000217437744\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.6896726 Vali Loss: 0.7556894 Test Loss: 3.0806372\n",
      "Validation loss decreased (inf --> 0.755689).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.8323508\n",
      "\tspeed: 0.4643s/iter; left time: 4641.0780s\n",
      "\titers: 200, epoch: 2 | loss: 0.8922726\n",
      "\tspeed: 0.0524s/iter; left time: 518.7365s\n",
      "Epoch: 2 cost time: 35.49177956581116\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.6776101 Vali Loss: 0.7755061 Test Loss: 3.1005499\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.4990257\n",
      "\tspeed: 0.4634s/iter; left time: 4536.3304s\n",
      "\titers: 200, epoch: 3 | loss: 0.8039528\n",
      "\tspeed: 0.0518s/iter; left time: 501.4084s\n",
      "Epoch: 3 cost time: 35.37839698791504\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.6642412 Vali Loss: 0.7665719 Test Loss: 3.0868976\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5488640\n",
      "\tspeed: 0.4617s/iter; left time: 4424.1670s\n",
      "\titers: 200, epoch: 4 | loss: 0.6874160\n",
      "\tspeed: 0.0516s/iter; left time: 489.2617s\n",
      "Epoch: 4 cost time: 35.223978996276855\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.6544036 Vali Loss: 0.7651895 Test Loss: 3.0953078\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4864448\n",
      "\tspeed: 0.4645s/iter; left time: 4355.4207s\n",
      "\titers: 200, epoch: 5 | loss: 0.6793131\n",
      "\tspeed: 0.0517s/iter; left time: 479.1766s\n",
      "Epoch: 5 cost time: 35.36916375160217\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.6491679 Vali Loss: 0.7725978 Test Loss: 3.0940664\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8080034\n",
      "\tspeed: 0.4622s/iter; left time: 4238.4135s\n",
      "\titers: 200, epoch: 6 | loss: 0.7913948\n",
      "\tspeed: 0.0519s/iter; left time: 470.5242s\n",
      "Epoch: 6 cost time: 35.313939809799194\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.6453066 Vali Loss: 0.7802095 Test Loss: 3.1005731\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "488, 488\n",
      "6832, 6832\n",
      " accuracy, f1: 0.018 & 0.036 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'coffee_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 14, 'pred_len': 14, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='coffee_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 14\n",
    "args.pred_len = 14\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.6717560\n",
      "\tspeed: 0.2922s/iter; left time: 2980.7278s\n",
      "\titers: 200, epoch: 1 | loss: 0.5663242\n",
      "\tspeed: 0.0519s/iter; left time: 524.6557s\n",
      "Epoch: 1 cost time: 35.52903747558594\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.8849214 Vali Loss: 0.9961210 Test Loss: 4.3159695\n",
      "Validation loss decreased (inf --> 0.996121).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.7586984\n",
      "\tspeed: 0.4639s/iter; left time: 4636.9572s\n",
      "\titers: 200, epoch: 2 | loss: 0.6653163\n",
      "\tspeed: 0.0517s/iter; left time: 511.8684s\n",
      "Epoch: 2 cost time: 35.3178174495697\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.8661423 Vali Loss: 1.0656055 Test Loss: 4.2815118\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7586295\n",
      "\tspeed: 0.4635s/iter; left time: 4536.7218s\n",
      "\titers: 200, epoch: 3 | loss: 0.8322031\n",
      "\tspeed: 0.0517s/iter; left time: 500.7040s\n",
      "Epoch: 3 cost time: 35.39368009567261\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.8513006 Vali Loss: 1.0308493 Test Loss: 4.3187785\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.9539112\n",
      "\tspeed: 0.4645s/iter; left time: 4451.6655s\n",
      "\titers: 200, epoch: 4 | loss: 0.9726741\n",
      "\tspeed: 0.0519s/iter; left time: 492.4698s\n",
      "Epoch: 4 cost time: 35.50194239616394\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.8401213 Vali Loss: 1.0704868 Test Loss: 4.3038239\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 1.0181060\n",
      "\tspeed: 0.4637s/iter; left time: 4347.7492s\n",
      "\titers: 200, epoch: 5 | loss: 0.6940854\n",
      "\tspeed: 0.0518s/iter; left time: 480.9362s\n",
      "Epoch: 5 cost time: 35.4362211227417\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.8363217 Vali Loss: 1.0559486 Test Loss: 4.3240190\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.7033159\n",
      "\tspeed: 0.4650s/iter; left time: 4264.0953s\n",
      "\titers: 200, epoch: 6 | loss: 0.7135251\n",
      "\tspeed: 0.0520s/iter; left time: 471.3259s\n",
      "Epoch: 6 cost time: 35.49214744567871\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.8337387 Vali Loss: 1.0569073 Test Loss: 4.3187594\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "488, 488\n",
      "6832, 6832\n",
      " accuracy, f1: 0.333 & 0.500 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'gold_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 28, 'pred_len': 28, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='gold_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 28\n",
    "args.pred_len = 28\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1636\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.6036149\n",
      "\tspeed: 0.2953s/iter; left time: 2982.8015s\n",
      "\titers: 200, epoch: 1 | loss: 0.7139179\n",
      "\tspeed: 0.0552s/iter; left time: 552.1386s\n",
      "Epoch: 1 cost time: 36.07482433319092\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.6889766 Vali Loss: 5.7942133 Test Loss: 8.5532084\n",
      "Validation loss decreased (inf --> 5.794213).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.5617464\n",
      "\tspeed: 0.4698s/iter; left time: 4649.6308s\n",
      "\titers: 200, epoch: 2 | loss: 0.4372019\n",
      "\tspeed: 0.0552s/iter; left time: 540.8448s\n",
      "Epoch: 2 cost time: 36.06253099441528\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.6784700 Vali Loss: 5.7197633 Test Loss: 8.5285025\n",
      "Validation loss decreased (5.794213 --> 5.719763).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.9923983\n",
      "\tspeed: 0.4677s/iter; left time: 4533.4017s\n",
      "\titers: 200, epoch: 3 | loss: 0.6609157\n",
      "\tspeed: 0.0554s/iter; left time: 531.6733s\n",
      "Epoch: 3 cost time: 35.96587586402893\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.6678389 Vali Loss: 5.7241445 Test Loss: 8.5311260\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5103097\n",
      "\tspeed: 0.4704s/iter; left time: 4463.7651s\n",
      "\titers: 200, epoch: 4 | loss: 0.6327842\n",
      "\tspeed: 0.0554s/iter; left time: 520.0902s\n",
      "Epoch: 4 cost time: 36.20465135574341\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.6611459 Vali Loss: 5.6801767 Test Loss: 8.5038929\n",
      "Validation loss decreased (5.719763 --> 5.680177).  Saving model ...\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5085599\n",
      "\tspeed: 0.4809s/iter; left time: 4465.3240s\n",
      "\titers: 200, epoch: 5 | loss: 0.6102890\n",
      "\tspeed: 0.0608s/iter; left time: 558.3616s\n",
      "Epoch: 5 cost time: 37.321659326553345\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.6573888 Vali Loss: 5.7064281 Test Loss: 8.5176420\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8944261\n",
      "\tspeed: 0.4777s/iter; left time: 4337.9982s\n",
      "\titers: 200, epoch: 6 | loss: 0.6827644\n",
      "\tspeed: 0.0554s/iter; left time: 497.1729s\n",
      "Epoch: 6 cost time: 36.32300353050232\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.6552238 Vali Loss: 5.6992564 Test Loss: 8.5231886\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.9001331\n",
      "\tspeed: 0.4718s/iter; left time: 4188.2061s\n",
      "\titers: 200, epoch: 7 | loss: 0.6773777\n",
      "\tspeed: 0.0579s/iter; left time: 508.3599s\n",
      "Epoch: 7 cost time: 36.49339747428894\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.6536298 Vali Loss: 5.7112350 Test Loss: 8.5299702\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 7.8125e-06\n",
      "\titers: 100, epoch: 8 | loss: 0.8009439\n",
      "\tspeed: 0.4723s/iter; left time: 4096.4190s\n",
      "\titers: 200, epoch: 8 | loss: 0.6653074\n",
      "\tspeed: 0.0553s/iter; left time: 474.2872s\n",
      "Epoch: 8 cost time: 36.286436319351196\n",
      "Epoch: 8, Steps: 204 | Train Loss: 0.6528633 Vali Loss: 5.6692367 Test Loss: 8.5293131\n",
      "Validation loss decreased (5.680177 --> 5.669237).  Saving model ...\n",
      "Updating learning rate to 3.90625e-06\n",
      "\titers: 100, epoch: 9 | loss: 0.7750760\n",
      "\tspeed: 0.4748s/iter; left time: 4021.2489s\n",
      "\titers: 200, epoch: 9 | loss: 0.7017864\n",
      "\tspeed: 0.0559s/iter; left time: 467.7779s\n",
      "Epoch: 9 cost time: 36.60947608947754\n",
      "Epoch: 9, Steps: 204 | Train Loss: 0.6515851 Vali Loss: 5.6593509 Test Loss: 8.5262289\n",
      "Validation loss decreased (5.669237 --> 5.659351).  Saving model ...\n",
      "Updating learning rate to 1.953125e-06\n",
      "\titers: 100, epoch: 10 | loss: 0.5385040\n",
      "\tspeed: 0.4779s/iter; left time: 3949.7257s\n",
      "\titers: 200, epoch: 10 | loss: 0.5149304\n",
      "\tspeed: 0.0584s/iter; left time: 476.9501s\n",
      "Epoch: 10 cost time: 36.64321160316467\n",
      "Epoch: 10, Steps: 204 | Train Loss: 0.6529223 Vali Loss: 5.6681175 Test Loss: 8.5329332\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 9.765625e-07\n",
      "\titers: 100, epoch: 11 | loss: 0.5071856\n",
      "\tspeed: 0.4731s/iter; left time: 3813.7153s\n",
      "\titers: 200, epoch: 11 | loss: 0.7669195\n",
      "\tspeed: 0.0553s/iter; left time: 440.5411s\n",
      "Epoch: 11 cost time: 36.31153750419617\n",
      "Epoch: 11, Steps: 204 | Train Loss: 0.6525986 Vali Loss: 5.7089295 Test Loss: 8.5305967\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 4.8828125e-07\n",
      "\titers: 100, epoch: 12 | loss: 0.6986902\n",
      "\tspeed: 0.4729s/iter; left time: 3715.5008s\n",
      "\titers: 200, epoch: 12 | loss: 0.7769380\n",
      "\tspeed: 0.0555s/iter; left time: 430.5833s\n",
      "Epoch: 12 cost time: 36.336740255355835\n",
      "Epoch: 12, Steps: 204 | Train Loss: 0.6521482 Vali Loss: 5.6951184 Test Loss: 8.5304413\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 2.44140625e-07\n",
      "\titers: 100, epoch: 13 | loss: 0.5664341\n",
      "\tspeed: 0.4729s/iter; left time: 3619.1067s\n",
      "\titers: 200, epoch: 13 | loss: 0.7009282\n",
      "\tspeed: 0.0555s/iter; left time: 418.8923s\n",
      "Epoch: 13 cost time: 36.3602135181427\n",
      "Epoch: 13, Steps: 204 | Train Loss: 0.6517024 Vali Loss: 5.7082376 Test Loss: 8.5308065\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.220703125e-07\n",
      "\titers: 100, epoch: 14 | loss: 0.7522146\n",
      "\tspeed: 0.4720s/iter; left time: 3515.9181s\n",
      "\titers: 200, epoch: 14 | loss: 0.8314126\n",
      "\tspeed: 0.0554s/iter; left time: 407.3373s\n",
      "Epoch: 14 cost time: 36.15999889373779\n",
      "Epoch: 14, Steps: 204 | Train Loss: 0.6525759 Vali Loss: 5.7054782 Test Loss: 8.5308161\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "472, 472\n",
      "13216, 13216\n",
      " accuracy, f1: 0.630 & 0.773 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'silver_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 28, 'pred_len': 28, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='silver_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 28\n",
    "args.pred_len = 28\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : silver_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1635\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.7415410\n",
      "\tspeed: 0.2968s/iter; left time: 2998.2327s\n",
      "\titers: 200, epoch: 1 | loss: 0.5628698\n",
      "\tspeed: 0.0555s/iter; left time: 554.9223s\n",
      "Epoch: 1 cost time: 36.31732106208801\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.6284080 Vali Loss: 1.1501398 Test Loss: 2.0240119\n",
      "Validation loss decreased (inf --> 1.150140).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.5531747\n",
      "\tspeed: 0.4730s/iter; left time: 4680.8213s\n",
      "\titers: 200, epoch: 2 | loss: 0.5956247\n",
      "\tspeed: 0.0553s/iter; left time: 542.0996s\n",
      "Epoch: 2 cost time: 36.236055850982666\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.6160459 Vali Loss: 1.1111615 Test Loss: 1.9955672\n",
      "Validation loss decreased (1.150140 --> 1.111161).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7645026\n",
      "\tspeed: 0.4748s/iter; left time: 4601.7739s\n",
      "\titers: 200, epoch: 3 | loss: 0.7646417\n",
      "\tspeed: 0.0554s/iter; left time: 531.1114s\n",
      "Epoch: 3 cost time: 36.33234739303589\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.6052900 Vali Loss: 1.1223692 Test Loss: 2.0228081\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.6073306\n",
      "\tspeed: 0.4731s/iter; left time: 4488.8916s\n",
      "\titers: 200, epoch: 4 | loss: 0.8205689\n",
      "\tspeed: 0.0555s/iter; left time: 520.9491s\n",
      "Epoch: 4 cost time: 36.36379027366638\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.5973377 Vali Loss: 1.1283543 Test Loss: 2.0386779\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.4534679\n",
      "\tspeed: 0.4717s/iter; left time: 4380.0014s\n",
      "\titers: 200, epoch: 5 | loss: 0.8295865\n",
      "\tspeed: 0.0554s/iter; left time: 508.5918s\n",
      "Epoch: 5 cost time: 36.15819525718689\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.5908729 Vali Loss: 1.1495153 Test Loss: 2.0588803\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.4771291\n",
      "\tspeed: 0.4740s/iter; left time: 4304.3870s\n",
      "\titers: 200, epoch: 6 | loss: 0.6043503\n",
      "\tspeed: 0.0554s/iter; left time: 497.5987s\n",
      "Epoch: 6 cost time: 36.330841302871704\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.5852025 Vali Loss: 1.1882151 Test Loss: 2.0966284\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.5018680\n",
      "\tspeed: 0.4723s/iter; left time: 4192.2678s\n",
      "\titers: 200, epoch: 7 | loss: 0.6292035\n",
      "\tspeed: 0.0554s/iter; left time: 485.8238s\n",
      "Epoch: 7 cost time: 36.28451895713806\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.5825742 Vali Loss: 1.1867163 Test Loss: 2.0951774\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : silver_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "472, 472\n",
      "13216, 13216\n",
      " accuracy, f1: 0.054 & 0.102 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'crude_oil_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 28, 'pred_len': 28, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='crude_oil_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 28\n",
    "args.pred_len = 28\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : crude_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1637\n",
      "val 225\n",
      "test 476\n",
      "\titers: 100, epoch: 1 | loss: 0.7813736\n",
      "\tspeed: 0.2971s/iter; left time: 3001.0793s\n",
      "\titers: 200, epoch: 1 | loss: 1.0524077\n",
      "\tspeed: 0.0556s/iter; left time: 555.7866s\n",
      "Epoch: 1 cost time: 36.35511612892151\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.7973842 Vali Loss: 1.5683143 Test Loss: 1.4443076\n",
      "Validation loss decreased (inf --> 1.568314).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6355556\n",
      "\tspeed: 0.4723s/iter; left time: 4674.0955s\n",
      "\titers: 200, epoch: 2 | loss: 0.8990454\n",
      "\tspeed: 0.0553s/iter; left time: 541.5876s\n",
      "Epoch: 2 cost time: 36.19926643371582\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.7897036 Vali Loss: 1.4759213 Test Loss: 1.3795062\n",
      "Validation loss decreased (1.568314 --> 1.475921).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7093701\n",
      "\tspeed: 0.4733s/iter; left time: 4587.7198s\n",
      "\titers: 200, epoch: 3 | loss: 0.8825181\n",
      "\tspeed: 0.0554s/iter; left time: 531.4074s\n",
      "Epoch: 3 cost time: 36.28863883018494\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.7745186 Vali Loss: 1.5270795 Test Loss: 1.5134329\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.5952355\n",
      "\tspeed: 0.4742s/iter; left time: 4500.1522s\n",
      "\titers: 200, epoch: 4 | loss: 0.9181113\n",
      "\tspeed: 0.0554s/iter; left time: 519.9265s\n",
      "Epoch: 4 cost time: 36.36145901679993\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.7678328 Vali Loss: 1.4787322 Test Loss: 1.4384297\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.8198908\n",
      "\tspeed: 0.4741s/iter; left time: 4402.2129s\n",
      "\titers: 200, epoch: 5 | loss: 0.6876121\n",
      "\tspeed: 0.0555s/iter; left time: 510.0662s\n",
      "Epoch: 5 cost time: 36.50466275215149\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.7638978 Vali Loss: 1.4798590 Test Loss: 1.4510818\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8759443\n",
      "\tspeed: 0.4749s/iter; left time: 4312.5059s\n",
      "\titers: 200, epoch: 6 | loss: 0.8617988\n",
      "\tspeed: 0.0556s/iter; left time: 498.9206s\n",
      "Epoch: 6 cost time: 36.52747416496277\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.7617991 Vali Loss: 1.5058873 Test Loss: 1.4727173\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.8936497\n",
      "\tspeed: 0.4721s/iter; left time: 4191.0147s\n",
      "\titers: 200, epoch: 7 | loss: 0.7725074\n",
      "\tspeed: 0.0556s/iter; left time: 487.8728s\n",
      "Epoch: 7 cost time: 36.31062412261963\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.7611091 Vali Loss: 1.5183083 Test Loss: 1.4844086\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : crude_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 476\n",
      "472, 472\n",
      "13216, 13216\n",
      " accuracy, f1: 0.008 & 0.015 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : gold_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1650\n",
      "val 239\n",
      "test 489\n",
      "\titers: 100, epoch: 1 | loss: 0.7150675\n",
      "\tspeed: 0.2930s/iter; left time: 2988.4866s\n",
      "\titers: 200, epoch: 1 | loss: 1.0009717\n",
      "\tspeed: 0.0520s/iter; left time: 524.7884s\n",
      "Epoch: 1 cost time: 35.681819915771484\n",
      "Epoch: 1, Steps: 206 | Train Loss: 0.7102846 Vali Loss: 5.8019609 Test Loss: 8.5066147\n",
      "Validation loss decreased (inf --> 5.801961).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6824962\n",
      "\tspeed: 0.4716s/iter; left time: 4714.0353s\n",
      "\titers: 200, epoch: 2 | loss: 0.5746744\n",
      "\tspeed: 0.0517s/iter; left time: 511.1028s\n",
      "Epoch: 2 cost time: 35.59051203727722\n",
      "Epoch: 2, Steps: 206 | Train Loss: 0.7020987 Vali Loss: 5.6443119 Test Loss: 8.4638128\n",
      "Validation loss decreased (5.801961 --> 5.644312).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.7148576\n",
      "\tspeed: 0.4705s/iter; left time: 4605.9767s\n",
      "\titers: 200, epoch: 3 | loss: 0.6398650\n",
      "\tspeed: 0.0519s/iter; left time: 502.9938s\n",
      "Epoch: 3 cost time: 35.68192005157471\n",
      "Epoch: 3, Steps: 206 | Train Loss: 0.6855648 Vali Loss: 5.7093654 Test Loss: 8.4993229\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.4827015\n",
      "\tspeed: 0.4690s/iter; left time: 4494.8102s\n",
      "\titers: 200, epoch: 4 | loss: 0.5259620\n",
      "\tspeed: 0.0519s/iter; left time: 492.5124s\n",
      "Epoch: 4 cost time: 35.634434938430786\n",
      "Epoch: 4, Steps: 206 | Train Loss: 0.6806992 Vali Loss: 5.7225533 Test Loss: 8.4656172\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.5274728\n",
      "\tspeed: 0.4702s/iter; left time: 4409.5033s\n",
      "\titers: 200, epoch: 5 | loss: 0.9052879\n",
      "\tspeed: 0.0519s/iter; left time: 481.0916s\n",
      "Epoch: 5 cost time: 35.6324622631073\n",
      "Epoch: 5, Steps: 206 | Train Loss: 0.6768525 Vali Loss: 5.7834997 Test Loss: 8.4701185\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5503940\n",
      "\tspeed: 0.4684s/iter; left time: 4295.9317s\n",
      "\titers: 200, epoch: 6 | loss: 0.5321650\n",
      "\tspeed: 0.0519s/iter; left time: 470.9759s\n",
      "Epoch: 6 cost time: 35.719215393066406\n",
      "Epoch: 6, Steps: 206 | Train Loss: 0.6739516 Vali Loss: 5.6728654 Test Loss: 8.4779854\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.6875799\n",
      "\tspeed: 0.4694s/iter; left time: 4208.0537s\n",
      "\titers: 200, epoch: 7 | loss: 0.4752099\n",
      "\tspeed: 0.0519s/iter; left time: 460.4619s\n",
      "Epoch: 7 cost time: 35.7357280254364\n",
      "Epoch: 7, Steps: 206 | Train Loss: 0.6720031 Vali Loss: 5.6672688 Test Loss: 8.4728003\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : gold_classification_Autoformer_custom_ftM_sl96_ll14_pl14_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 489\n",
      "488, 488\n",
      "6832, 6832\n",
      " accuracy, f1: 0.721 & 0.838 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'natural_gas_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 28, 'pred_len': 28, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='natural_gas_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 28\n",
    "args.pred_len = 28\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : natural_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1637\n",
      "val 225\n",
      "test 476\n",
      "\titers: 100, epoch: 1 | loss: 0.9038433\n",
      "\tspeed: 0.2971s/iter; left time: 3000.8960s\n",
      "\titers: 200, epoch: 1 | loss: 1.2418987\n",
      "\tspeed: 0.0555s/iter; left time: 555.0736s\n",
      "Epoch: 1 cost time: 36.35600709915161\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.8919711 Vali Loss: 2.0839107 Test Loss: 7.1337161\n",
      "Validation loss decreased (inf --> 2.083911).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 1.0575428\n",
      "\tspeed: 0.4739s/iter; left time: 4690.3778s\n",
      "\titers: 200, epoch: 2 | loss: 0.8329572\n",
      "\tspeed: 0.0557s/iter; left time: 546.1489s\n",
      "Epoch: 2 cost time: 36.44255757331848\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.8874255 Vali Loss: 2.0596650 Test Loss: 7.1137242\n",
      "Validation loss decreased (2.083911 --> 2.059665).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.9319574\n",
      "\tspeed: 0.4746s/iter; left time: 4600.0347s\n",
      "\titers: 200, epoch: 3 | loss: 1.4445350\n",
      "\tspeed: 0.0556s/iter; left time: 532.9700s\n",
      "Epoch: 3 cost time: 36.40581560134888\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.8748788 Vali Loss: 2.1494071 Test Loss: 7.1635571\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.8365091\n",
      "\tspeed: 0.4719s/iter; left time: 4477.6584s\n",
      "\titers: 200, epoch: 4 | loss: 0.5164811\n",
      "\tspeed: 0.0554s/iter; left time: 520.2290s\n",
      "Epoch: 4 cost time: 36.17454719543457\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.8640526 Vali Loss: 2.0808532 Test Loss: 7.1511679\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7736896\n",
      "\tspeed: 0.4752s/iter; left time: 4412.6146s\n",
      "\titers: 200, epoch: 5 | loss: 0.9432966\n",
      "\tspeed: 0.0554s/iter; left time: 508.6131s\n",
      "Epoch: 5 cost time: 36.45685958862305\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.8526407 Vali Loss: 2.0759482 Test Loss: 7.1575508\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.8729364\n",
      "\tspeed: 0.4725s/iter; left time: 4291.0672s\n",
      "\titers: 200, epoch: 6 | loss: 0.7294174\n",
      "\tspeed: 0.0556s/iter; left time: 499.6899s\n",
      "Epoch: 6 cost time: 36.271475315093994\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.8470595 Vali Loss: 2.0953372 Test Loss: 7.1603193\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.7862776\n",
      "\tspeed: 0.4734s/iter; left time: 4202.6631s\n",
      "\titers: 200, epoch: 7 | loss: 0.7639600\n",
      "\tspeed: 0.0556s/iter; left time: 488.0179s\n",
      "Epoch: 7 cost time: 36.263641357421875\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.8437746 Vali Loss: 2.1145451 Test Loss: 7.1614614\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : natural_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 476\n",
      "472, 472\n",
      "13216, 13216\n",
      " accuracy, f1: 0.168 & 0.288 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'DLinear', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'corn_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 28, 'pred_len': 28, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'DLinear' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='corn_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 28\n",
    "args.pred_len = 28\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : corn_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1634\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.5086083\n",
      "\tspeed: 0.2962s/iter; left time: 2992.3232s\n",
      "\titers: 200, epoch: 1 | loss: 0.5787496\n",
      "\tspeed: 0.0554s/iter; left time: 554.1252s\n",
      "Epoch: 1 cost time: 36.26370906829834\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.6733489 Vali Loss: 0.7653006 Test Loss: 3.0912483\n",
      "Validation loss decreased (inf --> 0.765301).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.6788849\n",
      "\tspeed: 0.4722s/iter; left time: 4673.1789s\n",
      "\titers: 200, epoch: 2 | loss: 0.8291518\n",
      "\tspeed: 0.0553s/iter; left time: 541.7891s\n",
      "Epoch: 2 cost time: 36.21080708503723\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.6601443 Vali Loss: 0.7662546 Test Loss: 3.1142321\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.6309078\n",
      "\tspeed: 0.4728s/iter; left time: 4582.8892s\n",
      "\titers: 200, epoch: 3 | loss: 0.5616982\n",
      "\tspeed: 0.0555s/iter; left time: 532.6244s\n",
      "Epoch: 3 cost time: 36.36563801765442\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.6450601 Vali Loss: 0.8111833 Test Loss: 3.1292398\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 0.8925178\n",
      "\tspeed: 0.4733s/iter; left time: 4491.6126s\n",
      "\titers: 200, epoch: 4 | loss: 0.8307012\n",
      "\tspeed: 0.0554s/iter; left time: 520.4932s\n",
      "Epoch: 4 cost time: 36.292189836502075\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.6342014 Vali Loss: 0.8245059 Test Loss: 3.1469195\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7413740\n",
      "\tspeed: 0.4737s/iter; left time: 4397.9903s\n",
      "\titers: 200, epoch: 5 | loss: 0.5281507\n",
      "\tspeed: 0.0554s/iter; left time: 509.1310s\n",
      "Epoch: 5 cost time: 36.302571535110474\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.6293653 Vali Loss: 0.8320662 Test Loss: 3.1440685\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 0.5458947\n",
      "\tspeed: 0.4732s/iter; left time: 4297.3776s\n",
      "\titers: 200, epoch: 6 | loss: 0.4703794\n",
      "\tspeed: 0.0554s/iter; left time: 497.7045s\n",
      "Epoch: 6 cost time: 36.332605600357056\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.6271046 Vali Loss: 0.8221062 Test Loss: 3.1407413\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : corn_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "472, 472\n",
      "13216, 13216\n",
      " accuracy, f1: 0.075 & 0.139 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "{'is_training': 1, 'model_id': 'classification', 'model': 'Autoformer', 'data': 'custom', 'root_path': './dataset/commodity/', 'data_path': 'coffee_data_c.csv', 'features': 'M', 'target': 'OT', 'freq': 'd', 'checkpoints': './checkpoints/', 'seq_len': 96, 'label_len': 28, 'pred_len': 28, 'individual': False, 'embed_type': 0, 'enc_in': 6, 'dec_in': 6, 'c_out': 6, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'moving_avg': 25, 'factor': 3, 'distil': True, 'dropout': 0.05, 'embed': 'timeF', 'do_predict': True, 'num_workers': 10, 'itr': 1, 'train_epochs': 50, 'patience': 5, 'learning_rate': 0.0005, 'batch_size': 8, 'lradj': 'type1', 'des': 'Exp', 'gpu': 0, 'devices': '0', 'use_gpu': True, 'use_multi_gpu': False}\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np\n",
    "from utils.tools import dotdict\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Autoformer & Transformer family for Time Series Forecasting')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--is_training', type=int, required=True, default=1, help='status')\n",
    "parser.add_argument('--model_id', type=str, required=True, default='test', help='model id')\n",
    "parser.add_argument('--model', type=str, required=True, default='Autoformer',\n",
    "                    help='model name, options: [Autoformer, Informer, Transformer]')\n",
    "\n",
    "# data loader\n",
    "parser.add_argument('--data', type=str, required=True, default='ETTm1', help='dataset type')\n",
    "parser.add_argument('--root_path', type=str, default='./data/ETT/', help='root path of the data file')\n",
    "parser.add_argument('--data_path', type=str, default='ETTh1.csv', help='data file')\n",
    "parser.add_argument('--features', type=str, default='M',\n",
    "                    help='forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate')\n",
    "parser.add_argument('--target', type=str, default='OT', help='target feature in S or MS task')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "parser.add_argument('--checkpoints', type=str, default='./checkpoints/', help='location of model checkpoints')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=96, help='prediction sequence length')\n",
    "\n",
    "\n",
    "# DLinear\n",
    "parser.add_argument('--individual', action='store_true', default=False, help='DLinear: a linear layer for each variate(channel) individually')\n",
    "\n",
    "# Formers \n",
    "parser.add_argument('--embed_type', type=int, default=0, help='0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size') # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=512, help='dimension of model')\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=2, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=2048, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.05, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "parser.add_argument('--do_predict', action='store_true', help='whether to predict unseen future data')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "parser.add_argument('--itr', type=int, default=2, help='experiments times')\n",
    "parser.add_argument('--train_epochs', type=int, default=10, help='train epochs')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size of train input data')\n",
    "parser.add_argument('--patience', type=int, default=3, help='early stopping patience')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.0001, help='optimizer learning rate')\n",
    "parser.add_argument('--des', type=str, default='test', help='exp description')\n",
    "parser.add_argument('--loss', type=str, default='mse', help='loss function')\n",
    "parser.add_argument('--lradj', type=str, default='type1', help='adjust learning rate')\n",
    "parser.add_argument('--use_amp', action='store_true', help='use automatic mixed precision training', default=False)\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)\n",
    "parser.add_argument('--devices', type=str, default='0,1,2,3', help='device ids of multile gpus')\n",
    "parser.add_argument('--test_flop', action='store_true', default=False, help='See utils/tools for usage')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "args = dotdict()\n",
    "\n",
    "# basic config\n",
    "args.is_training = 1\n",
    "args.model_id = 'classification'\n",
    "args.model = 'Autoformer' #Autoformer/Informer/Transformer/DLinear/NLinear\n",
    "\n",
    "# data loader\n",
    "args.data = 'custom'\n",
    "args.root_path = './dataset/commodity/'\n",
    "args.data_path ='coffee_data_c.csv' \n",
    "args.features = 'M'\n",
    "args.target = 'OT'\n",
    "args.freq = 'd'\n",
    "args.checkpoints = './checkpoints/'\n",
    "\n",
    "# forecasting task\n",
    "args.seq_len = 96\n",
    "args.label_len = 28\n",
    "args.pred_len = 28\n",
    "\n",
    "# DLinear\n",
    "args.individual = False\n",
    "\n",
    "# Formers \n",
    "args.embed_type = 0\n",
    "args.enc_in = 6\n",
    "args.dec_in = 6\n",
    "args.c_out = 6\n",
    "args.d_model = 512\n",
    "args.n_heads = 8\n",
    "args.e_layers = 2\n",
    "args.d_layers = 1\n",
    "args.d_ff = 2048\n",
    "args.moving_avg = 25\n",
    "args.factor = 3\n",
    "args.distil = True\n",
    "args.dropout = 0.05\n",
    "args.embed = 'timeF'\n",
    "args.do_predict = True\n",
    "\n",
    "# optimization\n",
    "args.num_workers = 10\n",
    "args.itr = 1\n",
    "args.train_epochs = 50\n",
    "args.patience = 5\n",
    "args.learning_rate = 0.0005\n",
    "args.batch_size = 8\n",
    "args.lradj = 'type1'\n",
    "args.des = 'Exp'\n",
    "\n",
    "# GPU\n",
    "args.gpu = 0\n",
    "args.devices = '0'\n",
    "args.use_gpu = True\n",
    "args.use_multi_gpu = False\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)\n",
    "\n",
    "Exp = Exp_Main\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : coffee_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 1636\n",
      "val 225\n",
      "test 475\n",
      "\titers: 100, epoch: 1 | loss: 0.5853355\n",
      "\tspeed: 0.2972s/iter; left time: 3002.2371s\n",
      "\titers: 200, epoch: 1 | loss: 0.6268731\n",
      "\tspeed: 0.0556s/iter; left time: 556.0614s\n",
      "Epoch: 1 cost time: 36.37827157974243\n",
      "Epoch: 1, Steps: 204 | Train Loss: 0.8894222 Vali Loss: 1.0232641 Test Loss: 4.3021717\n",
      "Validation loss decreased (inf --> 1.023264).  Saving model ...\n",
      "Updating learning rate to 0.0005\n",
      "\titers: 100, epoch: 2 | loss: 0.8018863\n",
      "\tspeed: 0.4739s/iter; left time: 4690.1961s\n",
      "\titers: 200, epoch: 2 | loss: 0.7081355\n",
      "\tspeed: 0.0558s/iter; left time: 547.1419s\n",
      "Epoch: 2 cost time: 36.34145712852478\n",
      "Epoch: 2, Steps: 204 | Train Loss: 0.8686871 Vali Loss: 0.9638785 Test Loss: 4.2979031\n",
      "Validation loss decreased (1.023264 --> 0.963879).  Saving model ...\n",
      "Updating learning rate to 0.00025\n",
      "\titers: 100, epoch: 3 | loss: 0.8720999\n",
      "\tspeed: 0.4797s/iter; left time: 4650.2032s\n",
      "\titers: 200, epoch: 3 | loss: 0.7691034\n",
      "\tspeed: 0.0555s/iter; left time: 532.8903s\n",
      "Epoch: 3 cost time: 36.55611753463745\n",
      "Epoch: 3, Steps: 204 | Train Loss: 0.8587194 Vali Loss: 1.0038081 Test Loss: 4.3123116\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Updating learning rate to 0.000125\n",
      "\titers: 100, epoch: 4 | loss: 1.0256716\n",
      "\tspeed: 0.4726s/iter; left time: 4484.2537s\n",
      "\titers: 200, epoch: 4 | loss: 0.9325147\n",
      "\tspeed: 0.0557s/iter; left time: 523.4033s\n",
      "Epoch: 4 cost time: 36.315669298172\n",
      "Epoch: 4, Steps: 204 | Train Loss: 0.8497476 Vali Loss: 0.9842067 Test Loss: 4.3067403\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Updating learning rate to 6.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.7325174\n",
      "\tspeed: 0.4740s/iter; left time: 4400.8972s\n",
      "\titers: 200, epoch: 5 | loss: 1.1122305\n",
      "\tspeed: 0.0571s/iter; left time: 524.1709s\n",
      "Epoch: 5 cost time: 36.478904247283936\n",
      "Epoch: 5, Steps: 204 | Train Loss: 0.8442850 Vali Loss: 0.9923119 Test Loss: 4.3060808\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Updating learning rate to 3.125e-05\n",
      "\titers: 100, epoch: 6 | loss: 1.0555700\n",
      "\tspeed: 0.4746s/iter; left time: 4309.5507s\n",
      "\titers: 200, epoch: 6 | loss: 0.7941568\n",
      "\tspeed: 0.0554s/iter; left time: 497.1376s\n",
      "Epoch: 6 cost time: 36.347081661224365\n",
      "Epoch: 6, Steps: 204 | Train Loss: 0.8417721 Vali Loss: 1.0273951 Test Loss: 4.3211503\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Updating learning rate to 1.5625e-05\n",
      "\titers: 100, epoch: 7 | loss: 0.7014234\n",
      "\tspeed: 0.4739s/iter; left time: 4206.5171s\n",
      "\titers: 200, epoch: 7 | loss: 0.8541638\n",
      "\tspeed: 0.0554s/iter; left time: 486.0499s\n",
      "Epoch: 7 cost time: 36.40168237686157\n",
      "Epoch: 7, Steps: 204 | Train Loss: 0.8388955 Vali Loss: 1.0153066 Test Loss: 4.3186059\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      ">>>>>>>testing : coffee_classification_Autoformer_custom_ftM_sl96_ll28_pl28_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 475\n",
      "472, 472\n",
      "13216, 13216\n",
      " accuracy, f1: 0.314 & 0.478 \n"
     ]
    }
   ],
   "source": [
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.data_path.split(\"_\")[0],\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting)\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from win10toast import ToastNotifier\n",
    "toaster = ToastNotifier()\n",
    "toaster.show_toast(\"Hello World!!!\",\n",
    "\"Python is 10 seconds awsm!\",\n",
    "icon_path=\"custom.ico\",\n",
    "duration=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "exp.args.root_path = './dataset/commodity/'\n",
    "exp.args.data_path = 'gold_data.csv'\n",
    "\n",
    "df = pd.read_csv(os.path.join(args.root_path, args.data_path))\n",
    "df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# # df.to_csv(os.path.join(args.root_path, args.data_path), index=False) \n",
    "# last_date = df['date'].max()\n",
    "# new_rows = pd.DataFrame(columns=df.columns, index=range(1, 31))\n",
    "# new_rows['date'] = pd.date_range(start=last_date + pd.DateOffset(1), periods=30)\n",
    "# df = pd.concat([df, new_rows], sort=False)\n",
    "# df.tail()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_predict:\n",
    "    print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    prediction=exp.predict(setting, True)\n",
    "    prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "    prediction.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from datetime import datetime\n",
    "\n",
    "# date_obj = '10/14/2022'\n",
    "# #date_obj = datetime.strptime(date, '%m/%d/%Y')\n",
    "\n",
    "# print(date_obj)\n",
    "\n",
    "# def create_prediction_dataset(data, seq_len, pred_len, start_date):\n",
    "#     start_index = data[data['date'] == start_date].index[0]\n",
    "#     end_index = start_index + args.seq_len + args.pred_len\n",
    "#     prediction_data = data.iloc[start_index:end_index]\n",
    "#     return prediction_data\n",
    "\n",
    "# # Create the prediction dataset\n",
    "\n",
    "# pred_data = create_prediction_dataset(df, args.seq_len, args.pred_len, date_obj)\n",
    "# pred_data.head()\n",
    "# print(pred_data)\n",
    "\n",
    "# pred_loader = DataLoader(pred_data, batch_size = args.batch_size, shuffle=False)\n",
    "\n",
    "# predictions = exp.predict(setting, pred_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_Pred\n",
    "from torch.utils.data import DataLoader\n",
    "from data_provider.data_loader import Dataset_Custom\n",
    "Data = Dataset_Custom\n",
    "\n",
    "freq = 'b'\n",
    "\n",
    "Data = Dataset_Custom\n",
    "timeenc = 0 if args.embed!='timeF' else 1\n",
    "flag = 'test'; shuffle_flag = False; drop_last = True; batch_size = 1\n",
    "\n",
    "data_set = Data(\n",
    "    root_path=args.root_path,\n",
    "    data_path=args.data_path,\n",
    "    flag=flag,\n",
    "    size=[args.seq_len, args.label_len, args.pred_len],\n",
    "    features=args.features,\n",
    "    timeenc=timeenc,\n",
    "    target=args.target, # HULL here\n",
    "    freq=args.freq # 'h': hourly, 't':minutely\n",
    ")\n",
    "data_loader = DataLoader(\n",
    "    data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle_flag,\n",
    "    num_workers=args.num_workers,\n",
    "    drop_last=drop_last)\n",
    "\n",
    "print(len(data_set), len(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.predict(setting, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we finished exp.train(setting) and exp.test(setting), we will get a trained model and the results of test experiment\n",
    "# The results of test experiment will be saved in ./results/{setting}/pred.npy (prediction of test dataset) and ./results/{setting}/true.npy (groundtruth of test dataset)\n",
    "\n",
    "preds = np.load('./results/'+setting+'/pred.npy')\n",
    "trues = np.load('./results/'+setting+'/true.npy')\n",
    "\n",
    "# [samples, pred_len, dimensions]\n",
    "print(preds.shape, trues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "data_pred = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "data_pred = torch.from_numpy(data_pred).permute(0,2,1)\n",
    "\n",
    "plt.figure()\n",
    "print(data_pred.shape)\n",
    "#预测OT\n",
    "plt.plot(data_pred[-1,-1,:])\n",
    "print(data_pred[-1,-1,:].shape)\n",
    "plt.show()\n",
    "plt.plot(data_pred[0,-1,:])\n",
    "print(data_pred[0,-1,:].shape)\n",
    "#plt.show()\n",
    "# draw HUFL prediction\n",
    "plt.plot(data_pred[0,0,:])\n",
    "print(data_pred[-1,-1,:].shape)\n",
    "plt.show()\n",
    "'''\n",
    "Ground Truth\n",
    "'''\n",
    "data_gt = np.load('./results/'+setting+'/true.npy')\n",
    "data_gt = torch.from_numpy(data_gt).permute(0,2,1)\n",
    "\n",
    "# OT\n",
    "plt.plot(data_gt[-1,-1,:])\n",
    "print(data_gt[-1,-1,:].shape)\n",
    "plt.show()\n",
    "plt.plot(data_gt[0,-1,:])\n",
    "print(data_gt[0,-1,:].shape)\n",
    "#plt.show()\n",
    "# draw HUFL prediction\n",
    "plt.plot(data_gt[0,0,:])\n",
    "print(data_gt[-1,-1,:].shape)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = 'coffee_classification_Autoformer_custom_ftS_sl96_ll56_pl56_dm512_nh8_el2_dl1_df2048_fc3_ebtimeF_dtTrue_Exp_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Linear_Seasonal.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m weights_list \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 10\u001b[0m weights_list[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseasonal\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLinear_Seasonal.weight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m weights_list[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrend\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m weights[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear_Trend.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#print(root)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#print(root.split('/'))\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Linear_Seasonal.weight'"
     ]
    }
   ],
   "source": [
    "if (args.model == 'DLinear'):\n",
    "    model_name = setting\n",
    "    for root, dirs, files in os.walk(\"checkpoints\"):\n",
    "        for name in files:\n",
    "            model_path = os.path.join(root, name)\n",
    "            if model_name not in model_path:\n",
    "                continue\n",
    "            weights = torch.load(model_path,map_location=torch.device('cpu'))\n",
    "            weights_list = {}\n",
    "            weights_list['seasonal'] = weights['Linear_Seasonal.weight'].numpy()\n",
    "            weights_list['trend'] = weights['Linear_Trend.weight'].numpy()\n",
    "\n",
    "            #print(root)\n",
    "            #print(root.split('/'))\n",
    "\n",
    "            save_root = 'weights_plot/%s'%root.split('\\\\')[1] #changed from save_root = 'weights_plot/%s'%root.split('/')[1]\n",
    "            if not os.path.exists('weights_plot'):\n",
    "                os.mkdir('weights_plot')\n",
    "            if not os.path.exists(save_root):\n",
    "                os.mkdir(save_root)\n",
    "        \n",
    "            for w_name,weight in weights_list.items():\n",
    "                fig,ax=plt.subplots()\n",
    "                im=ax.imshow(weight,cmap='plasma_r')\n",
    "                fig.colorbar(im,pad=0.03)\n",
    "                plt.savefig(os.path.join(save_root,w_name + '.pdf'),dpi=500)\n",
    "                plt.show()\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'setting' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msetting\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'setting' is not defined"
     ]
    }
   ],
   "source": [
    "setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57785bf53e86c458d31dd8512073d1ac6cae98f342ec9a1a9a8506681d63dcb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
